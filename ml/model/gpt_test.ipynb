{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpt import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 128, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 256,        # Embedding dimension\n",
    "    \"n_heads\": 4,         # Number of attention heads\n",
    "    \"n_layers\": 4,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you mosquitoes popcorn appeaseAMES Lucy decide Pittsburgh Stainless utilizationRegistration\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from gpt import generate_text_simple, create_dataloader_v1\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' = Valkyria Chronicles III = \\n',\n",
       " '',\n",
       " ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n',\n",
       " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\",\n",
       " \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\",\n",
       " '',\n",
       " ' = = Gameplay = = \\n',\n",
       " '',\n",
       " \" As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\",\n",
       " ' The game \\'s battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character <unk> can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n',\n",
       " \" Troops are divided into five classes : Scouts , <unk> , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types . \\n\",\n",
       " '',\n",
       " ' = = Plot = = \\n',\n",
       " '',\n",
       " ' The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , <unk> <unk> , meaning \" Always Ready . \" The three main characters are No.7 Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace No.1 Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and No.13 Riela <unk> , a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers . \\n',\n",
       " \" As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , <unk> , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of <unk> in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \\n\",\n",
       " \" Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient <unk> super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last trump card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the <unk> weapon . Each member then goes their separate ways in order to begin their lives anew . \\n\",\n",
       " '',\n",
       " ' = = Development = = \\n',\n",
       " '',\n",
       " ' Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development beginning shortly after this . The director of Valkyria Chronicles II , Takeshi Ozawa , returned to that role for Valkyria Chronicles III . Development work took approximately one year . After the release of Valkyria Chronicles II , the staff took a look at both the popular response for the game and what they wanted to do next for the series . Like its predecessor , Valkyria Chronicles III was developed for PlayStation Portable : this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II , and they had not come up with the \" revolutionary \" idea that would warrant a new entry for the PlayStation 3 . Speaking in an interview , it was stated that the development team considered Valkyria Chronicles III to be the series \\' first true sequel : while Valkyria Chronicles II had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of Valkyria Chronicles II due to being on the same platform . In addition to Sega staff from the previous games , development work was also handled by <unk> The original scenario was written Kazuki Yamanobe , while the script was written by Hiroyuki Fujii , Koichi Majima , <unk> Miyagi , Seiki <unk> and Takayuki <unk> . Its story was darker and more somber than that of its predecessor . \\n',\n",
       " \" The majority of material created for previous games , such as the <unk> system and the design of maps , was carried over . Alongside this , improvements were made to the game 's graphics and some elements were expanded , such as map layouts , mission structure , and the number of playable units per mission . A part of this upgrade involved creating unique polygon models for each character 's body . In order to achieve this , the cooperative elements incorporated into the second game were removed , as they took up a large portion of memory space needed for the improvements . They also adjusted the difficulty settings and ease of play so they could appeal to new players while retaining the essential components of the series ' gameplay . The newer systems were decided upon early in development . The character designs were done by <unk> Honjou , who had worked on the previous Valkyria Chronicles games . When creating the Nameless Squad , Honjou was faced with the same problem he had had during the first game : the military uniforms essentially destroyed character individuality , despite him needing to create unique characters the player could identify while maintaining a sense of reality within the Valkyria Chronicles world . The main color of the Nameless was black . As with the previous Valkyria games , Valkyria Chronicles III used the <unk> graphics engine . The anime opening was produced by Production I.G. \\n\",\n",
       " '',\n",
       " ' = = = Music = = = \\n',\n",
       " '',\n",
       " ' The music was composed by Hitoshi Sakimoto , who had also worked on the previous Valkyria Chronicles games . When he originally heard about the project , he thought it would be a light tone similar to other Valkyria Chronicles games , but found the themes much darker than expected . An early theme he designed around his original vision of the project was rejected . He <unk> the main theme about seven times through the music production due to this need to reassess the game . The main theme was initially recorded using orchestra , then Sakimoto removed elements such as the guitar and bass , then adjusted the theme using a synthesizer before redoing segments such as the guitar piece on their own before incorporating them into the theme . The rejected main theme was used as a hopeful tune that played during the game \\'s ending . The battle themes were designed around the concept of a \" modern battle \" divorced from a fantasy scenario by using modern musical instruments , constructed to create a sense of atonality . While Sakimoto was most used to working with synthesized music , he felt that he needed to incorporate live instruments such as orchestra and guitar . The guitar was played by Mitsuhiro Ohta , who also arranged several of the later tracks . The game \\'s opening theme song , \" If You Wish for ... \" ( <unk> , <unk> Kimi ga <unk> Nara ) , was sung by Japanese singer May \\'n . Its theme was the reason soldiers fought , in particular their wish to protect what was precious to them rather than a sense of responsibility or duty . Its lyrics were written by Seiko Fujibayashi , who had worked on May \\'n on previous singles . \\n',\n",
       " '',\n",
       " ' = = = Release = = = \\n',\n",
       " '',\n",
       " \" In September 2010 , a teaser website was revealed by Sega , hinting at a new Valkyria Chronicles game . In its September issue , Famitsu listed that Senjō no Valkyria 3 would be arriving on the PlayStation Portable . Its first public appearance was at the 2010 Tokyo Game Show ( TGS ) , where a demo was made available for journalists and attendees . During the publicity , story details were kept scant so as not to spoil too much for potential players , along with some of its content still being in flux at the time of its reveal . To promote the game and detail the story leading into the game 's events , an episodic Flash visual novel written by Fujii began release in January 2011 . The game was released January 27 , 2011 . During an interview , the development team said that the game had the capacity for downloadable content ( DLC ) , but that no plans were finalized . Multiple DLC maps , featuring additional missions and recruitable characters , were released between February and April 2011 . An expanded edition of the game , Valkyria Chronicles III Extra Edition , released on November 23 , 2011 . Packaged and sold at a lower price than the original , Extra Edition game with seven additional episodes : three new , three chosen by staff from the game 's DLC , and one made available as a pre @-@ order bonus . People who also owned the original game could transfer their save data between versions . \\n\",\n",
       " \" Unlike its two predecessors , Valkyria Chronicles III was not released in the west . According to Sega , this was due to poor sales of Valkyria Chronicles II and the general unpopularity of the PSP in the west . An unofficial fan translation patch began development in February 2012 : players with a copy of Valkyria Chronicles III could download and apply the patch , which translated the game 's text into English . Compatible with the Extra Edition , the patch was released in January 2014 . \\n\",\n",
       " '',\n",
       " ' = = Reception = = \\n',\n",
       " '',\n",
       " ' On its day of release in Japan , Valkyria Chronicles III topped both platform @-@ exclusive and multi @-@ platform sales charts . By early February , the game sold 102 @,@ 779 units , coming in second overall to The Last Story for the Wii . By the end of the year , the game had sold just over 152 @,@ 500 units . \\n',\n",
       " ' Famitsu enjoyed the story , and were particularly pleased with the improvements to gameplay . Japanese gaming site Game Watch Impress , despite negatively noting its pacing and elements recycled from previous games , was generally positive about its story and characters , and found its gameplay entertaining despite off @-@ putting difficulty spikes . 4Gamer.net writer Naohiko <unk> , in a \" Play Test \" article based on the game \\'s PSN demo , felt that Valkyria Chronicles III provided a \" profound feeling of closure \" for the Valkyria Chronicles series . He praised its gameplay despite annoying limitations to aspects such as special abilities , and positively noted its shift in story to a tone similar to the first game . \\n',\n",
       " \" PlayStation Official Magazine - UK praised the story 's blurring of Gallia 's moral standing , art style , and most points about its gameplay , positively noting the latter for both its continued quality and the tweaks to balance and content . Its one major criticism were multiple difficulty spikes , something that had affected the previous games . Heath Hindman of gaming website PlayStation Lifestyle praised the addition of non @-@ linear elements and improvements or removal of mechanics from Valkyria Chronicles II in addition to praising the returning gameplay style of previous games . He also positively noted the story 's serious tone . Points criticized in the review were recycled elements , awkward cutscenes that seemed to include all characters in a scene for no good reason , pacing issues , and occasional problems with the game 's AI . \\n\",\n",
       " \" In a preview of the TGS demo , Ryan Geddes of IGN was left excited as to where the game would go after completing the demo , along with enjoying the improved visuals over Valkyria Chronicles II . Kotaku 's Richard Eisenbeis was highly positive about the game , citing is story as a return to form after Valkyria Chronicles II and its gameplay being the best in the series . His main criticisms were its length and gameplay repetition , along with expressing regret that it would not be localized . \\n\",\n",
       " '',\n",
       " ' = = Legacy = = \\n',\n",
       " '',\n",
       " ' Kurt and Riela were featured in the Nintendo 3DS crossover Project X Zone , representing the Valkyria series . Media.Vision would return to the series to develop Valkyria : Azure Revolution , with Ozawa returning as director . Azure Revolution is a role @-@ playing video game for the PlayStation 4 that forms the beginning of a new series within the Valkyria franchise . \\n',\n",
       " '',\n",
       " ' = = = Adaptations = = = \\n',\n",
       " '',\n",
       " ' Valkyria Chronicles 3 was adapted into a two @-@ episode original video animation series in the same year of its release . Titled Senjō no Valkyria 3 : Taga Tame no <unk> ( <unk> <unk> , lit . Valkyria of the Battlefield 3 : The Wound Taken for Someone \\'s Sake ) , it was originally released through PlayStation Network and <unk> between April and May 2011 . The initially @-@ planned release and availability period needed to be extended due to a stoppage to PSN during the early summer of that year . It later released for DVD on June 29 and August 31 , 2011 , with separate \" Black \" and \" Blue \" editions being available for purchase . The anime is set during the latter half of Valkyria Chronicles III , detailing a mission by the Nameless against their Imperial rivals Calamity Raven . The anime was first announced in November 2010 . It was developed by A @-@ 1 Pictures , produced by Shinji <unk> , directed by Nobuhiro Kondō , and written by Hiroshi <unk> . Sakimoto \\'s music for the game was used in the anime . \\n',\n",
       " ' The anime \\'s title was inspired by the principle purpose of the Nameless : to suffer in battle for the goals of others . A subtitle attached to the project during development was \" The Road to Kubinka \" , which referenced the Kubinka Tank Museum in Moscow . The game \\'s main theme was how the characters regained their sense of self when stripped of their names and identities , along with general themes focused on war and its consequences . While making the anime , the production team were told by Sega to make it as realistic as possible , with the consequence that the team did extensive research into aspects such as what happened when vehicles like tanks were overturned or damaged . Due to it being along the same timeline as the original game and its television anime adaptation , the cast of Valkyria Chronicles could make appearances , which pleased the team . The opening theme , \" Akari ( Light ) <unk> \" ( <unk> @-@ <unk> ) , was sung by Japanese singer <unk> . The ending theme , \" Someday the Flowers of Light Will Bloom \" ( <unk> , Itsuka Saku Hikari no Hana ) , was sung by Minami Kuribayashi . Both songs \\' lyrics were written by their respective artists . \\n',\n",
       " \" Two manga adaptations were produced , following each of the game 's main female protagonists Imca and Riela . They were Senjō no Valkyria 3 : Namo <unk> <unk> no Hana ( 戦場のヴァルキュリア3 <unk> , lit . Valkyria of the Battlefield 3 : The Flower of the Nameless Oath ) , illustrated by Naoyuki Fujisawa and eventually released in two volumes after being serialized in Dengeki Maoh between 2011 and 2012 ; and Senjō no Valkyria 3 : <unk> Unmei no <unk> <unk> ( 戦場のヴァルキュリア3 <unk> , lit . Valkyria of the Battlefield 3 -The Valkyrie of the Crimson Fate ) , illustrated by Mizuki Tsuge and eventually released in a single volume by Kadokawa Shoten in 2012 . \\n\",\n",
       " '',\n",
       " '',\n",
       " ' = Tower Building of the Little Rock Arsenal = \\n',\n",
       " '',\n",
       " \" The Tower Building of the Little Rock Arsenal , also known as U.S. Arsenal Building , is a building located in MacArthur Park in downtown Little Rock , Arkansas . Built in 1840 , it was part of Little Rock 's first military installation . Since its decommissioning , The Tower Building has housed two museums . It was home to the Arkansas Museum of Natural History and Antiquities from 1942 to 1997 and the MacArthur Museum of Arkansas Military History since 2001 . It has also been the headquarters of the Little Rock Æsthetic Club since 1894 . \\n\",\n",
       " ' The building receives its name from its distinct octagonal tower . Besides being the last remaining structure of the original Little Rock Arsenal and one of the oldest buildings in central Arkansas , it was also the birthplace of General Douglas MacArthur , who became the supreme commander of US forces in the South Pacific during World War II . It was also the starting place of the Camden Expedition . In 2011 it was named as one of the top 10 attractions in the state of Arkansas by <unk> \\n',\n",
       " '',\n",
       " ' = = Construction = = \\n',\n",
       " '',\n",
       " ' The arsenal was constructed at the request of Governor James Sevier Conway in response to the perceived dangers of frontier life and fears of the many Native Americans who were passing through the state on their way to the newly established Oklahoma Territory . Thirty @-@ six acres were appropriated on the outskirts of Little Rock by Major Robert B. Lee of the U.S. Army . The land had been previously used as a racetrack by the local jockey club . John Wormley Walker , a builder for the Federal Government , supervised the construction . Originally $ 14 @,@ 000 was allocated for the construction of the arsenal , but proved inadequate . The budget was later increased to $ 30 @,@ 000 . Work began on the Tower Building in 1840 , and it was the first permanent structure of the arsenal to be built . Being originally constructed to store ammunition , the building was designed with 3 @-@ foot @-@ thick ( 0 @.@ 91 m ) exterior walls . The original plans called for it to be built of stone , however , masonry was used instead . The Arkansas Gazette referred to the structure as \" A splendid specimen of masonry \" . \\n',\n",
       " '',\n",
       " ' = = Civil War = = \\n',\n",
       " '',\n",
       " ' For several years the arsenal , which was owned by the federal government , served as a simple arms depot and was staffed with only a handful of soldiers . But in November 1860 , with the American Civil War on the horizon , a company of the Second United States Artillery , consisting of sixty @-@ five men , was transferred to Little Rock under the command of Captain James Totten . On January 15 , 1861 , the state legislature decided to hold a referendum to determine if a state convention should be held to consider the issue of secession and to elect delegates to such a convention . It was planned for February 18 ; however , events at the arsenal , would not wait . On January 28 , then Governor Henry Massey Rector informed Captain Totten that he and his soldiers would be \" permitted to remain in the possession of the Federal officers until the State , by authority of the people , shall have determined to sever their connection with the General Government , \" Totten responded to this by telling the Governor that his orders came from the United States Government and began a desperate but ultimately futile dispatch of letters and telegrams asking for reinforcements , although rumors were widely spread that they were already coming . The first telegraph wire to span between Little Rock and Memphis had recently been completed . Local attorney John M Harrel was asked to compose the first telegraph dispatched from Arkansas \\'s capital . In his message , Harrel reported unconfirmed rumors that more federal troops had been sent to reinforce the Little Rock Arsenal . \\n',\n",
       " ' The United States troops at the outposts of the western frontier of the state and in the Indian nation have all been recalled from winter quarters to reinforce the garrison at Fort Smith . The garrison at Fort Smith had been previously transferred to the United States Arsenal in this city ( Little Rock ) . The arsenal is one of the richest depositories of military stores in the United States and is supposed to be the ultimate destination of the <unk> [ sic ] ordered from the frontier . \\n',\n",
       " ' <unk> M Harrel Telegram , January 31 , 1861 \\n',\n",
       " ' The item was intended simply as a piece of news , but telegraph lines quickly spread the news throughout the state , fueling procession sentiment . The rumor was interpreted by some Arkansans as a call from the governor to assemble to help expel the federal troops from the arsenal . By February 5 , six militia units , consisting of 1 @,@ 000 men , with a guarantee that the numbers could be increased to 5 @,@ 000 if the situations deemed it necessary , had assembled in Little Rock . Governor Rector vehemently denied ordering the troops to assemble or giving any order at all in connection with the troops . Faced with the fact that the military had assembled believing they were following his orders and the consensus of the citizens of Little Rock against any armed conflict between the civilian army and federal troops , Governor Rector was forced to take control of the situation . On February 6 , he sent a formal demand for surrender of the arsenal to Captain Totten , \\n',\n",
       " ' This movement is prompted by the feeling that pervades the citizens of this State that in the present emergency the arms and munitions of war in the Arsenal should be under the control of the State authorities , in order to their security . This movement , although not authorized by me , has assumed such an aspect that it becomes my duty , as the executive of this <unk> , to interpose my official authority to prevent a collision between the people of the State and the Federal troops under your command . I therefore demand in the name of the State the delivery of the possession of the Arsenal and munitions of war under your charge to the State authorities , to be held subject to the action of the convention to be held on the 4th of March next . \\n',\n",
       " ' Perhaps because Abraham Lincoln had not yet been inaugurated as President , Captain Totten received no instructions from his superiors and was forced to withdraw his troops . He agreed to surrender the arsenal as long as the governor agreed to three provisions : \\n',\n",
       " ' The governor would take possession of the arsenal in the name of the United States . \\n',\n",
       " ' The soldiers would be allowed safe passage in any direction carrying any personal and public property besides munitions of war . \\n',\n",
       " ' The soldiers would be allowed to march away as men leaving under orders , not as conquered and surrendering soldiers . \\n',\n",
       " \" On the morning of February 8 , 1861 , Rector and Totten signed an agreement placing the arsenal in the hands of state officials . That afternoon , the citizen militia marched to the arsenal with Governor Rector at its head . All of the federal troops had left at this point , except Totten who had stayed behind to listen to the Governor 's speech and to hand the arsenal over in person . \\n\",\n",
       " ' The Little Rock Arsenal was classified in 1860 as an \" arsenal of deposit , \" meaning that it was simply a warehouse for the storage of weapons intended for the use of the state militia in times of crisis . Thus there were no substantial operations for ordnance fabrication or repairs , nor for the manufacture of cartridges at the time the Arsenal fell into State hands . Most of these operations were started from scratch through the efforts of the Arkansas Military Board . \\n',\n",
       " \" Inside the Little Rock Arsenal after its seizure in February , 1861 , the Confederates inventoried some 10 @,@ 247 weapons , 250 @,@ 000 musket cartridges , and 520 @,@ 000 percussion caps , as well as the four bronze cannon of Totten 's battery . Long arms in the Arsenal 's inventory consisted of : \\n\",\n",
       " ' M1822 .69 cal ( flintlock ) 5 @,@ 625 \\n',\n",
       " ' M1822 .69 cal ( percussion @-@ converted ) 53 \\n',\n",
       " ' <unk> .69 cal smoothbore ( percussion ) 357 \\n',\n",
       " ' <unk> .58 cal rifle @-@ muskets 900 \\n',\n",
       " ' <unk> common rifles 125 \\n',\n",
       " ' <unk> rifle ( \" Mississippi Rifle \" ) 54 \\n',\n",
       " ' <unk> <unk> 2 \\n',\n",
       " \" Hall 's carbines 267 \\n\",\n",
       " \" Hall 's rifles ( flintlock ) 2 @,@ 864 \\n\",\n",
       " ' Total 10 @,@ 247 \\n',\n",
       " ' Of this number , approximately 9600 weapons were serviceable , or ready @-@ for @-@ issue . Note there were only 1 @,@ 364 percussion weapons available . Disposition of the weapons found in the Arsenal is somewhat sketchy , but from various records it can be surmised that the 5th , 6th , 7th , and 8th Arkansas Infantry Regiments , mustered in June , 1861 , were issued <unk> / M1822 .69 caliber flintlocks . The 9th and 10th Arkansas , four companies of Kelly \\'s 9th Arkansas Battalion , and the 3rd Arkansas Cavalry Regiment were issued flintlock Hall \\'s Rifles . The units comprising the infantry force of Van Dorn \\'s Army of the West were the 1st and 2nd Arkansas Mounted Rifles were also armed with M1822 flintlocks from the Little Rock Arsenal . By the time the 11th and 12th Arkansas Infantry Regiments mustered in at Little Rock , the supply of arms had been almost completely exhausted , and only old \" junker \" weapons were left . \\n',\n",
       " ' Most of the equipment , arms , and machinery at the Little Rock Arsenal was removed to east of the Mississippi River by order of Maj. Gen. Earl Van Dorn in April and May 1862 , and accountability for it is lost at that point . By all appearances , the equipment was sent down the river to Napoleon , Arkansas , and from there to Jackson Mississippi , where it was probably destroyed during the Vicksburg campaign in the early summer of 1863 . \\n',\n",
       " ' Major General Thomas C. Hindman , sent to command the district of Arkansas in May , 1862 , found the state nearly destitute of military material . Hindman established another armory at Arkadelphia , and revived the Little Rock Arsenal as a collection point and depot for armaments and ammunition manufacture for small arms . Hindman recorded : \\n',\n",
       " ' \" Machinery was made for manufacturing percussion caps and small arms , and both were turned out in small quantity , but of excellent quality . Lead mines were opened and worked , and a chemical laboratory was established and successfully operated in aid of the Ordnance Department and in the manufacture of calomel , castor oil , spirits of nitre , the various tinctures of iron , and other valuable medicines . Most of these works were located at or near Arkadelphia on the Ouachita River , 75 miles south from Little Rock . The tools , machinery , and the material were gathered piecemeal or else made by hand labor . Nothing of this sort had been before attempted on Government account in Arkansas to my knowledge , except for the manufacture of small arms , the machinery for which was taken away by General Van Dorn and there was neither capital nor sufficient enterprise among the citizens to engage in such undertakings <unk> A further supply , along with lead and caps , was procured from the citizens of Little Rock and vicinity by donation , purchases , and impressments . \\n',\n",
       " ' This ammunition , and that which I brought with me , was rapidly prepared for use at the Laboratory established at the Little Rock Arsenal for that purpose . As illustrating as the pitiful scarcity of material in the country , the fact may be stated that it was found necessary to use public documents of the State Library for cartridge paper . <unk> were employed or conscripted , tools purchased or impressed , and the repair of the damaged guns I brought with me and about an equal number found at Little Rock commenced at once . But , after inspecting the work and observing the spirit of the men I decided that a garrison 500 strong could hold out against Fitch and that I would lead the remainder - about 1500 - to Gen \\'l Rust as soon as shotguns and rifles could be obtained from Little Rock instead of pikes and lances , with which most of them were armed . Two days elapsed before the change could be effected . \" \\n',\n",
       " ' The Confederate ordnance establishment at Little Rock was reactivated in August , 1862 . Looking around for a suitable person to head this activity , General Hindman turned to the Confederate Navy and borrowed Lieutenant John W. Dunnington . Lt. Dunnington was the commander of the gunboat C.S.S. Ponchartrain , which had been brought to Little Rock in hopes of converting it to an ironclad . Dunnington was selected to head the ordnance works at Little Rock , and although he continued to draw his pay from the Confederate Navy Department , he was placed in charge of all Confederate ordnance activities ( which included artillery functions ) there with the rank of lieutenant colonel . \\n',\n",
       " ' Lt. Col. Dunnington \\'s \" Returns for the month of August , 1862 , at Little Rock Arsenal , C.S.A. , \" are found in Vol . 149 , Chapter IV of the \" Captured Rebel Ordnance Records , \" and are most enlightening as to the scope of Confederate ordnance activities at Little Rock during this crucial time . According to Dunnington , \" When I assumed command at this Post , all material had been removed to Arkadelphia . There were no persons employed . No shops were open for repair of arms or for fabricating ammunition . Material , tools , etc . , had to be procured as well as the employment of laborers . Work commenced the last part of the month . \" \\n',\n",
       " \" The military force at Little Rock under Dunnington 's command consisted of four officers : himself , Major John B. Lockman , Captain C.C. Green , and 2nd Lt. W.W. Murphy . In addition to these , he had 20 enlisted men and a civilian force composed of a foreman , 2 clerks , 3 gunsmiths for repairing small arms , a <unk> , 26 laborers in the ammunition laboratory , and a carpenter for making packing boxes . \\n\",\n",
       " ' During the month of August , 1862 , the following work was performed : \" <unk> : one pair of musket bullet moulds ; 10 @,@ 000 buck & ball shot cartridges ; repaired : 750 muskets , shotguns , and rifles ; received and repaired : ordnance stores and <unk> ; performed : guard , office , and police duties ; inspected : Posts at Camden and Arkadelphia . \" \\n',\n",
       " ' Lt. Col. Dunnington continued to build up his works at Little Rock until November 1862 , when Captain Sanford C. Faulkner ( composer of The Arkansas Traveler ) was placed in charge of the Arsenal . Dunnington presumably returned to his naval duties and the Ponchartrain . \\n',\n",
       " ' A \" Summary of the Work Done for November , 1862 , Little Rock Arsenal \" shows : Fabrication : \\n',\n",
       " ' 75 @,@ 000 buck & ball cartridges - percussion \\n',\n",
       " ' 14 @,@ 000 buck & ball cartridges - flint \\n',\n",
       " ' 275 paper fuzes \\n',\n",
       " ' 117 rounds , 6 @-@ pounder canister shot \\n',\n",
       " ' 130 rounds , 6 @-@ pounder ball shot \\n',\n",
       " ' 96 ammunition packing boxes \\n',\n",
       " ' Repaired : \\n',\n",
       " ' 2 @,@ 236 shotguns and rifles ( repaired mostly for troops in service ) \\n',\n",
       " ' 23 pistols ( repaired mostly for troops in service ) \\n',\n",
       " ' Received & Issued : \\n',\n",
       " ' 752 packages of ordnance and ordnance stores received and mostly issued to troops in service . \\n',\n",
       " ' Repaired and painted : \\n',\n",
       " ' 4 gun carriages \\n',\n",
       " ' Performed : \\n',\n",
       " ' Guard , office , and police duties . \\n',\n",
       " ' Perhaps the most illuminating points of the above \" Summary of Work \" and those for following months are that the standard ammunition made was . \" buck & ball \" , indicating that the .69 caliber smoothbores and shotguns remained the predominant caliber weapon in use , and of this , nearly one sixth or more of all small arms ammunition was still for flintlock weapons , indicating that no less than a sixth of the Confederate troops in this vicinity were still armed with obsolete flintlock weapons . \\n',\n",
       " ' The \" Summaries of Work done at Little Rock Arsenal , C.S.A. \" continue at about the same pace and scale from August 1862 until August 1863 . <unk> to the \" Summary \" for August , 1863 is the ominous notation , \" During the last week in the month , nearly all stores at the Arsenal have been packed and sent to Arkadelphia , in obedience to orders from Chief of Ordnance , District of Arkansas . \" This then marks the beginning of the evacuation of ordnance activities from Little Rock , with the city being surrendered to the advancing Federal troops of Frederick Steele \\'s Arkansas Expedition on September 11 , 1863 . \\n',\n",
       " ' In 1864 , after Little Rock fell to the Union Army and the arsenal had been recaptured , General Fredrick Steele marched 8 @,@ 500 troops from the arsenal beginning the Camden Expedition . \\n',\n",
       " ' The arsenal was briefly seized once more by Joseph Brooks loyalists during the Brooks @-@ Baxter War of 1874 . \\n',\n",
       " '',\n",
       " ' = = Decommissioning = = \\n',\n",
       " '',\n",
       " ' In 1873 , the building was renamed Little Rock Barracks and used as a barracks for married officers and their families . The building was drastically altered the inside and outside . Prior to renovation , a rear basement door provided the only entrance to the building , while the tower served as a hoist to move munitions between floors . By 1868 , front and rear porches had been added to the building , as well as interior walls and stairs , some of which remain today , including the central staircase . In 1880 , Douglas MacArthur was born on the northwest upper floor of this building while his father , Captain Arthur MacArthur , was stationed there . \\n',\n",
       " ' In the 1880s , the federal government began closing many small arsenals around the country in favor of smaller ones built near railroads for quick deployment . The arsenal commander received word from Washington that the Little Rock site must be abandoned \" not later than October 1 , 1890 . \" On April 12 , 1893 the tower building and the surrounding buildings were traded to the city of Little Rock for 1 @,@ 000 acres ( 4 km ² ) in North Little Rock under the condition that the building and land be \" forever exclusively devoted to the uses and purposes of a public park \" for 1 @,@ 000 acres ( 4 km ² ) in Big Rock Mountain on the north side of the Arkansas River , present day North Little Rock . That site later became Fort Logan H. Roots . All of the original buildings surrounding the Tower Building were demolished . \\n',\n",
       " '',\n",
       " ' = = Æsthetic Club = = \\n',\n",
       " '',\n",
       " ' In 1894 the Little Rock Æsthetic Club , one of the oldest women \\'s societies west of the Mississippi River , moved into the Tower Building . This was prompted due to increased membership and a need for larger , more permanent quarters . The previous year , club members working with women \\'s organizations throughout the state , raised money to furnish the Arkansas Building of the Columbian Exposition at The Chicago World \\'s Fair . At the fair \\'s conclusion , artifacts from the exhibit were displayed in the Tower Building , with the Æsthetic Club invited to meet in the \" Columbian Room . \" \\n',\n",
       " ' Except for Æsthetic Club meetings , the Tower Building remained largely unoccupied for almost fifty years and suffered significant deterioration . The Æsthetic Club provided much @-@ needed financial support during the period and even paid the electric bill during the Great Depression . The Æsthetic Club is still headquartered in the Tower Building . \\n',\n",
       " '',\n",
       " ' = = Public use = = \\n',\n",
       " '',\n",
       " \" The building and the surrounding park were used for many public purposes throughout the early 20th century . The Tower Building served as headquarters for the United Confederate Veterans Reunion , May 15 – 18 , 1911 . Over 106 @,@ 000 Civil War veterans , the largest popular gathering in the history of the city up to that time , attended and were housed in the building or camped in the park , which had also become a popular camping area . Later the building served as an armory for the Arkansas National Guard . In 1912 , the second floor of the Tower Building became Little Rock 's first public library . In 1917 , Little Rock built a fire station in the park , that building is now gone . A band shell named for H. H. Foster also was built in the park during this time , but also no longer exists . In 1936 , Works Progress Administration built the Museum of Fine Arts , now called the Arkansas Arts Center , just south of the Tower Building . \\n\",\n",
       " ' The arsenal was listed in the National Register of Historic Places in 1970 . Due to its association with the Camden Expedition of 1864 , the arsenal may be included in the Camden Expedition Sites National Historic Landmark designated in 1994 . \\n',\n",
       " ' In 1942 , the Tower Building was renovated due to the efforts of the Æsthetic Club , Little Rock philanthropist Frederick W. Allsop , and the Works Progress Administration . It became the new home of The Arkansas Museum of Natural History and Antiquities , which had been located in Little Rock City Hall . The museum remained in the tower building for approximately fifty @-@ five years . The area surrounding the Tower Building had been known as Arsenal Park when the first decommissioned and then later renamed City Park . Due to the efforts of Bernie Babcock , however , the city finally named it MacArthur Park in 1942 in honor of Douglas MacArthur . \\n',\n",
       " \" In 1997 , the Museum of Science and Natural History merged with the Little Rock Children 's Museum , which had been located in Union Station , to form the Arkansas Museum of Discovery . The new museum was relocated to a historic building in the Little Rock River Market District . The MacArthur Museum of Arkansas Military History opened on May 19 , 2001 in the Tower Building . The new museum 's goal is to educate and inform visitors about the military history of Arkansas , preserve the Tower Building , honor servicemen and servicewomen of the United States and commemorate the birthplace of Douglas MacArthur . \\n\",\n",
       " '',\n",
       " '',\n",
       " ' = Cicely Mary Barker = \\n',\n",
       " '',\n",
       " \" Cicely Mary Barker ( 28 June 1895 – 16 February 1973 ) was an English illustrator best known for a series of fantasy illustrations depicting fairies and flowers . Barker 's art education began in girlhood with correspondence courses and instruction at the Croydon School of Art . Her earliest professional work included greeting cards and juvenile magazine illustrations , and her first book , Flower Fairies of the Spring , was published in 1923 . Similar books were published in the following decades . \\n\",\n",
       " \" Barker was a devout Anglican , and donated her artworks to Christian fundraisers and missionary organizations . She produced a few Christian @-@ themed books such as The Children ’ s Book of Hymns and , in collaboration with her sister Dorothy , He Leadeth Me . She designed a stained glass window for St. Edmund 's Church , Pitlake , and her painting of the Christ Child , The Darling of the World Has Come , was purchased by Queen Mary . \\n\",\n",
       " \" Barker was equally proficient in watercolour , pen and ink , oils , and pastels . Kate Greenaway and the Pre @-@ Raphaelites were the principal influences on her work . She claimed to paint instinctively and rejected artistic theories . Barker died in 1973 . Though she published Flower Fairy books with spring , summer , and autumn themes , it wasn 't until 1985 that a winter collection was assembled from her remaining work and published posthumously . \\n\",\n",
       " '',\n",
       " ' = = Biography = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = Early life = = = \\n',\n",
       " '',\n",
       " ' Barker was born the second daughter and youngest child of Walter Barker , a partner in a seed supply company and an amateur artist , and his wife Mary Eleanor ( Oswald ) Barker on 28 June 1895 at home at 66 Waddon Road in Croydon , Surrey , England . Barker was an epileptic as a child , and cared for at home by her parents . Later , her sister and elder by two years , Dorothy Oswald Barker , continued the care . \\n',\n",
       " ' The family of four was moderately well off , and belonged to the lower end of the upper middle class . A nanny , a governess , and a cook to prepare special meals for Barker were hired . She spent much time in bed at home amusing herself with painting books and a nursery library that included the works of Kate Greenaway and Randolph Caldecott – two artists who exerted strong influences on her later art . \\n',\n",
       " '',\n",
       " ' = = = Art education and first professional work = = = \\n',\n",
       " '',\n",
       " ' Barker took correspondence courses in art , probably until about 1919 . In 1908 at 13 years , she entered an evening class at the Croydon School of Art , and attended the school into the 1940s . In time , she received a teaching position . \\n',\n",
       " ' In 1911 , Raphael Tuck & Sons bought four of Barker \\'s \" little drawings \" for half a sovereign , and published them as postcards . In October 1911 , she won second prize in the Croydon Art Society \\'s poster competition , and shortly afterward was elected the youngest member of the Society . The art critic for the Croydon Advertiser remarked , \" Her drawings show a remarkable freedom of spirit . She has distinct promise . \" \\n',\n",
       " \" Following her father ’ s death in June 1912 , the seventeen @-@ year @-@ old Barker submitted art and poetry to My Magazine , Child ’ s Own , Leading Strings , and Raphael Tuck annuals in an effort to support both her mother and sister . Her sister Dorothy taught kindergarten in two private schools before opening a kindergarten at home . She brought in some money for the family 's support while supervising the household . \\n\",\n",
       " '',\n",
       " ' = = = Flower Fairies of the Spring , 1923 = = = \\n',\n",
       " '',\n",
       " ' Fairies became a popular theme in art and literature in the early 20th century following the releases of The Coming of the Fairies by Sir Arthur Conan Doyle , Peter Pan by J.M. Barrie , and the fairy @-@ themed work of Australian Ida <unk> Outhwaite . Queen Mary made such themes even more popular by sending Outhwaite postcards to friends during the 1920s . In 1918 , Barker produced a postcard series depicting elves and fairies . \\n',\n",
       " ' In 1923 , Barker sent her flower fairy paintings to various publishers . Blackie paid £ 25 for 24 paintings with accompanying verses , but it wasn \\'t until publication of Flower Fairies of the Summer in 1925 that Barker received royalties for her work . Mary Violet Clayton Calthrop , wife of author Dion Clayton Calthrop , wrote in April 1925 about Barker and Flower Fairies of the Spring : \" She has such exquisite taste , besides draughtsmanship . \" \\n',\n",
       " '',\n",
       " ' = = = The Waldrons = = = \\n',\n",
       " '',\n",
       " ' In 1924 , the family moved into a four @-@ level , semi @-@ detached Victorian house at 23 The Waldrons . Barker had a studio built in the garden and her sister conducted a kindergarten in a room at the back of the house . The family lived frugally and attended both St. Edmund \\'s and St. Andrew \\'s in Croydon – \" low \" churches for the less privileged . Barker sometimes incorporated portraits of her fellow parishioners in her religious works . She was described by Canon Ingram Hill as \" one of the pillars \" of St. Andrew \\'s . \\n',\n",
       " ' The children in the kindergarten modelled for the Flower Fairies until the kindergarten closed in 1940 . In an interview in 1958 , Barker said , \" My sister ran a kindergarten and I used to borrow her students for models . For many years I had an atmosphere of children about me – I never forgot it . \" She also painted the children of relatives as well as Gladys Tidy , the Barkers \\' young housekeeper , who posed for the Primrose Fairy in 1923 . The plants were painted from life , and if a specimen was not readily at hand , Kew Gardens staff would provide her the specimens needed . Barker designed and built the Flower Fairy costumes , and based each on the flowers and leaves of the particular plant to be illustrated . The costumes were kept in a trunk in her studio along with wings made of twigs and gauze . Each was broken down after an illustration was completed and the parts recycled for other costumes . She often referred to Dion Clayton Calthrop \\'s English Costume . \\n',\n",
       " '',\n",
       " ' = = = Middle years = = = \\n',\n",
       " '',\n",
       " ' In the late 1920s , Barker began to doubt she was doing enough for the church and considered focusing solely on sacred works . Family and friends recommended she continue secular and sacred works , which she did . \\n',\n",
       " ' Barker continued to attend evening classes at the Croydon Art School between the 1920s and the 1940s , eventually receiving a teaching position . She took sketching trips to Amberley and Storrington in Sussex and to Cornwall and the southern coast with family and friends . She visited and stayed with artist Margaret Tarrant in Gomshall , Surrey and with family in <unk> , Near Whitby , North Yorkshire . \\n',\n",
       " \" In 1940 , the Barker 's live @-@ in maid retired , and Dorothy Barker closed her school at the back of the house in The Waldrons . She continued to supervise the household , and to give both her mother and sister the care they needed . Dorothy and her sister collaborated upon only two books : Our Darling 's First Book and the Christian @-@ themed , He Leadeth Me . In 1954 Dorothy Barker died of a heart attack . Barker was unable to pursue her art to any significant extent following her sister 's death , as all the care of her aged mother devolved upon her , but she did manage to begin planning a stained glass window design in her sister 's memory for St. Edmund 's , Pitlake . \\n\",\n",
       " '',\n",
       " ' = = = Later life and death = = = \\n',\n",
       " '',\n",
       " \" Barker 's mother died in 1960 , and , in 1961 , Barker moved from 23 The Waldrons to 6 <unk> Avenue in Croydon . She restored a maisonette in Storrington , Sussex , England , bequeathed by her friend Edith Major , and named it St. Andrew 's . After taking up residence , her health began to deteriorate . She was in and out of nursing and convalescent homes , and tended by relatives and friends . \\n\",\n",
       " \" Barker died at Worthing Hospital on 16 February 1973 , aged 77 years . Two funeral services were held – one in Storrington Church and one in Barker 's maisonette . Her ashes were scattered in Storrington churchyard . In 1989 , Frederick Warne , a division of Penguin Books since 1983 , acquired the Flower Fairies properties . \\n\",\n",
       " '',\n",
       " ' = = Art = = \\n',\n",
       " '',\n",
       " ' Barker worked principally in watercolor with pen @-@ and @-@ ink , but she was equally competent in black @-@ and @-@ white , in oils , and in pastels . She carried a sketchbook with her for capturing interesting children . She once indicated , \" I have always tried to paint instinctively in a way that comes naturally to me , without any real thought or attention to artistic theories . \" \\n',\n",
       " \" Kate Greenaway was a childhood favorite and an influence on her art . Barker 's child subjects wear nostalgic clothing as Greenaway 's children do , though Barker 's children are less melancholy and less flat in appearance , due perhaps to advances in printing technology . Barker studied flowers with an analytical eye and was friend to children 's illustrator , Margaret Tarrant . Along with Greenaway , illustrator Alice B. Woodward also influenced Barker 's work . \\n\",\n",
       " ' The Pre @-@ Raphaelites were a strong , lifelong influence on Barker . She once indicated , \" I am to some extent influenced by them — not in any technical sense , but in the choice of subject matter and the feeling and atmosphere they could achieve . \" She admitted a fondness for the early paintings of John Everett Millais and \" the wonderful things \" of Edward Burne @-@ Jones . \\n',\n",
       " '',\n",
       " ' = = = Depictions of children = = = \\n',\n",
       " '',\n",
       " \" Barker 's sketches , drawings , and paintings of children were given to friends or to the parents of the subjects , donated to charitable institutions and church sponsored events , or exhibited through various art organizations . She illustrated magazine covers , dust jackets , and produced series of postcards for Raphael Tuck and other publishers such as Picturesque Children of the Allies ( 1915 ) , Seaside Holidays ( 1918 ) , and Shakespeare 's Boy and Girl Characters ( 1917 , 1920 ) . Her own Old Rhymes for All Times ( 1928 ) and The Lord of the Rushie River ( 1938 ) , a tale about a girl who lives among swans on a riverbank , were critically well received . Set about 1800 , Groundsel and Necklaces ( 1943 ) tells of a girl named Jenny who rescues her family from poverty through the agency of the fairies . The story features an old Scrooge @-@ like man called Mr. <unk> and tonally suggests a Dickensian social consciousness . Simon the Swan , intended as a sequel to Rushie River was outlined in 1943 with Groundsel , but only developed in 1953 . It was published posthumously in 1988 and is critically considered less successful than Groundsel . \\n\",\n",
       " '',\n",
       " ' = = = Christian @-@ themed works = = = \\n',\n",
       " '',\n",
       " \" Barker was a devout Christian , and produced religious @-@ themed works throughout her life . She published eight postcards and five guardian angel birthday cards for the Society for Promoting Christian Knowledge in 1916 and in 1923 respectively . Christmas cards were designed for The Girls ' Friendly Society over a 20 @-@ year period , and the first three designs sold out a combined printing of 46 @,@ 500 in 1923 . An original design for the society called The Darling of the World Has Come was purchased by Queen Mary for ₤ 5 @.@ 5 @.@ 0 in 1926 . The Croydon Art Society hung Barker 's booklet cover design for the Society for the Propagation of the Gospel in its November 1919 exhibition . \\n\",\n",
       " \" Religious @-@ themed books include The Children 's Book of Hymns ( 1929 ) and He Leadeth Me ( 1933 ) , the latter written in collaboration with her sister . Major religious works include the triptychs in oil , The Feeding of the Five Thousand ( 1929 ) , for the chapel in Llandaff House , a home for destitute women at Penarth , Wales , and The Parable of the Great Supper ( 1934 ) for St. George 's Chapel , Waddon . The Feeding has since disappeared , and only a black @-@ and @-@ white photograph dated 1929 reproduces the work . In 1941 , she completed oil panels on the subject of the seven sacraments for the baptismal font at St. Andrew 's , South Croydon . She designed baptismal rolls for the wall behind the font in 1948 and 1962 . In 1946 , she completed the 4 x 7 ft. oil painting , Out of Great Tribulation , for the Memorial Chapel of Norbury Methodist Church . Following the death of her sister in 1954 , Barker began designs for a stained glass memorial window depicting Christ preparing to wash the feet of his disciples . Her last religious @-@ themed work , it was installed in St. Edmund 's , Pitlake , in 1962 . \\n\",\n",
       " '',\n",
       " ' = = Works = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = Cards = = = \\n',\n",
       " '',\n",
       " ' Picturesque Children of the Allies ; J. Salmon , 1916 \\n',\n",
       " ' National Mission ; Society for the Preservation of Christian Knowledge , 1916 \\n',\n",
       " \" Shakespeare 's Boy Characters ; C. W. Faulkner , 1917 \\n\",\n",
       " \" Shakespeare 's Girl Characters ; C. W. Faulkner , 1920 \\n\",\n",
       " ' Seaside Holiday ; J. Salmon , 1918 , 1921 \\n',\n",
       " ' Elves and Fairies ; S. Harvey , 1918 \\n',\n",
       " ' Guardian Angel ; Society for the Preservation of Christian Knowledge , 1923 \\n',\n",
       " \" Christmas cards ; Girls ' Friendly Society , 1920s , 1930s \\n\",\n",
       " ' Christmas cards ( US ) ; Barton @-@ Colton , 1920s , 1930s \\n',\n",
       " ' Beautiful Bible Pictures ; Blackie , 1932 \\n',\n",
       " '',\n",
       " ' = = = Books = = = \\n',\n",
       " '',\n",
       " ' Flower Fairies of the Spring ; Blackie , 1923 \\n',\n",
       " ' Spring Songs with Music ; Blackie , 1923 \\n',\n",
       " ' Flower Fairies of the Summer ; Blackie , 1925 \\n',\n",
       " ' Child Thoughts in Picture and Verse ( by M. K. Westcott ) ; Blackie , 1925 \\n',\n",
       " ' Flower Fairies of the Autumn ; Blackie , 1926 \\n',\n",
       " ' Summer Songs with Music ; Blackie , 1926 \\n',\n",
       " ' The Book of the Flower Fairies ; Blackie , 1927 \\n',\n",
       " ' Autumn Songs with Music ; Blackie , 1927 \\n',\n",
       " ' Old Rhymes for All Times ; Blackie , 1928 \\n',\n",
       " ' The Children ’ s Book of Hymns ; Blackie , 1929 ; rep . 1933 \\n',\n",
       " ' Our Darling ’ s First Book ( written in collaboration with Dorothy Barker ) ; Blackie , 1929 \\n',\n",
       " ' The Little Picture Hymn Book ; Blackie , 1933 \\n',\n",
       " ' Rhymes New and Old ; Blackie , 1933 \\n',\n",
       " ' A Flower Fairy Alphabet ; Blackie , 1934 \\n',\n",
       " ' A Little Book of Old Rhymes ; Blackie , 1936 \\n',\n",
       " ' He Leadeth Me ( written in collaboration with Dorothy Barker ) ; Blackie , 1936 \\n',\n",
       " ' A Little Book of Rhymes New and Old ; Blackie , 1937 \\n',\n",
       " ' The Lord of the Rushie River ; Blackie , 1938 \\n',\n",
       " ' Flower Fairies of the Trees ; Blackie , 1940 \\n',\n",
       " ' When Spring Came In at the Window ; Blackie , 1942 \\n',\n",
       " ' A Child ’ s Garden of Verses ( Robert Louis Stevenson ) ; Blackie , 1944 \\n',\n",
       " ' Flower Fairies of the Garden ; Blackie , 1944 \\n',\n",
       " ' Groundsel and Necklaces ; Blackie , 1946 ; reprinted as Fairy Necklaces \\n',\n",
       " ' Flower Fairies of the Wayside ; Blackie , 1948 \\n',\n",
       " ' Flower Fairies of the Flowers and Trees ; Blackie , 1950 \\n',\n",
       " ' Lively Stories ; Macmillan , 1954 \\n',\n",
       " ' The Flower Fairy Picture Book ; Blackie , 1955 \\n',\n",
       " ' Lively Numbers ; Macmillan , 1957 \\n',\n",
       " ' Lively Words ; Macmillan , 1961 . \\n',\n",
       " ' The Sand , the Sea and the Sun ; Gibson , 1970 \\n',\n",
       " '',\n",
       " ' = = = = Posthumously published = = = = \\n',\n",
       " '',\n",
       " ' Flower Fairies of the Winter ; Blackie , 1985 \\n',\n",
       " ' Simon the Swan ; Blackie , 1988 \\n',\n",
       " ' Flower Fairies of the Seasons ; <unk> / Blackie , 1988 \\n',\n",
       " ' A Little Book of Prayers and Hymns ; Frederick Warne , 1994 \\n',\n",
       " ' A Flower Fairies Treasury ; Frederick Warne , 1997 \\n',\n",
       " ' <unk> ; Frederick Warne , 2005 \\n',\n",
       " ' Wild Cherry Makes A Wish ; ( collaboration with Pippa Le Quesne ) Frederick Warne , 2006 \\n',\n",
       " ' How to find Flower Fairies ; Frederick Warne , 2007 \\n',\n",
       " ' Return to <unk> ; Frederick Warne , 2008 \\n',\n",
       " '',\n",
       " ' = = = Book covers = = = \\n',\n",
       " '',\n",
       " ' A New Epiphany ; Society for the Preservation of Christian Knowledge , 1919 \\n',\n",
       " ' 43 Annuals ; Blackie , 1920s , 1930s \\n',\n",
       " '',\n",
       " ' = = = Religious works = = = \\n',\n",
       " '',\n",
       " \" St. Cecily 's Garden ; 1920 \\n\",\n",
       " \" Cradle roll design ; St. Edmund 's , Pitlake , 1922 \\n\",\n",
       " \" Banner design ; St. Mary 's , <unk> , 1923 \\n\",\n",
       " ' The Feeding of the Five Thousand ; reredos triptych , chapel at Penarth , Wales ; 1929 \\n',\n",
       " \" The Parable of the Great Supper ; triptych , St. George 's chapel , Waddon \\n\",\n",
       " \" The Seven Sacraments ; baptismal font panels , St. Andrew 's , Croydon \\n\",\n",
       " ' St. John the Baptist ; central banner panel , <unk> church , 1943 \\n',\n",
       " ' Lettering , sword , and shield ; mount for a list of men and woman serving in the Forces , St. Andrews , Croydon , 1943 \\n',\n",
       " ' <unk> rolls ; St. Andrews , Croydon , 1948 , 1962 \\n',\n",
       " \" The font in St Andrew 's Church , South Croydon \\n\",\n",
       " ' Out of Great Tribulation ; memorial chapel , Norbury <unk> church , 1948 \\n',\n",
       " \" I Am Among You As He That <unk> ; stained glass window design , St. Edmund 's , Pitlake , 1962 \\n\",\n",
       " '',\n",
       " '',\n",
       " \" = Gambia women 's national football team = \\n\",\n",
       " '',\n",
       " \" The Gambia women 's national football team represents the Gambia in international football competition . The team , however , has not competed in a match recognised by FIFA , the sport 's international governing body , despite that organised women 's football has been played in the country since 1998 . The Gambia has two youth teams , an under @-@ 17 side that has competed in FIFA U @-@ 17 Women 's World Cup qualifiers , and an under @-@ 19 side that withdrew from regional qualifiers for an under @-@ 19 World Cup . The development of a national team faces challenges similar to those across Africa , although the national football association has four staff members focusing on women 's football . \\n\",\n",
       " '',\n",
       " ' = = The team = = \\n',\n",
       " '',\n",
       " \" In 1985 , few countries had women 's national football teams . While the sport gained popularity worldwide in later decades , the Gambia 's national team only played its first game in 2007 . That game was not FIFA @-@ recognised . As of March 2012 , the team was unranked by FIFA , and as of the following month the Gambia had not played in a FIFA @-@ sanctioned match . The team has not participated in major regional and international tournaments , including the Women 's World Cup , the 2010 African Women 's Championship or the 2011 All @-@ Africa Games . \\n\",\n",
       " \" The country did not have a FIFA @-@ recognised youth national team until 2012 , when the Gambia under @-@ 17 women 's team competed in Confederation of African Football qualifiers for the FIFA U @-@ 17 World Cup , to be held in Azerbaijan in September 2012 . The Gambia had fielded an under @-@ 17 team of 24 players , narrowed from an initial pool of 49 young women . Two girls from the SOS Children ’ s Village <unk> were chosen as a members of the team . The Gambia first played Sierra Leone in a pair of qualifying matches for the tournament . Gambia won the first match 3 @-@ 0 in Banjul , the Gambia 's capital . The return match was delayed in for 24 hours and played in Makeni . The Gambia beat Sierra Leone 4 @-@ 3 to qualify for the final round . The Gambia then beat Tunisia 1 @-@ 0 at home and won 2 @-@ 1 in Tunisia . Adama Tamba and Awa Demba scored the Gambia 's goals . Tunisia 's only goal was a Gambian own goal . The win qualified Gambia for the 2012 Azerbaijan World Cup . \\n\",\n",
       " \" The Gambia also has an under @-@ 19 team that was to play in the African Women 's U @-@ 19 Championship in 2002 . The Gambia 's first match was against Morocco , but the team withdrew from the competition . \\n\",\n",
       " '',\n",
       " ' = = Background and development = = \\n',\n",
       " '',\n",
       " \" The development of women 's football in Africa faces several challenges , including limited access to education , poverty amongst women , inequalities and human rights abuses targeting women . Funding is another issue impacting the game in Africa , where most financial assistance comes from FIFA and not national football associations . Another challenge is the retention of football players . Many women footballers leave the continent to seek greater opportunity in Europe or the United States . \\n\",\n",
       " \" Gambia 's national football association was founded in 1952 , and became affiliated with FIFA in 1968 . Football is the most popular women 's sport in the country , and was first played in an organized system in 1998 . A national competition was launched in 2007 , the same year FIFA started an education course on football for women . Competition was active on both the national and scholastic levels by 2009 . There are four staffers dedicated to women 's football in the Gambia Football Association , and representation of women on the board is required by the association 's charter . \\n\",\n",
       " '',\n",
       " '',\n",
       " ' = Plain maskray = \\n',\n",
       " '',\n",
       " ' The plain maskray or brown stingray ( Neotrygon annotata ) is a species of stingray in the family Dasyatidae . It is found in shallow , soft @-@ bottomed habitats off northern Australia . Reaching 24 cm ( 9 @.@ 4 in ) in width , this species has a diamond @-@ shaped , grayish green pectoral fin disc . Its short , whip @-@ like tail has alternating black and white bands and fin folds above and below . There are short rows of thorns on the back and the base of the tail , but otherwise the skin is smooth . While this species possesses the dark mask @-@ like pattern across its eyes common to its genus , it is not ornately patterned like other maskrays . \\n',\n",
       " ' Benthic in nature , the plain maskray feeds mainly on caridean shrimp and polychaete worms , and to a lesser extent on small bony fishes . It is viviparous , with females producing litters of one or two young that are nourished during gestation via histotroph ( \" uterine milk \" ) . This species lacks economic value but is caught incidentally in bottom trawls , which it is thought to be less able to withstand than other maskrays due to its gracile build . As it also has a limited distribution and low fecundity , the International Union for Conservation of Nature ( IUCN ) has listed it as Near Threatened . \\n',\n",
       " '',\n",
       " ' = = Taxonomy and phylogeny = = \\n',\n",
       " '',\n",
       " ' The first scientific description of the plain maskray was authored by Commonwealth Scientific and Industrial Research Organisation ( CSIRO ) researcher Peter Last in a 1987 issue of Memoirs of the National Museum of Victoria . The specific name <unk> comes from the Latin an ( \" not \" ) and <unk> ( \" marked \" ) , and refers to the ray \\'s coloration . The holotype is a male 21 @.@ 2 cm ( 8 @.@ 3 in ) across , caught off Western Australia ; several paratypes were also designated . Last tentatively placed the species in the genus Dasyatis , noting that it belonged to the \" maskray \" species group that also included the bluespotted stingray ( then Dasyatis kuhlii ) . In 2008 , Last and William White elevated the kuhlii group to the rank of full genus as Neotrygon , on the basis of morphological and molecular phylogenetic evidence . \\n',\n",
       " ' In a 2012 phylogenetic analysis based on mitochondrial and nuclear DNA , the plain maskray and the Ningaloo maskray ( N. <unk> ) were found to be the most basal members of Neotrygon . The divergence of the N. annotata lineage was estimated to have occurred ~ 54 Ma . Furthermore , the individuals sequenced in the study sorted into two genetically distinct clades , suggesting that N. annotata is a cryptic species complex . The two putative species were estimated to have diverged ~ 4 @.@ 9 Ma ; the precipitating event was likely the splitting of the ancestral population by coastline changes . \\n',\n",
       " '',\n",
       " ' = = Description = = \\n',\n",
       " '',\n",
       " ' The pectoral fin disc of the plain maskray is thin and diamond @-@ shaped with narrowly rounded outer corners , measuring 1 @.@ 1 – 1 @.@ 3 times longer than wide . The leading margins of the disc are gently concave and converge at a broad angle to the pointed tip of the snout . The small eyes are placed close together , and behind them are the spiracles . The nostrils are elongated and have a skirt @-@ shaped flap of skin between them . The small mouth bears prominent furrows at the corners and contains two slender papillae on the floor . Small papillae are also found around the outside of the mouth . There are five pairs of gill slits . The pelvic fins are fairly large and pointed . \\n',\n",
       " ' The tail is short , barely exceeding the length of the disc when intact , and has a broad and flattened base leading to usually two stinging spines . After the stings , the tail becomes slender and bears a long ventral fin fold and a much shorter , lower dorsal fin fold . Most of the body lacks dermal denticles ; a midline row of 4 – 13 small , closely spaced thorns is present behind the spiracles , and another row of 0 – 4 thorns before the stings . The dorsal coloration is grayish green , becoming pinkish towards the disc margins ; there is a dark mask @-@ like shape around the eyes and a pair of small dark blotches behind the spiracles . The tail behind the stings has alternating black and white bands of variable width , ending with black at the tip . The underside is plain white and the ventral fin fold is light grayish in color . This species grows to 24 cm ( 9 @.@ 4 in ) across and 45 cm ( 18 in ) long . \\n',\n",
       " '',\n",
       " ' = = Distribution and habitat = = \\n',\n",
       " '',\n",
       " ' The plain maskray inhabits the continental shelf of northern Australia from the Wellesley Islands in Queensland to the Bonaparte Archipelago in Western Australia , including the Gulf of Carpentaria and the Timor and Arafura Seas . There are unsubstantiated reports that its range extends to southern Papua New Guinea . It is the least common of the several maskray species native to the region . This species is a bottom @-@ dweller that prefers habitats with fine sediment . It has been recorded from between 12 and 62 m ( 39 and 203 ft ) deep , and tends to be found farther away from shore than other maskrays in its range . \\n',\n",
       " '',\n",
       " ' = = Biology and ecology = = \\n',\n",
       " '',\n",
       " ' The plain maskray generally hunts at the surface of the bottom substrate , rather than digging for prey . Its diet consists predominantly of caridean shrimp and polychaete worms . Small bony fishes are also eaten , along with the occasional penaeid prawn or amphipod . Larger rays consume a greater variety of prey and relatively more polychaete worms when compared to smaller rays . This species is parasitized by the tapeworm Acanthobothrium <unk> . \\n',\n",
       " ' Like other stingrays , the plain maskray is viviparous with the developing embryos sustained to term by histotroph ( \" uterine milk \" ) produced by the mother . Mature females have a single functional ovary and uterus , on the left . Litter size is one or two ; the newborns measure 12 – 14 cm ( 4 @.@ 7 – 5 @.@ 5 in ) across . Males and females reach sexual maturity at disc widths of 20 – 21 cm ( 7 @.@ 9 – 8 @.@ 3 in ) and 18 – 19 cm ( 7 @.@ 1 – 7 @.@ 5 in ) respectively . The maximum lifespan is estimated to be 9 years for males and 13 years for females . \\n',\n",
       " '',\n",
       " ' = = Human interactions = = \\n',\n",
       " '',\n",
       " \" The main conservation threat to the plain maskray is incidental capture by commercial bottom trawl fisheries . In the present day , this is mostly caused by Australia 's Northern Prawn Fishery , which operates throughout its range . Although this species is discarded when caught , it is more delicate @-@ bodied than other maskrays and is thus unlikely to survive encounters with trawling gear . Historically , this species may also have been negatively affected by Japanese , Chinese , and Taiwanese trawlers that fished intensively off northern Australia from 1959 to 1990 . These factors , coupled with the plain maskray 's limited distribution and low reproductive rate , have resulted in its being assessed as Near Threatened by the International Union for Conservation of Nature ( IUCN ) . \\n\",\n",
       " '',\n",
       " '',\n",
       " ' = 2011 – 12 Columbus Blue Jackets season = \\n',\n",
       " '',\n",
       " \" The 2011 – 12 Columbus Blue Jackets season was the team 's 12th season in the National Hockey League ( NHL ) . The Blue Jackets ' record of 29 – 46 – 7 [ note 1 ] was the worst record in the NHL for 2011 – 12 and the first time in franchise history they finished in last place . It also marked the third straight year that they missed the playoffs . Consequently , they had the best chance to receive the first overall selection in the 2012 NHL Entry Draft lottery , but lost out to the Edmonton Oilers and received the second pick instead . \\n\",\n",
       " \" The Blue Jackets began the year with the worst start in franchise history and the worst by any team in an NHL season in 19 years . After an 11 – 25 – 5 start , Head Coach Scott Arniel was fired and replaced by Assistant Coach Todd Richards . The poor season prompted several personnel changes including the trade of All @-@ Star forward Jeff Carter , who was acquired with much fanfare during the off @-@ season . With the prospect of another rebuild looming the Blue Jackets ' captain and best player , Rick Nash , requested to be traded , though he would remain with the team for the entire season . \\n\",\n",
       " ' The team was involved in a controversial loss to the Los Angeles Kings , when the Staples Center clock appeared to freeze at 1 @.@ 8 seconds allowing the Kings time to score the tying goal , before winning in overtime . During the season Columbus managed only two winning streaks of three or more games . One of which came towards the end of the year helping the Blue Jackets finish with 65 points , the third worst point total in franchise history . \\n',\n",
       " '',\n",
       " ' = = Off @-@ season = = \\n',\n",
       " '',\n",
       " \" In the off @-@ season the Blue Jackets ' approach to building their team changed , moving from a team of young developing players into one with established players . The first deal General Manager Scott Howson made was the acquisition of All @-@ Star forward Jeff Carter on June 23 , 2011 . The deal sent Jakub <unk> , Columbus ' first @-@ round draft choice , the eighth overall , and their third @-@ round pick in the 2011 Draft to the Philadelphia Flyers in exchange for Carter . The trade received a positive response in Columbus from fans and management who felt they finally had a number one center to play alongside of their best player , Rick Nash . Next , they traded for the negotiating rights of soon to be free agent James Wisniewski . Wisniewski scored a career high 51 points during the 2010 – 11 season , splitting time between the New York Islanders and Montreal Canadiens . The point total was fifth @-@ highest in the league for defenseman scoring , tying Tobias <unk> . The Blue Jackets came to terms with Wisniewski , just an hour prior to the start of free agency , signing him to a six @-@ year , $ 33 million deal . \\n\",\n",
       " ' Columbus also traded former first round draft pick Nikita Filatov to the Ottawa Senators for a third @-@ round pick in the 2011 Draft . Filatov had failed to live up to expectations in Columbus , playing in only 44 games over three seasons scoring six goals . Prior to the start of the season , the Blue Jackets were questioned for not signing a veteran back @-@ up to starting goaltender Steve Mason , as the former Calder Memorial Trophy winner had struggled in consecutive seasons . The Blue Jackets signed Mark <unk> as the back @-@ up who had only 50 minutes of NHL experience prior to the start of the season . Columbus did sign a veteran Curtis Sanford to be their third string goaltender and to start for their American Hockey League ( AHL ) affiliate , the Springfield Falcons . Sanford had not played in the NHL since 2009 . During training camp , <unk> suffered a high ankle sprain that was expected to keep him out of the line @-@ up for a month . Additionally , Sanford suffered a groin injury , leaving Allen York as the back @-@ up . York had only played four professional games , all in the AHL , entering the season . \\n',\n",
       " '',\n",
       " ' = = Regular season = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = October – December = = = \\n',\n",
       " '',\n",
       " ' After the first five games , all losses , Jeff Carter suffered a broken foot that kept him out of the line @-@ up for 10 games . While Carter was injured , the Blue Jackets continued to lose games . In the eighth game of the year , they had a chance to end the losing streak against the Ottawa Senators . Columbus held a 3 – 2 lead with under a minute to play . Jason Spezza tied the game on a late power play , and with just 4 @.@ 7 seconds remaining , Milan Michalek notched the winning goal for the Senators . The loss helped set a franchise record for futility with a 0 – 7 – 1 record to start a season . [ note 1 ] The losing streak came to an end three days later with a win over the Detroit Red Wings . During the game , several milestones were reached . James Wisniewski made his Columbus debut , Ryan Johansen and John Moore scored their first career NHL goals and Grant <unk> had a career @-@ high three assists . Columbus was unable to create any momentum from the win , however , and continued to struggle , culminating in a 2 – 12 – 1 record , which was the worst start to an NHL season for any team in 19 years . With the team struggling , management attempted to \" shake things up \" by making some roster moves . The first move was the acquisition of center Mark <unk> from the Pittsburgh Penguins . Next , they traded defenseman Kris Russell to the St. Louis Blues for Nikita Nikitin . As the clubs slow start continued , there were rumors that Head Coach Scott Arniel would be fired and replaced with Ken Hitchcock . Hitchcock had previously coached the Blue Jackets to their only playoff appearance in club history and was still under contract with the franchise through to the end of the season . Before any of these rumors came to fruition , the St. Louis Blues asked Columbus for permission to hire Hitchcock , which the Blue Jackets allowed . Hitchcock began his Blues coaching career with a 6 – 1 – 2 record in his first nine games , while Columbus amassed a 6 – 13 – 3 record to start the season . \\n',\n",
       " \" During the same time frame as the Hitchcock rumors , goaltender Curtis Sanford returned from his groin injury on November 13 . He made his first start of the season against the Boston Bruins , losing 2 – 1 in a shootout . Sanford continued his strong play , posting a 3 – 1 – 2 record , 1 @.@ 38 goals against average and <unk> save percentage over his next six games . Sanford started 12 consecutive games before Steve Mason made his next start . The number of starts might not have been as numerous , but prior to the November 23 game , Mason was hit in the head by a shot from Rick Nash during pre @-@ game warm @-@ ups and suffered a concussion . Mason returned from his concussion after two games , making a start against the Vancouver Canucks . Mason allowed only one goal in the game despite suffering from cramping in the third period , temporarily being replaced by Sanford for just over three minutes . Columbus won the game 2 – 1 in a shootout , breaking a nine @-@ game losing streak to the Canucks . After the game , Arniel stated that Sanford was still seen as the team 's number one goaltender . However , Mason started four of the next six games with the Blue Jackets going 0 – 5 – 1 during that stretch . \\n\",\n",
       " '',\n",
       " ' = = = January – February = = = \\n',\n",
       " '',\n",
       " ' With the losing continuing , more rumors began to surface . Unlike before , the rumors were about player moves rather than coaching changes . The majority of rumors were that the Blue Jackets would trade Rick Nash . While Howson stated that he had never brought up trading Nash in discussions , other teams had inquired about his availability . Nash stated that if Columbus felt it would make the franchise better than he would be willing to waive his no @-@ trade clause . Howson publicly stated that he had no intention of trading Nash . More rumors came to light when reports attributed to Réseau des sports stated that Carter was unhappy in Columbus and demanded a trade . Howson , Carter and his agent all denied that a trade request was ever made , and they were unsure where the reports were coming from . With the trade deadline approaching , speculation picked up on the Blue Jackets trading Carter . Reports were that Columbus was trying to trade Carter and that he was \" 100 percent available . \" \\n',\n",
       " ' At the halfway point of the season , with the Blue Jackets barely into double digit wins with an 11 – 25 – 5 record , worst in the league , and sitting 20 points out of playoff position , Columbus fired Arniel . He was replaced by Assistant Coach Todd Richards on an interim basis . Richards had previously coached the Minnesota Wild . He recorded his first coaching victory for the Blue Jackets in his second game , a 4 – 3 win over the Phoenix Coyotes . The change in coaching did not change the fortunes of the team , as they reached the All @-@ Star break with a 13 – 30 – 6 record . At the break , Blue Jackets \\' owner John P. McConnell sent out a letter to fans stating his understanding of their frustration . He added that action would be taken around the trade deadline , the Entry Draft and free agency to take the team in a new direction . When speaking of the season , McConnell stated \" disappointing is not a strong enough word \" and that he was committed to giving fans a team of which they can be proud of . He also thanked them for their dedication and passion , while reiterating that the team goal was to \" win consistently and compete for the Stanley Cup . \" Days later , a 250 @-@ person protest occurred outside of Nationwide Arena . Fans were upset with the Blue Jackets \\' management and were calling for changes at the top . The same day the fans protested , it was announced that the franchise would host the 2013 All @-@ Star Game . Columbus was without a representative for the 2012 All @-@ star Game , but Ryan Johansen represented the club as a rookie participant in the Super Skills Competition . In the competition , Johansen participated in the Allstate Insurance NHL Breakaway Challenge , a shootout themed event judged by the fans . He received just 1 % of the vote and finished last . \\n',\n",
       " ' Following the break , the Blue Jackets were on the road playing the Los Angeles Kings , and with the score tied late in the game , Kings \\' defenseman Drew Doughty scored with just 0 @.@ 4 seconds remaining to win the game . Upon review of the goal it , was determined that the clock at Staples Center froze at 1 @.@ 8 seconds for over a full second , which would have resulted in time expiring prior to the goal being scored . Kings \\' General Manager Dean Lombardi stated that the clock was correct and no extra time had been added due to the way the clock self @-@ corrects at various times . Howson stated on the team \\'s blog that \" It is an amazing coincidence that with the Kings on a power play at Staples Center and with a mad scramble around our net in the dying seconds of the third period of a 2 – 2 hockey game that the clock stopped for at least one full second , \" adding that , \" Either there was a deliberate stopping of the clock or the clock malfunctioned . \" NHL Senior Vice President of Hockey Operations Colin Campbell stated that the Blue Jackets were wronged , but that the outcome of the game could not be changed , and that the delay was not noticed by the off @-@ ice officials or the situation room in Toronto . To determine the true cause of the clock pause , the NHL launched an investigation , talking with the clock \\'s manufacturer and interviewing Staples Center staff . \\n',\n",
       " ' Two weeks prior to the NHL trade deadline , Columbus announced that unlike earlier in the season , they would listen to trade proposals involving Rick Nash , though they were not actively shopping him . Howson stated that the team was open to all options for improving the team , including trading Nash . Speculation was that in return for Nash the Blue Jackets would ask for a \" combination of young , proven players , high @-@ end prospects and draft picks . \" Leading up to the trade deadline , the Blue Jackets dealt Antoine <unk> to the Phoenix Coyotes for two draft picks and goaltender Curtis McElhinney . Despite being injured at the time , the acquisition of McElhinney was believed to give Columbus the flexibility to trade Curtis Sanford . The following day , on February 23 , Columbus traded Jeff Carter to the Kings . In the deal , Columbus acquired defenseman Jack Johnson and a first @-@ round draft pick ; the team was given the choice of taking the pick in either 2012 or 2013 . At the deadline , Columbus was unable to come to terms on a deal involving Nash , but they did make one more move ; they sent center Samuel Pahlsson to the Vancouver Canucks in exchange for two fourth @-@ round draft picks and minor league defenseman Taylor Ellington . Following the trade deadline , Howson announced that the team had attempted to trade Nash at the player \\'s request . Nash stated that he had requested the trade after being informed that the franchise was going into another rebuilding phase . He further noted that he felt that he \" could be a huge part of that towards bringing assets in , \" and in his view \" it was the best thing for the team , the organization , and personally for [ his ] career . \" After the personnel changes , the Blue Jackets closed out the month with a three @-@ game losing streak . \\n',\n",
       " '',\n",
       " ' = = = March – April = = = \\n',\n",
       " '',\n",
       " ' Columbus started March with a 2 – 0 shutout against the Colorado Avalanche . They proceeded to win their next game against the Phoenix Coyotes 5 – 2 , which marked the first time that the Blue Jackets posted back @-@ to @-@ back regulation victories during the season . Columbus again defeated the Coyotes three days later to earn their first three @-@ game win streak of the season . They extended the streak to four with a win over the Los Angeles Kings before it came to an end with a 4 – 1 loss to the St. Louis Blues . It was the only four @-@ game win streak of the season for the Blue Jackets . They immediately matched their four @-@ game win streak with a four @-@ game losing streak and with ten games remaining , the Blue Jackets were the first team eliminated from playoff contention . Shortly after being eliminated , they were defeated by the Edmonton Oilers 6 – 3 ; the loss clinched last place in the NHL for Columbus . It was the first time in franchise history the Blue Jackets finished in 30th place . \\n',\n",
       " \" Three days later , on March 28 , goaltender Steve Mason was injured in the morning skate when a shot from Colton Gillies hit him in the mask . With Sanford again injured , York made an emergency start . Playing against the Detroit Red Wings , York made 29 saves , including 17 in the third period , helping Columbus to a 4 – 2 victory and giving York his first career NHL win . York remained the starter and led the Blue Jackets to a second three @-@ game winning streak . In his fourth start , Columbus was shutout by the Coyotes despite a franchise @-@ record 54 shots on goal , losing 2 – 0 . The 54 saves by Phoenix goaltender Mike Smith set an NHL record for a regulation shutout . Mason returned to the starter 's role for the final two games , winning both . The two victories gave Columbus 65 points for the year , their third @-@ lowest total in franchise history . \\n\",\n",
       " ' The Blue Jackets struggled in shorthanded situations , allowing the most power @-@ play goals in the League , with 64 , and having the lowest penalty @-@ kill percentage , at 76 @.@ 64 % \\n',\n",
       " '',\n",
       " ' = = Post @-@ season = = \\n',\n",
       " '',\n",
       " \" Finishing with the worst record in the NHL , Columbus had the best chance of receiving the first overall pick in the 2012 draft . With the NHL 's weighted draft lottery the Blue Jackets had a 48 @.@ 2 % chance of drafting first overall . However , the lottery was won by the Edmonton Oilers , who proceeded to leapfrog Columbus and secure the number one draft pick for a third consecutive year . It was the fifth time that the Blue Jackets were dropped one draft position in the franchise 's 12 lottery participations . \\n\",\n",
       " ' A month later , on May 14 , the Blue Jackets announced that Richards would remain as head coach and signed him to a two @-@ year contract . During the press conference , Howson noted , \" Our team continuously improved under Todd and he has earned the opportunity to build upon the work he started . \" Columbus posted an 18 – 21 – 2 record under Richards , including winning seven of their final 11 games . \\n',\n",
       " '',\n",
       " ' = = Standings = = \\n',\n",
       " '',\n",
       " ' Since being founded as an expansion team , the Blue Jackets have played in the Central Division of the Western Conference . Division rivals Chicago Blackhawks , Detroit Red Wings , Nashville Predators and St. Louis Blues , all made the playoff during the 2011 – 12 season , which helped Columbus finish 36 points behind fourth place Chicago and 44 points out of first . \\n',\n",
       " ' Divisions : CE – Central , NW – Northwest , PA – Pacific \\n',\n",
       " \" bold - qualified for playoffs , y – Won division , p – Won Presidents ' Trophy ( best record in NHL ) \\n\",\n",
       " '',\n",
       " ' = = Schedule and results = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = Pre @-@ season = = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = Regular season = = = \\n',\n",
       " '',\n",
       " ' Green background indicates win ( 2 points ) . \\n',\n",
       " ' Red background indicates regulation loss ( 0 points ) . \\n',\n",
       " ' Silver background indicates overtime / shootout loss ( 1 point ) . \\n',\n",
       " '',\n",
       " ' = = Player statistics = = \\n',\n",
       " '',\n",
       " \" In ice hockey , a combination of a player 's goals and assists are collectively called points . Penalty minutes are the total number of minutes assigned to a player for infractions assessed during the <unk> @-@ minus is a statistic that tracks when a player was on the ice while goals were scored , both for and against their team , though some in game situations will not effect the statistic . Below is a listing of all player statistics for the Blue Jackets during the season . \\n\",\n",
       " '',\n",
       " ' = = = Skaters = = = \\n',\n",
       " '',\n",
       " ' Note : Pos \\n',\n",
       " ' = Position ; GP = \\n',\n",
       " ' Games played in ; G \\n',\n",
       " ' = Goals ; A = \\n',\n",
       " ' Assists ; Pts \\n',\n",
       " ' = Points ; PIM = \\n',\n",
       " ' Penalty minutes ; + / - = Plus / minus \\n',\n",
       " '',\n",
       " ' = = = Goaltenders = = = \\n',\n",
       " '',\n",
       " ' Note : GP \\n',\n",
       " ' = Games Played ; TOI = \\n',\n",
       " ' Time On Ice ( minutes ) ; W \\n',\n",
       " ' = Wins ; L = \\n',\n",
       " ' Losses ; OT \\n',\n",
       " ' = Overtime Losses ; GA = \\n',\n",
       " ' Goals Against ; GAA = Goals Against Average ; SA = Shots Against ; SV \\n',\n",
       " ' = Saves ; Sv % = \\n',\n",
       " ' Save Percentage ; SO = Shutouts \\n',\n",
       " ' † Denotes player spent time with another team before joining Blue Jackets . Stats reflect time with the Blue Jackets only . ‡ Traded mid @-@ season \\n',\n",
       " '',\n",
       " ' = = Milestones = = \\n',\n",
       " '',\n",
       " \" When Mason was injured in warm @-@ ups late in the year , Columbus was without an active goaltender on their roster . To remedy the situation , the team signed former University of Michigan goaltender Shawn Hunwick to a one @-@ day , amateur tryout contract . After being eliminated from the NCAA Tournament just days prior , Hunwick skipped an astronomy class and drove his worn down 2003 Ford Ranger to Columbus to make the game . He served as the back @-@ up to Allen York during the game , and the following day , he signed a contract for the remainder of the year . With Mason returning from injury , Hunwick was third on the team 's depth chart when an injury to York allowed Hunwick to remain as the back @-@ up for the final two games of the year . In the final game of the season , the Blue Jackets were leading the Islanders 7 – 3 with 2 : 33 remaining when , at the behest of his teammates , Head Coach Todd Richards put Hunwick in to finish the game . He did not face a shot . Hunwick was the franchise record ninth player to make his NHL debut during the season . Conversely , Vaclav <unk> played in his 1,000th NHL game during the year . \\n\",\n",
       " '',\n",
       " ' = = Transactions = = \\n',\n",
       " '',\n",
       " ' During the off @-@ season the Blue Jackets parted ways with defensemen Jan Hejda , Anton Stralman , Sami <unk> and Mike Commodore . Hejda , who played four of his first five NHL seasons with the Blue Jackets , was offered a contract by Columbus , but felt that the organization undervalued him and left via free agency . Columbus had offered him a three @-@ year , $ 7 @.@ 5 million contract . He instead signed a four @-@ year , $ 13 million deal with the Colorado Avalanche . Stralman and <unk> were not given qualifying offers which made them unrestricted free agents , and both signed with other teams . Commodore had originally signed a big contract with the Blue Jackets in 2008 , but fell out of favor . He was waived , sent to the minors and eventually had his contract bought out . In order to replace the departed players , Columbus not only acquired James Wisniewski , but also signed ten @-@ year NHL veteran Radek <unk> . <unk> played only seven games with the Blue Jackets before suffering a concussion and missing the remainder of the season . Brett <unk> was brought in to replace him . \\n',\n",
       " '',\n",
       " ' = Gregorian Tower = \\n',\n",
       " '',\n",
       " ' The Gregorian Tower ( Italian : Torre <unk> ) or Tower of the Winds ( Italian : Torre dei Venti ) is a round tower located above the Gallery of Maps , which connects the Villa Belvedere with the Apostolic Palace in Vatican City . The tower was built between 1578 and 1580 to a design by the Bolognese architect Ottaviano Mascherino ( who was credited with building the Apostolic Palace ) mainly to promote the study of astronomy for the Gregorian Calendar Reform which was commissioned by Pope Gregory XIII and promulgated in 1582 . It was then also known as the Tower of Winds . The tower is now called the \" <unk> Astronomica Vaticana \" , the Vatican Observatory . Four stages of progressive development have occurred since it was first established . The tower was an edifice of great value for astronomical observations made using a sundial as they provided essential confirmation of the need to reform the Julian calendar . \\n',\n",
       " '',\n",
       " ' = = Early history = = \\n',\n",
       " '',\n",
       " ' The first stage of building of the tower , as recorded by Leo XIII in his motu proprio Ut <unk> of 1891 , is credited to Pope Gregory XIII , Pope from 1572 to 1585 . The directive was to build a tower at a suitable location in the Vatican and equip it with the \" greatest and best instruments of the time \" . The design was effected after a series of meetings of the experts who had been appointed to reform the Julian calendar , in use since 45 BC , to verify their proposed reforms . Fr . Christoph Clavius , a Jesuit mathematician from the Roman College , was the expert on the committee who suggested the new system for the observations . The 73 metres ( 240 ft ) tower was then built above the museum and library , flanked by the Belvedere and della Pigna courtyards . The instrumentation for the observation of the sun rays falling over it consisted of a meridian line designed by Ignazio <unk> of Perugia . It was in the form of a circular marble plate in the centre , embellished with scientific designs . The tower still remains today , but has undergone improvements over the centuries . \\n',\n",
       " '',\n",
       " ' = = Second stage = = \\n',\n",
       " '',\n",
       " \" The second stage of construction in the 17th and 18th centuries , when the tower was under the charge of the Vatican librarian , involved Mgr . Filippo Luigi Gilii , a clergyman of St. Peter 's Basilica . Earlier in 1797 , Pius VI gave approval to placing a Latin inscription <unk> Vaticana at the entrance to the upper part of the tower , which was implemented by Cardinal <unk> with plans to enhance the instrumentation system in the tower 's observatory . The original observatory was then set up above the second level of the tower with the agreement of Pope Pius VI . Its instrumentation , apart from many normal devices ( such as meteorological and magnetic equipment , with a seismograph and a small transit and pendulum clock , ) was noted for the <unk> Telescope . The instrumentation facilitated recording of occurrences of eclipse , appearance of comets , Satellites of Jupiter and Mercury ’ s transit . As an addition , under the patronage of Pope Pius X , four rotary observatory domes were also added at strategic locations on the 1 @,@ 300 feet ( 400 m ) long fortification walls , more than a thousand years old . Mgr . Gilii , highly respected as a polyglot with a knowledge of physics , biology , archeology and the Hebrew language , was in charge of the observatory from 1800 to 1821 . He carried out continuous meteorological observations ( twice a day at 6 AM and 2 PM ) conforming to the programme of the Mannheim Meteorological Society . While the observation records for seven years were published , the balance data in a manuscript form was preserved in the Vatican Library . Subsequent to Gilii 's death in 1821 , the observatory on the tower was discontinued and the instruments were moved to the observatory at the Roman College . Established in 1787 , it was considered more suitable for making observations than the Vatican . \\n\",\n",
       " '',\n",
       " ' = = Third stage = = \\n',\n",
       " '',\n",
       " \" The revival of the observatory on the Gregorian Tower was initiated by the <unk> Francesco Denza with the approval of Pope Leo XIII . High quality instruments were procured , partly with generous donations from Hicks of London , and the automatic recording instruments were procured from Richard in Paris . A four @-@ inch equatorial , a three @-@ inch transit instrument , and four pendulum clocks with two chronometers , were also procured from the observatory at Modena . In 1888 , the gift of a 16 inch long telescope to Pope Leo XIII , became a part of the observatory . Father Denza joined the observatory in 1889 after it was upgraded with more modern instruments . The same year , a second tower was built some 400 metres ( 1 @,@ 300 ft ) away from the main Gregorian Tower , overlooking the Vatican Gardens behind St. Peter 's Basilica on the south @-@ west border . It was built to a diameter of 17 metres ( 56 ft ) with a lower wall thickness of 4 @.@ 5 metres ( 15 ft ) , which could bear the load of a 13 inch photographic refractor , newly procured from Paris . Augustinian Father Rodriguez was the expert meteorologist who held the post of director from 1898 to 1905 . In 1891 , Pope Leo XIII , promulgating the motu proprio Ut <unk> , designated the second tower as the seat of the newly established Vatican Observatory , a decision which required altering the roof to provide a flat terrace for astronomical observations . \\n\",\n",
       " '',\n",
       " ' = = Fourth stage = = \\n',\n",
       " '',\n",
       " ' The fourth stage involved remedying the problem of communicating between the two towers during the time of Pope Pius X. His plans were to make the Gregorian Tower into a historical tower and to record and carry out observations at the second tower by linking the two towers along the fortified wall with a 83 metres ( 272 ft ) iron bridge spanning the gap . At the west end of this bridge , a four @-@ inch equatorial was installed on semicircular bastion . The east end of the bridge , above the barracks of the gendarmes , had a heliograph , with a camera attached , used to photograph the Sun ( <unk> ) . A new 16 @-@ inch visual telescope , called Torre Pio X , was erected in the second tower . As a result of these modifications , the original library was moved to the Pontifical Academy Lincei , and the old meteorological and seismic instruments were shifted to the Valle di Pompei observatory . The new Astronomical Library was housed in two rooms of the building . The two new <unk> machines were used for recording on the <unk> plates . The recorded observations were published along with explanatory notes together with the last two series of the atlas of stars . Charts were printed on silver bromide paper . \\n',\n",
       " '',\n",
       " ' = = Features = = \\n',\n",
       " '',\n",
       " ' The tower had two floors and a mezzanine . On the first floor was the famous Sundial Room or Meridian Room , which was initially an open loggia . Pope Urban VIII had it enclosed and it was subsequently decorated with long sequences of frescoes painted between 1580 and 1582 by Simon Lagi and the two Flemish artists Paul and Matthijs <unk> . Today the tower has paintings by Cristoforo Roncalli and <unk> da Siena . \\n',\n",
       " ' The Sundial Room , also called the Meridian Hall , was once the residence of Queen Christina of Sweden , then newly converted to Catholicism . The room was further modified by two additions which gave it its current name : a sundial , and a delicate but sophisticated <unk> which was fixed to the ceiling of the Meridian Hall . These were created by Ignazio <unk> , the papal <unk> , in association with the Gregorian Calendar Reform . The sundial consisted of a straight line in white marble running across the floor in a north @-@ south direction , intended to measure the height of the Sun at noon according to the seasons of the year . The observations made with the sundial provided essential confirmation of the need to reform the Julian calendar . The <unk> , in contrast , was a complex mechanism attached to the ceiling which was used to measure the strength and direction of the wind but soon stopped functioning . The instrument may have led to the other name of the tower , Tower of the Winds ; however , an ancient observatory at Athens was also called the Tower of the Winds and might have been the source for inspiration . \\n',\n",
       " ' The interior walls and ceiling of the hall were richly decorated , in some cases with gaudy frescoes of the hills and Roman countryside , the <unk> , religious themes , the buildings surrounding the area , and naval shipwrecks with Jesus calming the storm and so forth . \\n',\n",
       " '',\n",
       " '',\n",
       " \" = There 's Got to Be a Way = \\n\",\n",
       " '',\n",
       " ' \" There \\'s Got to Be a Way \" is a song by American singer and songwriter Mariah Carey from her self @-@ titled debut studio album ( 1990 ) . Columbia released it as the fifth and final single from the album in the United Kingdom . It was one of four songs Carey wrote with Ric Wake during their first recording session together , but \" There \\'s Got to Be a Way \" was the only composition to make the final track listing . It is a socio @-@ political conscious R & B @-@ pop song which addresses the existence of poverty , racism and war in the world which gradually becomes more aspirational and positive as it progresses . The track garnered a mixed reception upon the album \\'s release in 1990 . While Carey \\'s vocals were praised , it was seen as too political . An accompanying music video highlights social injustices . The song reached number 54 on the UK Singles Chart . \\n',\n",
       " '',\n",
       " ' = = Background and release = = \\n',\n",
       " '',\n",
       " ' \" There \\'s Got to Be a Way \" was written by Mariah Carey and Ric Wake for Carey \\'s self @-@ titled debut studio album ( 1990 ) . It was written during Carey and Wake \\'s first recording session together . They composed four songs , but only \" There \\'s Got to Be a Way \" was chosen for the final track listing . Co @-@ produced by Wake and Narada Michael Walden , it appears as the second of ten songs on the track listing . The track was recorded and engineered by Bob <unk> at Cove City Sound Studios and The Power Station , both located in New York City . He was assisted by Dana Jon Chappelle . It was mixed by David Frazer at Tarpan Studios in San Rafael . The keyboards , bass and rhythm engineering was carried out by Louis Biancaniello , while Joe Franco performed the percussion , Vernon \" Ice \" Black played the guitar , and Rich Tancredo also performing on the keyboards . Walter Afanasieff played the synth horns . Carey provided her own background vocals along with Billy T. Scott , <unk> Muhammed and The Billy T. Scott Ensemble . The song was released as the fifth and final single from the album in the United Kingdom . It is available to purchase as a CD single while the remixes are available on vinyl . \\n',\n",
       " '',\n",
       " ' = = Composition = = \\n',\n",
       " '',\n",
       " ' \" There \\'s Got to Be a Way \" is an R & B @-@ pop music song with elements of gospel . The theme of social activism can be heard in the lyrics \" There ’ s got to be a way / to connect this world today . \" The song begins with Carey publicly denouncing the existence of poverty and racism in the world , and she uses the bridge to shift the lyrics towards an uplifting and aspirational tone . Carey suggests we should be more tolerant of each other and not resort so readily to war in the lyrics \" Couldn \\'t we accept each other / Can \\'t we make ourselves aware . \" \\n',\n",
       " '',\n",
       " ' = = Critical reception = = \\n',\n",
       " '',\n",
       " ' Music critic Robert Christgau felt that Carey was being too political in her \" brave , young , idealistic attack \" on war and destitution . Ralph Novak , David Hiltbrand and David Grogan of People wrote that it is a \" testimony to her talent that she does so much with so little . \" They continued to write that Carey \\'s \" tone and clarity \" makes \" There \\'s Got to Be a Way \" a \" mesmerizing \" track . To mark twenty @-@ five years since the release of Mariah Carey in June 1990 , Billboard writer Trevor Anderson wrote a track @-@ by @-@ track review of the album in June 2015 . He noted that \" There \\'s Got to Be a Way \" follows the same melodic tone as the album \\'s opener \" Vision of Love \" but highlighted their stark lyrical differences , as the former is about social activism and the latter is about love . Although he praised Carey \\'s vocals , writing that she \" deploys \" one of her best whistle notes of her career , he felt that \" the aim for broad appeal comes at the expense of memorable lyrics . \" \\n',\n",
       " '',\n",
       " ' = = Music video = = \\n',\n",
       " '',\n",
       " ' The accompanying music video begins with a shot of an empty street , followed by clips of disadvantaged and poorer members of society going about their daily activities . Two men play dominoes on a wooden crate outside a building , a gang make fun of an elderly man hanging newspapers outside his store and an obese woman walks down the street . Clips of Carey leaning against a wall and sitting on some steps looking on at what is happening are shown . As the first chorus begins , everyone starts to dance joyfully in the street and help those in need . A gospel choir comes out of one of the buildings as the street becomes more crowded with people of all ages and backgrounds rejoicing and getting along with each other . One of the shops in the background has a neon light outside the entrance which says \" Jesus Saves \" . \\n',\n",
       " '',\n",
       " ' = = Track listings = = \\n',\n",
       " '',\n",
       " ' \" There \\'s Got to Be a Way \" ( Original album version ) – 4 : 52 \\n',\n",
       " ' \" There \\'s Got to Be a Way \" ( 7 \" remix ) \\n',\n",
       " ' \" There \\'s Got to Be a Way \" ( 12 \" remix ) \\n',\n",
       " ' \" There \\'s Got to Be a Way \" ( Alternative Vocal Dub Mix ) \\n',\n",
       " '',\n",
       " ' = = Charts = = \\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' = Nebraska Highway 88 = \\n',\n",
       " '',\n",
       " ' Nebraska Highway 88 ( N @-@ 88 ) is a highway in northwestern Nebraska . It has a western terminus at Wyoming Highway 151 ( WYO 151 ) at the Wyoming – Nebraska state line . The road travels eastward to N @-@ 71 , where it turns south . N @-@ 88 continues east to south of Bridgeport . The road turns north , ends at an intersection with U.S. Highway 385 ( US 385 ) and N @-@ 92 in Bridgeport . The route was designated in 1937 , before the official state highway system was created . It was extended to the state line in 1986 . \\n',\n",
       " '',\n",
       " ' = = Route description = = \\n',\n",
       " '',\n",
       " ' N @-@ 88 starts at the Nebraska – Wyoming state line in Banner County , where WYO 151 ends , and travels northeast . The road quickly bends east after less than one mile ( 1 @.@ 6 km ) , and continues in a straight line . For the next twenty miles ( 32 km ) , N @-@ 88 intersects minor streets , through rural farmland . The route turns south at N @-@ 71 , and becomes concurrent . Four miles ( 6 @.@ 4 km ) later , N @-@ 88 turns east , ending the concurrency with N @-@ 71 . The route continues to travel through farmland for sixteen miles ( 26 km ) , where it enters Morrill County . The road crosses over Pumpkin Creek four times , and enters the unincorporated community of <unk> . Two rock formations , Courthouse and Jail Rocks , become visible from the road . N @-@ 88 turns north toward Bridgeport soon after . The road crosses over Pumpkin Creek for the fifth time , and enters into Bridgeport five miles ( 8 @.@ 0 km ) later . The road intersects a railroad owned by BNSF Railway . N @-@ 88 turns northeast soon after , and ends at the intersection of US 385 and N @-@ 92 . In 2012 , Nebraska Department of Roads ( <unk> ) calculated as many as 2 @,@ 410 vehicles traveling on the N @-@ 71 / N @-@ 88 concurrency , and as few as 315 vehicles traveling east of the Banner – Morrill county line . This is expressed in terms of annual average daily traffic ( AADT ) , a measure of traffic volume for any average day of the year . Only the N @-@ 71 / N @-@ 88 concurrency is part of the National Highway System ( NHS ) , a network of highways identified as being most important for the economy , mobility and defense of the nation . \\n',\n",
       " '',\n",
       " ' = = History = = \\n',\n",
       " '',\n",
       " ' N @-@ 88 was unofficially designated around 1937 , connecting from N @-@ 29 , to N @-@ 86 and N @-@ 19 in Bridgeport . The route remained relatively the same as the state highway system was officially designated . Before 1955 , Nebraska did not have an adequate legal instrument to define the state highway system . By 1960 , N @-@ 19 was renumbered to US 385 , and US 26 was rerouted north near Bridgeport . The old alignment became part of N @-@ 92 . Two years later , N @-@ 29 was renumbered to N @-@ 71 . Between 1981 @-@ 82 , a road appeared on the official state map , extending from WYO 151 to N @-@ 71 . That road became part of N @-@ 88 by 1986 . No significant changes have been made since . \\n',\n",
       " '',\n",
       " ' = = Major intersections = = \\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' = USS Atlanta ( 1861 ) = \\n',\n",
       " '',\n",
       " ' Atlanta was a casemate ironclad that served in the Confederate and Union Navies during the American Civil War . She was converted from a Scottish @-@ built blockade runner named Fingal by the Confederacy after she made one run to Savannah , Georgia . After several failed attempts to attack Union blockaders , the ship was captured by two Union monitors in 1863 when she ran aground . Atlanta was floated off , repaired , and rearmed , serving in the Union Navy for the rest of the war . She spent most of her time deployed on the James River supporting Union forces there . The ship was decommissioned in 1865 and placed in reserve . Several years after the end of the war , Atlanta was sold to Haiti , but was lost at sea in December 1869 on her delivery voyage . \\n',\n",
       " '',\n",
       " ' = = Description and career as Fingal = = \\n',\n",
       " '',\n",
       " \" Fingal was designed and built as a merchantman by J & G Thomson 's Clyde Bank Iron Shipyard at Govan in Glasgow , Scotland , and was completed early in 1861 . She was described by Midshipman Dabney Scales , who served on the Atlanta before her battle with the monitors , as being a two @-@ masted , iron @-@ hulled ship 189 feet ( 57 @.@ 6 m ) long with a beam of 25 feet ( 7 @.@ 6 m ) . She had a draft of 12 feet ( 3 @.@ 7 m ) and a depth of hold of 15 feet ( 4 @.@ 6 m ) . He estimated her tonnage at around 700 tons bm . Fingal was equipped with two vertical single @-@ cylinder direct @-@ acting steam engines using steam generated by one flue @-@ tubular boiler . The engines drove the ship at a top speed of around 13 knots ( 24 km / h ; 15 mph ) . They had a bore of 39 inches ( 991 mm ) and a stroke of 30 inches ( 762 mm ) . \\n\",\n",
       " \" The ship briefly operated between Glasgow and other ports in Scotland for Hutcheson 's West Highland Service before she was purchased in September 1861 by James D. Bulloch , the primary foreign agent in Great Britain for the Confederacy , to deliver the military and naval ordnance and supplies that he had purchased . To disguise his control of Fingal , and the destination of her cargo , Bulloch hired an English crew and captain and put out his destination as Bermuda and Nassau in the Bahamas . The cargo was loaded in Greenock in early October , although Bullock and the other passengers would not attempt to board until they rendezvoused with the ship at Holyhead , Wales . On the night 14 / 15 October , as she was slowly rounding the breakwater at Holyhead , Fingal rammed and sank the Austrian brig <unk> , slowly swinging at anchor without lights . Bulloch and the passengers embarked in the steamer while Bulloch dispatched a letter to his financial agents instructing them to settle damages with the brig 's owners because he could not afford to take the time to deal with the affair lest he and Fingal be detained . The ship reached Bermuda on 2 November and , after leaving port on 7 November , Bulloch informed the crew that the steamer 's real destination was Savannah , Georgia ; he offered to take anyone who objected to the plan to Nassau . However , all of the crew agreed to join in the effort to run the Union blockade . Fingal was able slip safely into the Savannah estuary in a heavy fog on the night of 12 November without sighting any blockaders . \\n\",\n",
       " \" While Fingal was discharging her cargo , Bulloch went to Richmond to confer with Stephen Mallory , Secretary of the Navy . Mallory endorsed Bulloch 's plan to load Fingal with cotton to sell on the Navy Department 's account to be used to purchase more ships and equipment in Europe . He returned to Savannah on 23 November and it took him almost a month to purchase a cargo and acquire enough coal . He made one attempt to break through the blockade on 23 December , but it proved impossible to do as the Union controlled every channel from Savannah , aided by their occupation of Tybee Island at the mouth of the Savannah River . Bulloch reported to Mallory in late January 1862 that breaking out was hopeless so Mallory ordered him to turn the ship over to another officer and to return to Europe some other way . \\n\",\n",
       " '',\n",
       " ' = = As Atlanta = = \\n',\n",
       " '',\n",
       " ' The brothers Asa and Nelson Tift received the contract to convert the blockade runner into an ironclad in early 1862 with the name of Atlanta , after the city in Georgia . This was largely financed by contributions from the women of Savannah . Fingal was cut down to her main deck and large wooden sponsons were built out from the sides of her hull to support her casemate . After the conversion , Atlanta was 204 feet ( 62 @.@ 2 m ) long overall and had a beam of 41 feet ( 12 m ) . Her depth of hold was now 17 feet ( 5 @.@ 2 m ) and she now had a draft of 15 feet 9 inches ( 4 @.@ 8 m ) . Atlanta now displaced 1 @,@ 006 long tons ( 1 @,@ 022 t ) and her speed was estimated at 7 – 10 knots ( 13 – 19 km / h ; 8 @.@ 1 – 11 @.@ 5 mph ) . \\n',\n",
       " \" The armor of the casemate was angled at 30 ° from the horizontal and made from two layers of railroad rails , rolled into plates 2 inches ( 51 mm ) thick and 7 inches ( 180 mm ) wide . The outer layer ran vertically and the inner layer horizontally . Her armor was backed by 3 inches ( 76 mm ) of oak , vertically oriented , and two layers of 7 @.@ 5 inches ( 191 mm ) of pine , alternating in direction . The bottom of the casemate was some 20 inches ( 508 mm ) from the waterline and its top was 8 feet 6 inches ( 2 @.@ 59 m ) above the waterline . The pyramidal pilothouse was armored in the same way and had room for two men . The upper portion of Atlanta 's hull received two inches of armor . \\n\",\n",
       " ' The rectangular casemate was pierced with eight narrow gun ports , one each at the bow and stern and three along each side . Each gun port was protected by an armored shutter made of two layers of iron riveted together and allowed the guns to elevate only to a maximum of + 5 to + 7 ° . Atlanta was armed with single @-@ banded , 7 @-@ inch ( 178 mm ) Brooke rifles on pivot mounts at the bow and stern . The middle gun port on each side was occupied by a single @-@ banded , 6 @.@ 4 @-@ inch ( 163 mm ) Brooke rifle . The 17 @-@ caliber , seven @-@ inch guns weighed about 15 @,@ 000 pounds ( 6 @,@ 800 kg ) and fired 80 @-@ pound ( 36 kg ) armor @-@ piercing \" bolts \" and 110 @-@ pound ( 50 kg ) explosive shells . The equivalent statistics for the 18 @.@ 5 @-@ caliber , 6 @.@ 4 @-@ inch gun were 9 @,@ 110 pounds ( 4 @,@ 130 kg ) with 80 @-@ pound bolts and 64 @-@ pound ( 29 kg ) shells . Atlanta was also armed with a 20 @-@ foot ( 6 @.@ 1 m ) , solid iron , ram that was reinforced by a series of vertical steel bars . In front of the ram was a spar torpedo that carried 50 pounds ( 23 kg ) of black powder on a wooden pole connected to an iron lever that could be raised or lowered by means of pulleys . \\n',\n",
       " ' On 31 July 1862 , under the command of Lieutenant Charles H. McBlair , Atlanta conducted her sea trials down the Savannah River toward Fort Pulaski . The ship proved to be difficult to steer , and the additional weight of her armor and guns significantly reduced her speed and increased her draft . This latter was a real problem in the shallow waters near Savannah . She also leaked significantly , and her design virtually eliminated air circulation . One report said that \" it was almost intolerable on board the Atlanta , there being no method of ventilation , and the heat was intense . \" Scales commented in his diary , \" What a comfortless , infernal and God @-@ forsaken ship ! ! \" \\n',\n",
       " \" Attempts were made to fix the problems and were at least partially successful in stopping many of the leaks . The ship was commissioned on 22 November and became the flagship of Flag Officer Josiah Tattnall , commander of the naval defenses of Georgia . Under pressure from Mallory to engage the blockading ships , Tattnall attempted to engage them before any ironclads arrived on 5 January 1863 , but army engineers could not clear the obstacles blocking the channel in a timely manner , despite early coordination being made by Tattnall to do so . It took another month to actually clear the obstacles and two monitors arrived before the end of January . Nonetheless Tattnall attempted to pass through the obstructions during high tide on 3 February , but high winds prevented the water from rising enough to allow the ship to do so . After Atlanta successfully passed through them on 19 March , Tattnall planned to attack the Union base at Port Royal , South Carolina while the monitors were attacking Charleston . Deserters revealed Tatnall 's plan while he was waiting at the head of <unk> Sound and he was forced to retreat when three monitors augmented the defenses at Port Royal . Dissatisfied with Tattnall 's perceived lack of aggressiveness , Mallory replaced Tattnall as commander of the Savannah squadron later that month with Commander Richard L. Page . Page , in his turn was relieved in May by Commander William A. Webb ; Atlanta remained the squadron flagship throughout this time . \\n\",\n",
       " \" Webb demonstrated his aggressiveness when he attempted to sortie on the first spring tide ( 30 May ) after taking command , but Atlanta 's forward engine broke down after he had passed the obstructions , and the ship ran aground . She was not damaged although it took over a day to pull her free . He planned to make another attempt on the next full tide , rejecting Mallory 's idea that he wait until the nearly complete ironclad Savannah was finished before his next sortie . In the meantime , Rear Admiral Samuel F. Du Pont , commander of the South Atlantic Blockading Squadron , had ordered the monitors Weehawken and Nahant into <unk> Sound . Commander John Rodgers in Weehawken had overall command of the two ships . \\n\",\n",
       " ' In the early evening of 15 June , Webb began his next attempt by passing over the lower obstructions in the Wilmington River and spent the rest of the night coaling . He moved forward the next evening to a concealed position within easy reach of the monitors for an attack early the following morning . Webb planned to sink one of the monitors with his spar torpedo and then deal with the other one with his guns . The gunboat <unk> and the tugboat Resolute were to accompany him to tow one or both of the monitors back to Savannah . \\n',\n",
       " \" A lookout aboard Weehawken spotted Atlanta at 04 : 10 on the morning of 17 June . When the latter ship closed to within about 1 @.@ 5 miles ( 2 @.@ 4 km ) of the two Union ships , she fired one round from her bow gun that passed over Weehawken and landed near Nahant . Shortly afterward , Atlanta ran aground on a sandbar ; she was briefly able to free herself , but the pressure of the tide pushed her back onto the sandbar . This time Webb was unable to get off and the monitors closed the range . When Weehawken , the leading ship , closed to within 200 – 300 yards ( 180 – 270 m ) she opened fire with both of her guns . The 11 @-@ inch ( 279 mm ) shell missed , but the 15 @-@ inch ( 381 mm ) shell struck the ironclad above the port middle gun port , penetrated her armor and broke the wooden backing behind it , spraying splinters and fragments that disabled the entire gun crew and half the crew of the bow gun , even though it failed to cleanly penetrate through the backing . The next shot from the 11 @-@ inch Dahlgren gun struck the upper hull and started a small leak even though it failed to penetrate the two @-@ inch armor there . The next shell from the 15 @-@ inch Dahlgren glanced off the middle starboard gun shutter as it was being opened , wounding half the gun 's crew with fragments . The final shell was also from the 15 @-@ inch Dahlgren and it struck the top of the pilothouse , breaking the armor there and wounding both pilots in it . By this time , Atlanta had been able to fire only seven shots , none of which hit either Union ship , and was hard aground with high tide not due for another hour and a half . Weehawken and Nahant were able to freely maneuver into positions from which the Atlanta 's narrow gun ports would not allow her to reply and the damage already inflicted by the former ship made further resistance futile . Webb surrendered his ship within 15 minutes of opening fire , before Nahant even had a chance to fire . Of the ironclad 's 21 officers and 124 enlisted men , one man was killed and another sixteen were wounded badly enough to require hospitalization . \\n\",\n",
       " '',\n",
       " ' = = In the Union Navy = = \\n',\n",
       " '',\n",
       " \" Atlanta was easily pulled free by the Union ships and she reached Port Royal under her own power . Not badly damaged , she was repaired and bought by the Union Navy . The prize money of $ 350 @,@ 000 was shared between the crews of Weehawken , Nahant and the gunboat Cimarron , the only ships within signaling distance . The ship retained her name and was commissioned again on 2 February 1864 , rearmed with a pair of 8 @-@ inch ( 203 mm ) , 150 @-@ pound Parrott rifles in the bow and stern and 6 @.@ 4 @-@ inch , 100 @-@ pound Parrott rifles amidships . The 150 @-@ pound Parrott rifle weighed 16 @,@ 500 pounds ( 7 @,@ 500 kg ) and was 17 calibers long . The 100 @-@ pounder weighed 9 @,@ 800 pounds ( 4 @,@ 400 kg ) and was 20 calibers long . It fired a 100 @-@ pound ( 45 kg ) shell a distance of 6 @,@ 900 yards ( 6 @,@ 300 m ) at an elevation of + 25 ° . All four of her Brooke rifles are currently located in Willard Park in the Washington Navy Yard . Atlanta was assigned to the North Atlantic Blockading Squadron and spent most of her time stationed up the James River where she could support operations against Richmond and defend against a sortie by the ironclads of the James River Squadron . On 21 May 1864 , she and the gunboat Dawn fired on and dispersed Confederate cavalry that was attacking Fort Powhatan and she was deployed further upriver in February 1865 after the Battle of Trent 's Reach to better blockade the Confederate ironclads at Richmond . \\n\",\n",
       " ' After the end of the war in April , Atlanta was decommissioned in Philadelphia on 21 June 1865 and placed in reserve at League Island . She was sold to Sam Ward on 4 May 1869 for the price of $ 25 @,@ 000 and subsequently delivered to representatives of Haiti on 8 December by Sydney <unk> , a lawyer who had received an advance of $ 50 @,@ 000 on her purchase price of $ 260 @,@ 000 . The ship was briefly seized by the Customs Service , possibly for violations of neutrality laws as she had just loaded four large guns and a number of recruits for the forces of Sylvain <unk> , President of Haiti , who was embroiled in a civil war . Atlanta was released and sailed for Port @-@ au @-@ Prince three days later . She broke down in Delaware Bay and had to put in at Chester , Pennsylvania for repairs . The ship , now renamed either Triumph or <unk> , departed on 18 December 1869 and vanished en route , apparently sinking with the loss of all hands , either off Cape Hatteras or the Delaware Capes . \\n',\n",
       " '',\n",
       " '',\n",
       " ' = Jacqueline Fernandez = \\n',\n",
       " '',\n",
       " ' Jacqueline Fernandez ( born 11 August 1985 ) is a Sri Lankan actress , former model , and the winner of the 2006 Miss Universe Sri Lanka pageant . As Miss Universe Sri Lanka she represented her country at the 2006 world Miss Universe pageant . She graduated with a degree in mass communication from the University of Sydney , and worked as a television reporter in Sri Lanka . \\n',\n",
       " \" While on a modelling assignment in India in 2009 , Fernandez successfully auditioned for Sujoy Ghosh 's fantasy drama Aladin , which marked her acting debut . Fernandez ' breakthrough role was in Mohit Suri 's psychological thriller Murder 2 ( 2011 ) , her first commercial success . This was followed by glamorous roles in the ensemble @-@ comedy Housefull 2 ( 2012 ) and its sequel Housefull 3 , and the action thriller Race 2 ( 2013 ) , all of which were box @-@ office successes . Her performance in the first of these garnered her an IIFA Award for Best Supporting Actress nomination . In 2014 , Fernandez played the leading lady in Sajid Nadiadwala 's Kick , which is one of the highest @-@ grossing Bollywood films of all time . \\n\",\n",
       " ' One of the most popular actresses in India , she was the recipient of the IIFA Award for Star Debut of the Year – Female in 2010 . Alongside her screen acting career , Fernandez has participated in stage shows , and is active in humanitarian work . \\n',\n",
       " '',\n",
       " ' = = Early life and modeling career = = \\n',\n",
       " '',\n",
       " \" Fernandez was born on 11 August 1985 , in Manama , Bahrain , and was raised in a multi @-@ ethnic family . Her father , Elroy , is Sri Lankan , and her mother , Kim , is of Malaysian descent . Her grandfather , on her mother 's side of the family , is Canadian and her great grandparents were from Goa , India . Her father , who was a musician in Sri Lanka , moved to Bahrain in the 1980s to escape civil unrest between the Tamils and Sinhalese and subsequently met her mother who was an air hostess . She is the youngest of four children with one elder sister and two elder brothers . She hosted television shows in Bahrain at the age of fourteen . After receiving her early education in Bahrain , she pursued a degree in mass communication from the University of Sydney in Australia . After graduating she worked as a television reporter in Sri Lanka . She also attended the Berlitz school of languages , where she learnt Spanish and improved her French and Arabic . \\n\",\n",
       " ' According to Fernandez , she had aspired to become an actress at a young age and fantasized about becoming a Hollywood movie star . She received some training at the John School of Acting . Although , she was a television reporter , she accepted offers in the modeling industry , which came as a result of her pageant success . In 2006 , she was crowned the winner of the Miss Universe Sri Lanka pageant and represented Sri Lanka at the world Miss Universe 2006 pageant held in Los Angeles . In a 2015 interview , Fernandez described the modeling industry as \" a good training ground \" and said : \" It is a medium that is about shedding your inhibitions , knowing your body , confidence \" . In 2006 , she appeared in a music video for the song \" O Sathi \" by music duo <unk> and <unk> . \\n',\n",
       " '',\n",
       " ' = = Acting career = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = 2009 – 2013 = = = \\n',\n",
       " '',\n",
       " ' In 2009 , Fernandez traveled to India for a modeling assignment . She studied acting under the mentorship of theatre director Barry John , and successfully auditioned for Sujoy Ghosh \\'s fantasy film Aladin ( 2009 ) her acting debut . She played the love interest of <unk> Deshmukh \\'s character , a role based on the Princess Jasmine character . Fernandez garnered mixed reviews for her performance . Anupama Chopra of NDTV called her a \" plastic debutant [ e ] \" , and Rajeev Masand of CNN @-@ IBN felt that she was : \" easy on the eyes and appears confident but has precious little to do \" . Although the film was a critical and commercial failure , she won the IIFA Award for Star Debut of the Year - Female . \\n',\n",
       " ' In 2010 , Fernandez appeared opposite Deshmukh in the science fiction romantic comedy Jaane Kahan Se Aayi Hai . She was cast as a girl from Venus , who lands on Earth in search of love . The film , along with Fernandez \\'s performance , received poor reviews ; Rediff.com \\'s Sukanya Verma noted : \" She gamely makes a fool of herself whilst aping the actions of movie stars , ranging from Sridevi \\'s <unk> dance , Mithun Chakravarthy \\'s Disco Dancer moves , to Big B \\'s violent <unk> in Hum . Her Tara could be a keeper if only Jaane Kahan Se Aayi Hai wasn \\'t so intent on turning her into a love @-@ struck Barbie . \" Critic Anupama Chopra also criticized Fernandez , calling her \" a pin @-@ prick on a balloon \" . Later that year , she made a cameo appearance in Sajid Khan \\'s Housefull in the song \" <unk> \" . \\n',\n",
       " ' Mahesh Bhatt \\'s thriller Murder 2 was Fernandez \\'s first commercial success and marker a turning point in her career . She took on the role of Priya , a lonely model who is in a confused relationship with Arjun Bhagwat ( played by Emraan Hashmi ) . Fernandez was praised for the her performance , and for the boldness and sex appeal she displayed in the film . Gaurav Malini of The Times of India stated that she was \" tastefully tempting \" but noted that her romance with Hashmi was \" literally half @-@ baked \" . The following year , Fernandez appeared in the ensemble comedy Housefull 2 alongside Akshay Kumar , John Abraham , and Asin . It became one of the top grossing productions of India that year and earned ₹ 1 @.@ 86 billion ( US $ 28 million ) worldwide . Fernandez received mostly negative reviews for her performance . While Gaurav Malini praised her for her looks , NDTV called her a \" <unk> bimbo \" who \" find [ s ] no pleasure in [ her role ] \" . Despite the negative reviews , Fernandez received a Best Supporting Actress nomination at the 14th IIFA Awards for her performance . \\n',\n",
       " ' Fernandez \\'s first release of 2013 was Race 2 , an ensemble action thriller ( alongside Saif Ali Khan , John Abraham , Deepika Padukone , Ameesha Patel , and Anil Kapoor ) ) , described as the \" cinematic equivalent of a trashy novel \" by critic Rajeev Masand . She played <unk> , a femme fatale , a role which required her learn fencing and some acrobatics . The film emerged as a commercial success , with the domestic gross of more than ₹ 1 billion ( US $ 15 million ) . In a particularly scathing review , Saibal Chatterjee of NDTV wrote that both Fernandez and Padukone \" strut around like wound @-@ up automatons that are all decked @-@ up but have nowhere to go . \" Fernandez also appeared in an item number ( music video ) titled \" Jaadu Ki <unk> \" for Prabhu Deva \\'s <unk> <unk> <unk> . \\n',\n",
       " '',\n",
       " ' = = = 2014 – present = = = \\n',\n",
       " '',\n",
       " ' In 2014 , Fernandez appeared in Sajid Nadiadwala \\'s directorial debut — the action film Kick , a remake of a 2009 Telugu film of same name . She starred opposite Salman Khan , playing Shaina , a psychiatry student . She retained her real voice for the first time in Kick . While Sneha May Francis commented that she is : \" incredibly dazzling , and moves like a magic \" , Raja Sen of Rediff.com was more critical of her dialogue delivery , calling it \" unfortunate . \" The film received mixed reviews from critics , but with worldwide revenue of over ₹ 3 @.@ 75 billion ( US $ 56 million ) , it became the fourth highest @-@ grossing Bollywood film . The film established Fernandez as one of the most popular Bollywood actresses . \\n',\n",
       " ' In 2015 , Fernandez featured in Vicky Singh \\'s Roy , a romantic thriller , which critic Sarita A. Tanwar described as a \" boring , exhausting and pretentious \" film . Fernandez played dual roles , Ayesha Aamir , a filmmaker in a relationship with another filmmaker ( played by Arjun Rampal ) and Tia Desai , a girl in love with a thief ( played by Ranbir Kapoor ) . While India TV called it \" her best act till date \" , critic Rajeev Masand felt that she \" appears miscast in a part that required greater range . \" Roy failed to meet its box @-@ office expectations , and was a commercial failure . Later that year , she appeared in a guest appearance for the comedy @-@ satire <unk> . \\n',\n",
       " ' Karan Malhotra \\'s action drama Brothers was Fernandez \\'s next release . Co @-@ starring alongside Akshay Kumar and Sidharth Malhotra , Fernandez played Jenny , a fearless mother struggling for her child , a role which she described as \" challenging \" , \" intense \" , and \" difficult \" . The role marked a departure from the glamorous characters that she had a reputation for portraying . Film critics praised her performance , though their response to the film was mixed . <unk> Sharma of Zee News called her character \" soft , timid and promising \" , and praised her for : \" convincingly pull [ ing ] off a pleasing character of a street fighter \\'s wife \" . Film critic Subhash K. Jha noted that she : \" ... in a limited role gives her finest emotive shot \" , while critic Raja Sen remarked : \" [ she ] plays Kumar \\'s long @-@ sobbing wife who gets so deliriously happy on seeing a text message that it may well have contained news about a Kick sequel . \" \\n',\n",
       " \" As of September 2015 , Fernandez has several projects in various stages of production . She has completed shooting for Chandran <unk> 's English @-@ Sri Lankan crime @-@ thriller According to Mathew , and the horror thriller Definition of Fear , which marks her Hollywood debut . Fernandez has also signed on to appear in three other projects — Rohit Dhawan 's <unk> opposite Varun Dhawan and John Abraham as a part of three @-@ film deal with Nadiadwala Grandson Entertainment , Remo D 'Souza 's Flying Jat opposite Tiger Shroff , and in an Indo @-@ Chinese film starring opposite Abhay Deol , Amitabh Bachchan , and Jackie Chan titled Gold Struck . \\n\",\n",
       " '',\n",
       " ' = = Personal life and other work = = \\n',\n",
       " '',\n",
       " ' Fernandez shares a close bond with her family , and admits to missing being around them . She says : \" I miss them so much everyday . You don \\'t realise when you live away from home how difficult life can be [ ... ] At the same time , staying away from them has taught me to be more responsible . It has taught me so many things about myself , about priorities and time management . \" In March 2012 , Fernandez turned vegetarian for a 40 @-@ day period to observe Lent , a period from Ash Wednesday to Holy Saturday . \\n',\n",
       " \" In 2008 , Fernandez started dating Bahraini prince Hassan bin Rashid Al Khalifa , whom she met at a mutual friend 's party ; they separated in 2011 . While filming Housefull 2 in 2011 , Fernandez began a romantic relationship with director Sajid Khan . The relationship attracted media coverage in India and there was speculation of an impending wedding . However , the relationship ended in May 2013 . \\n\",\n",
       " ' In addition to acting in films , Fernandez has supported charitable organisations and a number of causes . In 2011 , on the behalf of People for the Ethical Treatment of Animals ( PETA ) , she sent a letter to the Mumbai Municipal Commissioner asking for an end to horse @-@ drawn carriage rides in Mumbai . In early 2013 , she asked the consulate general of the Philippines , William John T Perera in Colombo , to hasten the transfer of an elephant from its inadequate housing at the Manila Zoo to a humane sanctuary . Later that year , she auctioned a breakfast in Mayfair , London , where she raised around £ 4000 for the Pratham NGO , which helps children \\'s primary education . In 2014 , Fernandez was named \" Woman Of The Year \" by PETA ( India ) for advocating the protection of animals . The following year , she auctioned her outfits on an online portal for a philanthropic cause . Some of her outfits included the ones she wore in the song \" Party On My Mind \" ( from Race 2 ) and \" Hangover \" ( from Kick ) . In March 2016 , she was part of \" Jacqueline Builds \" campaign that raised funds for the victims of the 2015 South Indian floods . \\n',\n",
       " ' Fernandez has participated in several concert tours and televised award ceremonies . In 2013 , she performed at the Temptations Reloaded in Auckland , Perth , and Sydney alongside Shah Rukh Khan , Rani Mukerji , and Madhuri Dixit . She also performed at the live talent show \" Got Talent World Stage Live \" with Khan , Priyanka Chopra and Varun Dhawan the following year . In July 2014 , Fernandez opened a restaurant in Colombo , <unk> Sutra , in collaboration with chef <unk> <unk> , which specialises in contemporary Sri Lankan cuisine . \\n',\n",
       " '',\n",
       " ' = = In the media = = \\n',\n",
       " '',\n",
       " ' In the early 2013 , Fernandez became the ambassador for HTC One , which she endorses in India . She was the face of Indian Bridal Fashion Week — <unk> of 2013 . Later that year , she became the spokesperson for Gareth Pugh \\'s designed <unk> Diamonds in Mumbai , and was at the inaugural opening of the Forever 21 store in Mumbai . That year , she also launched Gillette Shaving System with Arbaaz Khan and Aditya Roy Kapur . While analysing Fernandez \\'s career , India TV noted : \" Slowly and steadily Jacqueline Fernandez is climbing up the ladder of success [ ... ] Jacqueline is comfortably grasping every aspect of the work , which an actress is required to do and is accordingly giving results . \" On the contrary , Charu Thakur of India Today criticized her acting skills , but remarked that : \" [ she has ] managed to find her feet in Bollywood now by banking on glamorous roles \" . \\n',\n",
       " ' In 2008 and 2011 , Fernandez featured in the UK magazine Eastern Eye \\'s \" World \\'s Sexiest Asian Women \" list , ranking twelfth . She was ranked third on The Times of India \\'s listing of the \" Most Desirable Woman \" in 2013 and 2014 , after being ranked eighth , seventh and fourteenth , respectively , in the preceding three years . In 2013 , Rediff.com placed her on their list of \" Bollywood \\'s Best Dressed Actresses \" . The following year , she held the sixty second position in the Indian edition of the Forbes \\' Celebrity 100 , a list based on the income and popularity of India \\'s celebrities . She has been the cover model for many Indian editions of magazines , including : Vogue , FHM , Maxim , Cosmopolitan , Grazia , Elle , Verve , Harper \\'s Bazaar , Women \\'s Health , and L \\'Officiel among others . \\n',\n",
       " '',\n",
       " ' = = Filmography = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = TV Appearances = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = Awards = = \\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' = John Cullen = \\n',\n",
       " '',\n",
       " \" Barry John Cullen ( born August 2 , 1964 ) is a Canadian former professional ice hockey centre who played in the National Hockey League ( NHL ) for the Pittsburgh Penguins , Hartford Whalers , Toronto Maple Leafs and Tampa Bay Lightning . He was a standout player for Boston University and is the school 's all @-@ time leading scorer . After the Buffalo Sabres selected him in the 1986 NHL Supplemental Draft but chose not to offer him a contract , Cullen signed with the Flint Spirits of the International Hockey League ( IHL ) for the 1987 – 88 season where he was named the IHL 's co @-@ Rookie of the Year and Most Valuable Player after leading the league in scoring . \\n\",\n",
       " ' His career was halted in 1997 when he was diagnosed with Non @-@ Hodgkin lymphoma . He attempted a brief comeback in 1998 after an 18 @-@ month battle with the disease , for which the NHL awarded him the Bill Masterton Memorial Trophy , before retiring to serve as an assistant coach for a year with the Lightning . Cullen played in two NHL All @-@ Star Games in his career . He joined his brother in the car dealership business after leaving the game , and briefly operated his own dealership until forced to close during the automotive industry crisis of 2008 – 10 . \\n',\n",
       " '',\n",
       " ' = = Early life = = \\n',\n",
       " '',\n",
       " ' Cullen was born in <unk> @-@ Ontario on August 2 , 1964 . He is one of six children of Barry and Loretta Cullen . His father and uncles Brian and Ray all played in the NHL , and while Cullen and his three brothers all played as well , their father never pressured them , preferring that they enjoy the game . \\n',\n",
       " \" He idolized his elder brother Terry , who was considered a top NHL prospect until Terry 's career was ended when he suffered a broken neck after being hit from behind into the boards during a college game . While his brother was highly sought by American universities , John received only two scholarship offers , choosing to play for Boston University ( BU ) in 1983 . \\n\",\n",
       " \" At the same time , his mother Loretta was diagnosed with skin cancer . Following her death early in his freshman year , Cullen contemplated returning to his Ontario home , but was convinced by his father to continue with both school and hockey . He used the game to cope with the loss and dedicated every game he played to his mother 's memory . Cullen felt that the inspiration he drew from his mother 's battle allowed him to become a better player . \\n\",\n",
       " '',\n",
       " ' = = Playing career = = \\n',\n",
       " '',\n",
       " \" Cullen was a standout with BU ; he was named the East Coast Athletic Conference Rookie of the Year in 1983 – 84 after leading his team in scoring with 56 points . The National Hockey League passed him over , however , as he went unclaimed in the 1984 NHL Entry Draft . He was named to the Hockey East All @-@ Star Teams in 1985 , 1986 and 1987 , and a National Collegiate Athletic Association East Second Team All @-@ American in 1986 . He graduated as BU 's all @-@ time scoring leader with 241 points , and was named to BU 's Hockey East 25th anniversary team in 2009 . \\n\",\n",
       " \" Passed over in the Entry Draft , Cullen was finally selected by the Buffalo Sabres in the 1986 NHL Supplemental Draft . When the Sabres failed to offer him a contract , Cullen signed with the Flint Spirits of the International Hockey League ( IHL ) for the 1987 – 88 season . He led the league with 157 points , scoring 48 goals , and won the James <unk> Memorial Trophy as league most valuable player while sharing the Gary F. Longman Memorial Trophy with Ed Belfour as rookie of the year . Cullen 's outstanding season in Flint caught the attention of the Sabres and the Pittsburgh Penguins . He signed a contract with the Penguins for the league minimum , passing up a superior contract offer from Buffalo as he remained upset at how they released him the year before . \\n\",\n",
       " '',\n",
       " ' = = = National Hockey League = = = \\n',\n",
       " '',\n",
       " \" Cullen made his NHL debut in 1988 – 89 , appearing in 79 games with the Penguins and scoring 49 points . He was given a greater role with the Penguins the following year after Mario Lemieux missed 21 games due to a back injury and responded by scoring 32 goals and 92 points to finish third in team scoring . Additionally , he played for Team Canada at the 1990 World Championship , scoring four points in ten games . Cullen had his best season in 1990 – 91 . As one of the team 's top offensive centres , he scored 94 points in the Penguins ' first 65 games and played in his first NHL All @-@ Star Game . However , when Lemieux returned after missing an additional 50 @-@ games due to injury , Cullen 's playing time and production declined . \\n\",\n",
       " ' The Penguins \\' needs led them to complete a blockbuster trade on March 1 , 1991 . Cullen was sent to the Hartford Whalers , along with Zarley Zalapski and Jeff Parker in exchange for Hartford \\'s all @-@ time leading scorer , Ron Francis , along with Ulf Samuelsson and Grant Jennings . The Penguins almost turned down the deal as they were concerned about giving up Cullen \\'s playmaking and leadership abilities , while his former teammates credited Cullen as being the primary reason they were in a playoff position at the time the trade happened . After the Penguins won their first Stanley Cup that season , Phil Bourque later said it \" broke his heart \" that Cullen was not able to share in that championship . \\n',\n",
       " \" In Hartford , Cullen worked to overcome the team 's fans ' disappointment at losing Francis . The Hartford fans initially booed him to show their dissatisfaction with the trade . He scored 16 points in 13 regular season games to finish the season with 110 points combined between the Penguins and Whalers , and was the team 's best player in their first round loss to the Boston Bruins in the 1991 Stanley Cup Playoffs . He initially accepted an invitation to join the Canadian team at the 1991 Canada Cup , but subsequently chose not to participate as his contract had expired , leading to greater insurance concerns . Still without a contract when the 1991 – 92 season began , Cullen missed the first four games before signing a four @-@ year deal with Hartford worth a total of $ 4 million . He returned to score 77 points in 77 games in his first full season with the Whalers and represented the team at the 1992 All @-@ Star Game . \\n\",\n",
       " \" Midway through the 1992 – 93 NHL season , the Whalers sent Cullen to the Toronto Maple Leafs for Toronto 's second round selection at the 1993 NHL Entry Draft . Cullen was excited to play for his father 's old team , but injuries reduced his ability to perform . His most significant injury was a herniated disc in his neck that doctors initially feared would end his career . A bulky neck brace allowed Cullen to return and play out his contract in Toronto . When the Leafs chose not to re @-@ sign him following the 1993 – 94 season , he returned to the Penguins for one season before Tony Esposito convinced him to sign with the Tampa Bay Lightning in 1995 . \\n\",\n",
       " ' Cullen enjoyed immediate success with linemates Shawn Burr and Alexander <unk> as the trio combined to score 130 points and helped lead the Lightning to the first playoff appearance in franchise history . They were eliminated by the Philadelphia Flyers in five games while Cullen led the team in playoff scoring with three goals and three assists . The Lightning looked to improve in 1996 – 97 ; Cullen was leading the team in scoring , but was suffering flu @-@ like symptoms that he could not shake . As Tampa was fighting for a playoff spot , he played through his condition for weeks . \\n',\n",
       " '',\n",
       " ' = = = Cancer and comeback = = = \\n',\n",
       " '',\n",
       " \" After two months of quietly dealing with his symptoms , Cullen 's wife finally called team trainers and asked them to check into his illness . The team took an x @-@ ray and found a large black shadow in his chest . He underwent a CAT scan which revealed Cullen had a baseball @-@ sized tumor ; he was diagnosed as having Non @-@ Hodgkin lymphoma . The diagnosis ended his season , and he immediately began chemotherapy treatments that quickly reduced his cancer . The tumor was gone by September 1997 , but a precautionary test prior to training camp revealed that Cullen still had cancer cells in his body . He missed the entire 1997 – 98 NHL season as he continued to battle the disease , while his teammates wore a uniform patch with his # 12 in support throughout the year . \\n\",\n",
       " ' On one day during his treatments , as his wife was wheeling him down a hospital corridor , Cullen went into cardiac arrest , requiring doctors to use a defibrillator to revive him . He underwent a bone marrow transplant that briefly reduced his immune system to the point that he could have very little human contact . Another examination in April 1998 revealed that the cancer was finally gone , and Cullen immediately began training for a comeback . \\n',\n",
       " ' The Lightning signed Cullen to a one @-@ year , $ 500 @,@ 000 contract for the 1998 – 99 season . He played his first game in nearly 18 months on September 18 , 1998 , in an exhibition game between the Lightning and Sabres at Innsbruck , Austria . Cullen scored the game @-@ winning goal in a 3 – 1 victory , after which he said he sat on the bench in disbelief over how he was given a second chance . He was named to the roster and was greeted with a loud standing ovation by the fans in Tampa Bay when he was introduced prior to their season opening game . \\n',\n",
       " \" Cullen appeared in four of the Lightning 's first eight games , but it was evident that he had lost much of his speed and strength . The Lightning assigned him to the IHL 's Cleveland Lumberjacks , but also gave him the option of retiring and taking up a position as an assistant coach . He chose to accept the demotion , giving himself one month to determine if he could continue playing . He appeared in six games for Cleveland , and in one game against the Chicago Wolves tied an IHL record when he scored seven points in a 7 – 3 victory . \\n\",\n",
       " ' However , a bout of bronchitis led Cullen to fear that his cancer had returned . Tests came back negative , but after spending time with his family , he realized that neither he nor his family were interested in returning to Cleveland . Cullen announced his retirement on November 28 , 1998 , and accepted the Lightning offer to become an assistant coach . In recognition of his comeback attempt , the NHL named him the 1999 winner of the Bill Masterton Memorial Trophy for dedication and perseverance , while the IHL renamed its Comeback Player of the Year award the John Cullen Award . \\n',\n",
       " ' Former Lightning head coach Terry Crisp has stated publicly that Cullen was a player that stood out as something special saying “ John Cullen ... beat cancer and came back to play and helped us win . ” \\n',\n",
       " '',\n",
       " ' = = Off the ice = = \\n',\n",
       " '',\n",
       " \" Cullen and his wife Valerie have three daughters , Kennedy and twins <unk> and <unk> . Unwilling to spend so much time away from his family , he left the Lightning in 1999 and settled in the Atlanta area , joining his brother 's car dealership in Jonesboro , Georgia . He had always expected to become a car dealer after his hockey career , as his father , uncles and brother all worked in the industry . After apprenticing under his brother for five years , he bought a Dodge dealership in Newnan , Georgia in 2007 . However , he owned the dealership for less than two years before Chrysler closed him down as part of its recovery plan in response to the Automotive industry crisis of 2008 – 2010 . He has since returned to his brother 's dealership , serving as its general manager . \\n\",\n",
       " \" Cullen 's battle with cancer inspired Timm Harmon of the Moffitt Cancer Centre to partner with the Lightning to raise awareness and money for cancer research . The NHL itself joined the cause in the winter of 1998 , creating the Hockey Fights Cancer program to raise money for research . Cullen has spent time promoting the initiative . \\n\",\n",
       " ' Prior to marrying his wife Valerie , John dated Carolyn Bessette the future wife of John F. Kennedy , Jr . The two met while attending University in Boston . \\n',\n",
       " '',\n",
       " ' = = Career statistics = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = Regular season and playoffs = = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = International = = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = Awards = = \\n',\n",
       " '',\n",
       " ' Cullen is the namesake of the John Cullen Award , previously given to key IHL players . \\n',\n",
       " '',\n",
       " '',\n",
       " ' = SMS Erzherzog Ferdinand Max = \\n',\n",
       " '',\n",
       " ' For the ironclad present at the Battle of Lissa of the same name , see SMS Erzherzog Ferdinand Max ( 1865 ) . \\n',\n",
       " ' SMS Erzherzog Ferdinand Max ( German : \" His Majesty \\'s ship Archduke Ferdinand Max \" ) was a pre @-@ dreadnought battleship built by the Austro @-@ Hungarian Navy in 1902 . The second ship of the Erzherzog Karl class , she was launched on 3 October 1903 . She was assigned to the III Battleship Division . \\n',\n",
       " ' For most of World War I , Erzherzog Ferdinand Max remained in her home port of Pola , in present @-@ day Croatia , except for four engagements . In 1914 , she formed part of the Austro @-@ Hungarian flotilla sent to protect the escape of the German ships SMS Goeben and SMS Breslau from the British @-@ held Mediterranean ; she advanced as far as Brindisi before being recalled to her home port . Her sole combat engagement occurred in late May 1915 , when she participated in the bombardment of the Italian port city of Ancona . She also took part in suppressing a major mutiny among the crew members of several armored cruisers stationed in Cattaro between 1 – 3 February 1918 . She also attempted to break through the Otranto Barrage in June of that year , but had to retreat when the dreadnought SMS Szent István was sunk . After the war , Erzherzog Ferdinand Max was awarded to the United Kingdom as a war prize in 1920 . \\n',\n",
       " '',\n",
       " ' = = Design = = \\n',\n",
       " '',\n",
       " ' Erzherzog Ferdinand Max displaced 10 @,@ 472 long tons ( 10 @,@ 640 t ) . She was 414 feet 2 inches ( 126 @.@ 2 m ) long , had a beam of 71 feet 5 inches ( 21 @.@ 8 m ) and a draft of 24 feet 7 inches ( 7 @.@ 5 m ) . She was manned by 700 men . She and her sisters were the last and largest pre @-@ dreadnought class built by the Austro @-@ Hungarian Navy , surpassing the Habsburg class by approximately 2 @,@ 000 tonnes ( 1 @,@ 968 long tons ) . She was propelled by two two @-@ shaft , four cylinder vertical triple expansion steam engines . On trials , they developed 18 @,@ 000 ihp ( 13 @,@ 423 kW ) , which propelled the ship at a speed of 20 @.@ 5 knots ( 38 @.@ 0 km / h ; 23 @.@ 6 mph ) . \\n',\n",
       " ' Erzherzog Ferdinand Max carried a primary armament of four 24 @-@ centimeter ( 9 @.@ 4 in ) / 40 caliber guns in two twin turrets on the centerline . These guns were an Austro @-@ Hungarian replica of the British 24 cm / 40 ( 9 @.@ 4 \" ) Krupp C / 94 , which was used on the Habsburgs . Her secondary armament consisted of twelve 19 @-@ centimeter ( 7 @.@ 5 in ) / 42 caliber guns , also made by Škoda , mounted in eight single casemates on either wing of the ship and two twin turrets on the <unk> shell 20 @,@ 000 metres ( 22 @,@ 000 yd ) at maximum elevation with a muzzle velocity of 800 metres per second ( 2 @,@ 600 ft / s ) . The gun weighed 12 @.@ 1 tons and could fire three rounds per <unk> ships had a tertiary armament for protection against torpedo boats in the form of the 6 @.@ 6 centimetres ( 2 @.@ 6 in ) / 45 caliber gun , also manufactured by Škoda . Anti @-@ aircraft and airship protection was covered by the four 37 @-@ millimeter ( 1 @.@ 5 in ) Vickers anti @-@ aircraft guns on the ship bought from Britain in 1910 and mounted onto Erzherzog Karl . Erzherzog Ferdinand Max was also fitted with two above water 45 @-@ centimeter ( 17 @.@ 7 in ) torpedo tubes , although rarely used . \\n',\n",
       " '',\n",
       " ' = = Service history = = \\n',\n",
       " '',\n",
       " ' At the outbreak of World War I , Erzherzog Ferdinand Max was in the III division of the Austrian @-@ Hungarian battle @-@ fleet . She was mobilized on the eve of the war along with the remainder of the fleet to support the flight of SMS Goeben and SMS Breslau . The two German ships were attempting to break out of Messina , which was surrounded by British troops , and make their way to Turkey . The breakout succeeded . When the flotilla had advanced as far south as Brindisi in south eastern Italy , the Austro @-@ Hungarian ships were recalled . In company with other units of the Austro Hungarian navy , Erzherzog Ferdinand Max took a minor part in the bombardment of Ancona on 24 May 1915 . There she and her sisters expended 24 rounds of 240 mm armor @-@ piercing shells at signal and semaphore stations as well as 74 rounds of 190 mm shells aimed at Italian gun @-@ batteries and other port installations . \\n',\n",
       " \" A major mutiny among crews of the armored cruisers stationed in Cattaro , including Sankt Georg and Kaiser Karl VI , began on 1 February 1918 . Two days later , Erzherzog Ferdinand Max and her sisters arrived in the port and assisted with the suppression of the mutiny . Following the restoration of order in the naval base , the armored cruisers Sankt Georg and Kaiser Karl VI were decommissioned and Erzherzog Ferdinand Max and her sisters were stationed in Cattaro in their place . On the morning of 11 June , Admiral Miklos Horthy planned a major assault on the Otranto Barrage ; the three Erzherzog Karls and the four Tegetthoff @-@ class battleships were to provide support for the Novara @-@ class cruisers on an assault on the Allied defenses at the Strait of Otranto . The plan was intended to replicate the success of the raid conducted one year earlier . Horthy 's plan was to destroy the blockading fleet by luring Allied ships to the cruisers and lighter ships , which were protected from the heavier guns of the battleships , including the guns of the Erzherzog Karl class . However , on the morning of 10 June , the dreadnought Szent István was torpedoed and sunk by an Italian torpedo boat . Horthy felt that the element of surprise had been compromised , and therefore called off the operation . This was to be the last military action Erzherzog Ferdinand Max was to take part in , and she and her sisters spent the rest of their career in port . \\n\",\n",
       " ' Near the end of World War I , the Erzherzog Karl @-@ class battleships were handed over to the newly formed State of Slovenes , Croats and Serbs but Erzherzog Ferdinand Max was later transferred to Great Britain as a war reparation . She was later broken up for scrap in 1921 . \\n',\n",
       " '',\n",
       " '',\n",
       " ' = Ancient Egyptian deities = \\n',\n",
       " '',\n",
       " \" Ancient Egyptian deities are the gods and goddesses worshipped in ancient Egypt . The beliefs and rituals surrounding these gods formed the core of ancient Egyptian religion , which emerged sometime in prehistory . Deities represented natural forces and phenomena , and the Egyptians supported and appeased them through offerings and rituals so that these forces would continue to function according to maat , or divine order . After the founding of the Egyptian state around 3100 BC , the authority to perform these tasks was controlled by the pharaoh , who claimed to be the gods ' representative and managed the temples where the rituals were carried out . \\n\",\n",
       " \" The gods ' complex characteristics were expressed in myths and in intricate relationships between deities : family ties , loose groups and hierarchies , and combinations of separate gods into one . Deities ' diverse appearances in art — as animals , humans , objects , and combinations of different forms — also alluded , through symbolism , to their essential features . \\n\",\n",
       " ' In different eras , various gods were said to hold the highest position in divine society , including the solar deity Ra , the mysterious god Amun , and the mother goddess Isis . The highest deity was usually credited with the creation of the world and often connected with the life @-@ giving power of the sun . Some scholars have argued , based in part on Egyptian writings , that the Egyptians came to recognize a single divine power that lay behind all things and was present in all the other deities . Yet they never abandoned their original polytheistic view of the world , except possibly during the era of Atenism in the 14th century BC , when official religion focused exclusively on the impersonal sun god Aten . \\n',\n",
       " \" Gods were assumed to be present throughout the world , capable of influencing natural events and the course of human lives . People interacted with them in temples and unofficial shrines , for personal reasons as well as for larger goals of state rites . Egyptians prayed for divine help , used rituals to compel deities to act , and called upon them for advice . Humans ' relations with their gods were a fundamental part of Egyptian society . \\n\",\n",
       " '',\n",
       " ' = = Definition = = \\n',\n",
       " '',\n",
       " ' The beings in ancient Egyptian tradition who might be labeled as deities are difficult to count . Egyptian texts list the names of many deities whose nature is unknown and make vague , indirect references to other gods who are not even named . The Egyptologist James P. Allen estimates that more than 1 @,@ 400 deities are named in Egyptian texts , whereas his colleague Christian Leitz says there are \" thousands upon thousands \" of gods . \\n',\n",
       " ' The Egyptian language \\'s terms for these beings were nṯr , \" god \" , and its feminine form <unk> , \" goddess \" . Scholars have tried to discern the original nature of the gods by proposing etymologies for these words , but none of these suggestions has gained acceptance , and the terms \\' origin remains obscure . The hieroglyphs that were used as ideograms and determinatives in writing these words show some of the traits that the Egyptians connected with divinity . The most common of these signs is a flag flying from a pole . Similar objects were placed at the entrances of temples , representing the presence of a deity , throughout ancient Egyptian history . Other such hieroglyphs include a falcon , reminiscent of several early gods who were depicted as falcons , and a seated male or female deity . The feminine form could also be written with an egg as determinative , connecting goddesses with creation and birth , or with a cobra , reflecting the use of the cobra to depict many female deities . \\n',\n",
       " ' The Egyptians distinguished <unk> , \" gods \" , from <unk> , \" people \" , but the meanings of the Egyptian and the English terms do not match perfectly . The term nṯr may have applied to any being that was in some way outside the sphere of everyday life . Deceased humans were called nṯr because they were considered to be like the gods , whereas the term was rarely applied to many of Egypt \\'s lesser supernatural beings , which modern scholars often call \" demons \" . Egyptian religious art also depicts places , objects , and concepts in human form . These personified ideas range from deities that were important in myth and ritual to obscure beings , only mentioned once or twice , that may be little more than metaphors . \\n',\n",
       " ' Confronting these blurred distinctions between gods and other beings , scholars have proposed various definitions of a \" deity \" . One widely accepted definition , suggested by Jan Assmann , says that a deity has a cult , is involved in some aspect of the universe , and is described in mythology or other forms of written tradition . According to a different definition , by Dimitri Meeks , nṯr applied to any being that was the focus of ritual . From this perspective , \" gods \" included the king , who was called a god after his coronation rites , and deceased souls , who entered the divine realm through funeral ceremonies . Likewise , the preeminence of the great gods was maintained by the ritual devotion that was performed for them across Egypt . \\n',\n",
       " '',\n",
       " ' = = Origins = = \\n',\n",
       " '',\n",
       " ' The first written evidence of deities in Egypt comes from the Early Dynastic Period ( c . 3100 – 2686 BC ) . Deities must have emerged sometime in the preceding Predynastic Period ( before 3100 BC ) and grown out of prehistoric religious beliefs . Predynastic artwork depicts a variety of animal and human figures . Some of these images , such as stars and cattle , are reminiscent of important features of Egyptian religion in later times , but in most cases there is not enough evidence to say whether the images are connected with deities . As Egyptian society grew more sophisticated , clearer signs of religious activity appeared . The earliest known temples appeared in the last centuries of the predynastic era , along with images that resemble the iconographies of known deities : the falcon that represents Horus and several other gods , the crossed arrows that stand for Neith , and the enigmatic \" Set animal \" that represents Set . \\n',\n",
       " \" Many Egyptologists and anthropologists have suggested theories about how the gods developed in these early times . Gustave <unk> , for instance , thought the Egyptians first revered primitive fetishes , then deities in animal form , and finally deities in human form , whereas Henri Frankfort argued that the gods must have been envisioned in human form from the beginning . Some of these theories are now regarded as too simplistic , and more current ones , such as Siegfried Morenz ' hypothesis that deities emerged as humans began to distinguish themselves from and personify their environment , are difficult to prove . \\n\",\n",
       " ' Predynastic Egypt originally consisted of small , independent villages . Because many deities in later times were strongly tied to particular towns and regions , many scholars have suggested that the pantheon formed as disparate communities coalesced into larger states , spreading and intermingling the worship of the old local deities . But others have argued that the most important predynastic gods were , like other elements of Egyptian culture , present all across the country despite the political divisions within it . \\n',\n",
       " ' The final step in the formation of Egyptian religion was the unification of Egypt , in which rulers from Upper Egypt made themselves pharaohs of the entire country . These sacred kings and their subordinates assumed the exclusive right to interact with the gods , and kingship became the unifying focus of the religion . \\n',\n",
       " ' New gods continued to emerge after this transformation . Some important deities like Isis and Amun are not known to have appeared until the Old Kingdom ( c . 2686 – 2181 BC ) . Places and concepts could suddenly inspire the creation of a deity to represent them , and deities were sometimes created to serve as opposite @-@ sex counterparts to established gods or goddesses . Kings were said to be divine , although only a few continued to be worshipped long after their deaths . Some non @-@ royal humans were said to have the favor of the gods and were venerated accordingly . This veneration was usually short @-@ lived , but the court architects Imhotep and Amenhotep son of <unk> were regarded as gods centuries after their lifetimes , as were some other officials . \\n',\n",
       " ' Through contact with neighboring civilizations , the Egyptians also adopted foreign deities . <unk> , who is first mentioned in the Old Kingdom , may have come from Nubia , and Baal , Anat , and Astarte , among others , were adopted from Canaanite religion during the New Kingdom ( c . 1550 – 1070 BC ) . In Greek and Roman times , from 332 BC to the early centuries AD , deities from across the Mediterranean world were revered in Egypt , but the native gods remained , and they often absorbed the cults of these newcomers into their own worship . \\n',\n",
       " '',\n",
       " ' = = Characteristics = = \\n',\n",
       " '',\n",
       " \" Modern knowledge of Egyptian beliefs about the gods is mostly drawn from religious writings produced by the nation 's scribes and priests . These people were the elite of Egyptian society and were very distinct from the general populace , most of whom were illiterate . Little is known about how well this broader population knew or understood the sophisticated ideas that the elite developed . Commoners ' perceptions of the divine may have differed from those of the priests . The populace may , for example , have mistaken the religion 's symbolic statements about the gods and their actions for literal truth . But overall , what little is known about popular religious belief is consistent with the elite tradition . The two traditions form a largely cohesive vision of the gods and their nature . \\n\",\n",
       " '',\n",
       " ' = = = Roles = = = \\n',\n",
       " '',\n",
       " \" Most Egyptian deities represent natural or social phenomena . The gods were generally said to be immanent in these phenomena — to be present within nature . The types of phenomena they represented include physical places and objects as well as abstract concepts and forces . The god Shu was the deification of all the world 's air ; the goddess <unk> oversaw a limited region of the earth , the Theban Necropolis ; and the god Sia personified the abstract notion of perception . Major gods often had many roles and were involved in several types of phenomena . For instance , Khnum was the god of Elephantine Island in the midst of the Nile , the river that was essential to Egyptian civilization . He was credited with producing the annual Nile flood that fertilized the nation 's farmland . Perhaps as an outgrowth of this life @-@ giving function , he was said to create all living things , fashioning their bodies on a potter 's wheel . Gods could share the same role in nature ; Ra , Atum , <unk> , Horus , and other deities acted as sun gods . Despite their diverse functions , most gods had an overarching role in common : maintaining maat , the universal order that was a central principle of Egyptian religion and was itself personified as a goddess . But some deities represented disruption to maat . Most prominently , Apep was the force of chaos , constantly threatening to annihilate the order of the universe , and Set was an ambivalent member of divine society who could both fight disorder and foment it . \\n\",\n",
       " ' Not all aspects of existence were seen as deities . Although many deities were connected with the Nile , no god personified it in the way that Ra personified the sun . Short @-@ lived phenomena , like rainbows or eclipses , were not represented by gods ; neither were elements like fire and water or many other components of the world . \\n',\n",
       " \" The roles of each deity were fluid , and each god could expand its nature to take on new characteristics . As a result , gods ' roles are difficult to categorize or define . But despite their flexibility , the gods had limited abilities and spheres of influence . Not even the creator god could reach beyond the boundaries of the cosmos that he created , and even Isis , though she was said to be the cleverest of the gods , was not omniscient . Richard H. Wilkinson , however , argues that some texts from the late New Kingdom suggest that , as beliefs about the god Amun evolved , he was thought to approach omniscience and omnipresence and to transcend the limits of the world in a way that other deities did not . \\n\",\n",
       " ' The deities with the most limited and specialized domains are often called \" minor divinities \" or \" demons \" in modern writing , although there is no firm definition for these terms . Among these lesser deities , Egyptologist Claude <unk> draws a distinction between \" genies \" — specialized patron spirits of certain places , objects , or activities , such as the sea or marsh god <unk> @-@ Wer and the harvest goddess Renenutet — and demons , who have a more dangerous character . Many demons are hostile , causing illness and other troubles among humans . Their power can also be protective ; they may guard certain places in the Duat , the realm of the dead , or advise and watch over humans . Egyptians believed the landscape was full of these unpredictable divine powers . Demons often act as servants and messengers to the greater gods , but their position in the hierarchy is not fixed . The protective deities <unk> and <unk> originally had minor , demon @-@ like roles , but over time they came to be credited with great influence . \\n',\n",
       " '',\n",
       " ' = = = Behavior = = = \\n',\n",
       " '',\n",
       " ' Divine behavior was believed to govern all of nature . Except for the few deities who disrupted the divine order , the gods \\' actions maintained maat and created and sustained all living things . They did this work using a force the Egyptians called heka , a term usually translated as \" magic \" . <unk> was a fundamental power that the creator god used to form the world and the gods themselves . \\n',\n",
       " ' The gods \\' actions in the present are described and praised in hymns and funerary texts . In contrast , mythology mainly concerns the gods \\' actions during a vaguely imagined past in which the gods were present on earth and interacted directly with humans . The events of this past time set the pattern for the events of the present . Periodic occurrences were tied to events in the mythic past ; the succession of each new pharaoh , for instance , reenacted Horus \\' accession to the throne of his father Osiris . Myths are metaphors for the gods \\' actions , which humans cannot fully understand . They contain seemingly contradictory ideas , each expressing a particular perspective on divine events . The contradictions in myth are part of the Egyptians \\' many @-@ faceted approach to religious belief — what Henri Frankfort called a \" multiplicity of approaches \" to understanding the gods . \\n',\n",
       " ' In myth , the gods behave much like humans . They feel emotion ; they can eat , drink , fight , weep , sicken , and die . Some have unique character traits . Set is aggressive and impulsive , and Thoth , patron of writing and knowledge , is prone to long @-@ winded speeches . Yet overall , the gods are more like archetypes than well drawn characters . Their behavior is inconsistent , and their thoughts and motivations are rarely stated . Most myths about them lack highly developed characters and plots , because the symbolic meaning of the myths was more important than elaborate storytelling . \\n',\n",
       " ' The first divine act is the creation of the cosmos , described in several creation myths . They focus on different gods , each of which may act as creator deities . The eight gods of the <unk> , who represent the chaos that precedes creation , give birth to the sun god , who establishes order in the newly formed world ; Ptah , who embodies thought and creativity , gives form to all things by envisioning and naming them ; Atum produces all things as emanations of himself ; and Amun , according to the myths promoted by his priesthood , preceded and created the other creator gods . These and other versions of the events of creation were not seen as contradictory . Each gives a different perspective on the complex process by which the organized universe and its many deities emerged from undifferentiated chaos . The period following creation , in which a series of gods rule as kings over the divine society , is the setting for most myths . The gods struggle against the forces of chaos and among each other before withdrawing from the human world and installing the historical kings of Egypt to rule in their place . \\n',\n",
       " \" A recurring theme in these myths is the effort of the gods to maintain maat against the forces of disorder . They fight vicious battles with the forces of chaos at the start of creation . Ra and Apep , battling each other each night , continue this struggle into the present . Another prominent theme is the gods ' death and revival . The clearest instance where a god dies is the myth of Osiris ' murder , in which that god is resurrected as ruler of the Duat . The sun god is also said to grow old during his daily journey across the sky , sink into the Duat at night , and emerge as a young child at dawn . In the process he comes into contact with the rejuvenating water of primordial chaos . Funerary texts that depict Ra 's journey through the Duat also show the corpses of gods who are enlivened along with him . Instead of being <unk> immortal , the gods periodically died and were reborn by repeating the events of creation , thus renewing the whole world . But it was always possible for this cycle to be disrupted and for chaos to return . Some poorly understood Egyptian texts even suggest that this calamity is destined to happen — that the creator god will one day dissolve the order of the world , leaving only himself and Osiris amid the primordial chaos . \\n\",\n",
       " '',\n",
       " ' = = = Locations = = = \\n',\n",
       " '',\n",
       " \" Gods were linked with specific regions of the universe . In Egyptian tradition , the world includes the earth , the sky , and the Duat . Surrounding them is the dark formlessness that existed before creation . The gods in general were said to dwell in the sky , although gods whose roles were linked with other parts of the universe were said to live in those places instead . Most events of mythology , set in a time before the gods ' withdrawal from the human realm , take place in an earthly setting . The deities there sometimes interact with those in the sky . The Duat , in contrast , is treated as a remote and inaccessible place , and the gods who dwell there have difficulty communicating with those in the world of the living . The space outside the cosmos is also said to be very distant . It too is inhabited by deities , some hostile and some beneficial to the other gods and their orderly world . \\n\",\n",
       " \" In the time after myth , most gods were said to be either in the sky or invisibly present within the world . Temples were their main means of contact with humanity . Each day , it was believed , the gods moved from the divine realm to their temples , their homes in the human world . There they inhabited the cult images , the statues that depicted deities and allowed humans to interact with them in temple rituals . This movement between realms was sometimes described as a journey between the sky and the earth . As temples were the focal points of Egyptian cities , the god in a city 's main temple was the patron god for the city and the surrounding region . Deities ' spheres of influence on earth centered on the towns and regions they presided over . Many gods had more than one cult center , and their local ties changed over time . They could establish themselves in new cities , or their range of influence could contract . Therefore , a given deity 's main cult center in historical times is not necessarily his or her place of origin . The political influence of a city could affect the importance of its patron deity . When kings from Thebes took control of the country at start of the Middle Kingdom ( c . 2055 – 1650 BC ) , they elevated Thebes ' patron gods — first the war god Montu and then Amun — to national prominence . \\n\",\n",
       " '',\n",
       " ' = = = Names and epithets = = = \\n',\n",
       " '',\n",
       " ' In Egyptian belief , names express the fundamental nature of the things to which they refer . In keeping with this belief , the names of deities often relate to their roles or origins . The name of the predatory goddess Sekhmet means \" powerful one \" , the name of the mysterious god Amun means \" hidden one \" , and the name of the goddess Nekhbet , who was worshipped in the city of <unk> , means \" she of <unk> \" . But many other names have no certain meaning , even when the gods who bear them are closely tied to a single role . The names of the sky goddess Nut and the earth god Geb do not resemble the Egyptian terms for sky and earth . \\n',\n",
       " ' The Egyptians also devised false etymologies giving more meanings to divine names . A passage in the Coffin Texts renders the name of the funerary god <unk> as sk r , meaning \" cleaning of the mouth \" , to link his name with his role in the Opening of the Mouth ritual , while one in the Pyramid Texts says the name is based on words shouted by Osiris , connecting <unk> with the most important funerary deity . \\n',\n",
       " ' The gods were believed to have many names . Among them were secret names that conveyed their true natures more profoundly than others . To know the true name of a deity was to have power over it . The importance of names is demonstrated by a myth in which Isis poisons the superior god Ra and refuses to cure him unless he reveals his secret name to her . Upon learning the name , she tells it to her son , Horus , and by learning it they gain greater knowledge and power . \\n',\n",
       " ' In addition to their names , gods were given epithets , like \" possessor of splendor \" , \" ruler of Abydos \" , or \" lord of the sky \" , that describe some aspect of their roles or their worship . Because of the gods \\' multiple and overlapping roles , deities can have many epithets — with more important gods accumulating more titles — and the same epithet can apply to many deities . Some epithets eventually became separate deities , as with <unk> , an epithet applied to several goddesses meaning \" great enchantress \" , which came to be treated as an independent goddess . The host of divine names and titles expresses the gods \\' multifarious nature . \\n',\n",
       " '',\n",
       " ' = = = Relationships = = = \\n',\n",
       " '',\n",
       " \" Egyptian deities are connected in a complex and shifting array of relationships . A god 's connections and interactions with other deities helped define its character . Thus Isis , as the mother and protector of Horus , was a great healer as well as the patroness of kings . Such relationships were the base material from which myths were formed . \\n\",\n",
       " \" Family relationships are a common type of connection between gods . Deities often form male and female pairs , reflecting the importance of procreation in Egyptian religious thought . Families of three deities , with a father , mother , and child , represent the creation of new life and the succession of the father by the child , a pattern that connects divine families with royal succession . Osiris , Isis , and Horus formed the quintessential family of this type . The pattern they set grew more widespread over time , so that many deities in local cult centers , like Ptah , Sekhmet , and their child <unk> at Memphis and Amun , Mut , and Khonsu at Thebes , were assembled into family triads . Genealogical connections like these are changeable , in keeping with the multiple perspectives in Egyptian belief . Hathor , as a fertility goddess , could act as mother to any child god , including the child form of the sun god , although in other circumstances she was the sun god 's daughter . \\n\",\n",
       " ' Other divine groups were composed of deities with interrelated roles , or who together represented a region of the Egyptian mythological cosmos . There were sets of gods for the hours of the day and night and for each nome ( province ) of Egypt . Some of these groups contain a specific , symbolically important number of deities . Paired gods can stand for opposite but interrelated concepts that are part of a greater unity . Ra , who is dynamic and light @-@ producing , and Osiris , who is static and shrouded in darkness , merge into a single god each night . Groups of three are linked with plurality in ancient Egyptian thought , and groups of four connote completeness . Rulers in the late New Kingdom promoted a particularly important group of three gods above all others : Amun , Ra , and Ptah . These deities stood for the plurality of all gods , as well as for their own cult centers ( the major cities of Thebes , Heliopolis , and Memphis ) and for many threefold sets of concepts in Egyptian religious thought . Sometimes Set , the patron god of the Nineteenth Dynasty kings and the embodiment of disorder within the world , was added to this group , which emphasized a single coherent vision of the pantheon . \\n',\n",
       " ' Nine , the product of three and three , represents a multitude , so the Egyptians called several large groups \" <unk> \" , or sets of nine , even if they had more than nine members . The most prominent ennead was the Ennead of Heliopolis , an extended family of deities descended from the creator god Atum , which incorporates many important gods . The term \" ennead \" was often extended to include all of Egypt \\'s deities . \\n',\n",
       " ' This divine assemblage had a vague and changeable hierarchy . Gods with broad influence in the cosmos or who were mythologically older than others had higher positions in divine society . At the apex of this society was the king of the gods , who was usually identified with the creator deity . In different periods of Egyptian history , different gods were most frequently said to hold this exalted position . Horus was the most important god in the Early Dynastic Period , Ra rose to preeminence in the Old Kingdom , Amun was supreme in the New , and in the Ptolemaic and Roman periods , Isis was the divine queen and creator goddess . Newly prominent gods tended to adopt characteristics from their predecessors . Isis absorbed the traits of many other goddesses during her rise , and when Amun became the ruler of the pantheon , he was conjoined with Ra , the traditional king of the gods , to become a solar deity . \\n',\n",
       " '',\n",
       " ' = = = Manifestations and combinations = = = \\n',\n",
       " '',\n",
       " \" The gods were believed to manifest in many forms . The Egyptians had complex conception of the human soul , consisting of several parts . The spirits of the gods were composed of many of these same elements . The ba was the component of the human or divine soul that affected the world around it . Any visible manifestation of a god 's power could be called its ba ; thus , the sun was called the ba of Ra . A depiction of a deity was considered a ka , another component of its being , which acted as a vessel for that deity 's ba to inhabit . The cult images of gods that were the focus of temple rituals , as well as the sacred animals that represented certain deities , were believed to house divine bas in this way . Gods could be ascribed many bas and kas , which were sometimes given names representing different aspects of the god 's nature . Everything in existence was said to be one of the kas of Atum the creator god , who originally contained all things within himself , and one deity could be called the ba of another , meaning that the first god is a manifestation of the other 's power . Divine body parts could act as separate deities , like the Eye of Ra and Hand of Atum , both of which were personified as goddesses . \\n\",\n",
       " \" Nationally important deities gave rise to local manifestations , which sometimes absorbed the characteristics of older regional gods . Horus had many forms tied to particular places , including Horus of Nekhen , Horus of Buhen , and Horus of Edfu . Such local manifestations could be treated almost as separate beings . During the New Kingdom , one man was accused of stealing clothes by an oracle supposed to communicate messages from Amun of Pe @-@ Khenty . He consulted two other local oracles of Amun hoping for a different judgment . Gods ' manifestations also differed according to their roles . Horus could be a powerful sky god or vulnerable child , and these forms were sometimes counted as independent deities . \\n\",\n",
       " ' Gods were combined with each other as easily as they were divided . A god could be called the ba of another , or two or more deities could be joined into one god with a combined name and iconography . Local gods were linked with greater ones , and deities with similar functions were combined . Ra was connected with the local deity Sobek to form Sobek @-@ Ra ; with his fellow ruling god , Amun , to form Amun @-@ Ra ; with the solar form of Horus to form Ra @-@ <unk> ; and with several solar deities as <unk> @-@ <unk> @-@ Ra @-@ Atum . On rare occasion , even deities of different sexes were joined in this way , producing combinations like Osiris @-@ Neith and Mut @-@ Min . This linking of deities is called syncretism . Unlike other situations for which this term is used , the Egyptian practice was not meant to fuse competing belief systems , although foreign deities could be syncretized with native ones . Instead , syncretism acknowledged the overlap between their roles , and extended the sphere of influence for each of them . <unk> combinations were not permanent ; a god who was involved in one combination continued to appear separately and to form new combinations with other deities . But closely connected deities did sometimes merge . Horus absorbed several falcon gods from various regions , such as Khenty @-@ <unk> and Khenty @-@ <unk> , who became little more than local manifestations of him ; Hathor subsumed a similar cow goddess , Bat ; and an early funerary god , <unk> @-@ <unk> , was supplanted by Osiris and Anubis . \\n',\n",
       " '',\n",
       " ' = = = The Aten and possible monotheism = = = \\n',\n",
       " '',\n",
       " \" In the reign of Akhenaten ( c . 1353 – 1336 BC ) in the mid @-@ New Kingdom , a single solar deity , the Aten , became the sole focus of the state religion . Akhenaten ceased to fund the temples of other deities and erased the gods ' names and images on monuments , targeting Amun in particular . This new religious system , sometimes called Atenism , differed dramatically from the polytheistic worship of many gods in all other periods . Whereas , in earlier times , newly important gods were integrated into existing religious beliefs , Atenism insisted on a single understanding of the divine that excluded the traditional multiplicity of perspectives . Yet Atenism may not have been full monotheism , which totally excludes belief in other deities . There is evidence suggesting that the general populace was still allowed to worship other gods in private . The picture is further complicated by Atenism 's apparent tolerance for some other deities , like Shu . For these reasons , the Egyptologist Dominic Montserrat suggested that Akhenaten may have been <unk> , worshipping a single deity while acknowledging the existence of others . In any case , Atenism 's aberrant theology did not take root among the Egyptian populace , and Akhenaten 's successors returned to traditional beliefs . \\n\",\n",
       " '',\n",
       " ' = = = Unity of the divine in traditional religion = = = \\n',\n",
       " '',\n",
       " ' Scholars have long debated whether traditional Egyptian religion ever asserted that the multiple gods were , on a deeper level , unified . Reasons for this debate include the practice of syncretism , which might suggest that all the separate gods could ultimately merge into one , and the tendency of Egyptian texts to credit a particular god with power that surpasses all other deities . Another point of contention is the appearance of the word \" god \" in wisdom literature , where the term does not refer to a specific deity or group of deities . In the early 20th century , for instance , E. A. Wallis Budge believed that Egyptian commoners were polytheistic , but knowledge of the true monotheistic nature of the religion was reserved for the elite , who wrote the wisdom literature . His contemporary James Henry <unk> thought Egyptian religion was instead pantheistic , with the power of the sun god present in all other gods , while Hermann Junker argued that Egyptian civilization had been originally monotheistic and became polytheistic in the course of its history . \\n',\n",
       " ' In 1971 , Erik Hornung published a study rebutting these views . He points out that in any given period many deities , even minor ones , were described as superior to all others . He also argues that the unspecified \" god \" in the wisdom texts is a generic term for whichever deity the reader chooses to revere . Although the combinations , manifestations , and iconographies of each god were constantly shifting , they were always restricted to a finite number of forms , never becoming fully interchangeable in a monotheistic or pantheistic way . <unk> , Hornung says , describes Egyptian religion better than other labels . An Egyptian could worship any deity at a particular time and credit it with supreme power in that moment , without denying the other gods or merging them all with the god that he or she focused on . Hornung concludes that the gods were fully unified only in myth , at the time before creation , after which the multitude of gods emerged from a uniform nonexistence . \\n',\n",
       " ' Hornung \\'s arguments have greatly influenced other scholars of Egyptian religion , but some still believe that at times the gods were more unified than he allows . Jan Assmann maintains that the notion of a single deity developed slowly through the New Kingdom , beginning with a focus on Amun @-@ Ra as the all @-@ important sun god . In his view , Atenism was an extreme outgrowth of this trend . It equated the single deity with the sun and dismissed all other gods . Then , in the backlash against Atenism , priestly theologians described the universal god in a different way , one that coexisted with traditional polytheism . The one god was believed to transcend the world and all the other deities , while at the same time , the multiple gods were aspects of the one . According to Assmann , this one god was especially equated with Amun , the dominant god in the late New Kingdom , whereas for the rest of Egyptian history the universal deity could be identified with many other gods . James P. Allen says that coexisting notions of one god and many gods would fit well with the \" multiplicity of approaches \" in Egyptian thought , as well as with the <unk> practice of ordinary worshippers . He says that the Egyptians may have recognized the unity of the divine by \" identifying their uniform notion of \\' god \\' with a particular god , depending on the particular situation . \" \\n',\n",
       " '',\n",
       " ' = = Descriptions and depictions = = \\n',\n",
       " '',\n",
       " ' Egyptian writings describe the gods \\' bodies in detail . They are made of precious materials ; their flesh is gold , their bones are silver , and their hair is lapis lazuli . They give off a scent that the Egyptians likened to the incense used in rituals . Some texts give precise descriptions of particular deities , including their height and eye color . Yet these characteristics are not fixed ; in myths , gods change their appearances to suit their own purposes . Egyptian texts often refer to deities \\' true , underlying forms as \" mysterious \" . The Egyptians \\' visual representations of their gods are therefore not literal . They symbolize specific aspects of each deity \\'s character , functioning much like the ideograms in hieroglyphic writing . For this reason , the funerary god Anubis is commonly shown in Egyptian art as a dog or jackal , a creature whose scavenging habits threaten the preservation of buried mummies , in an effort to counter this threat and employ it for protection . His black coloring alludes to the color of mummified flesh and to the fertile black soil that Egyptians saw as a symbol of resurrection . \\n',\n",
       " \" Most gods were depicted in several ways . Hathor could be a cow , cobra , lioness , or a woman with bovine horns or ears . By depicting a given god in different ways , the Egyptians expressed different aspects of its essential nature . The gods are depicted in a finite number of these symbolic forms , so that deities can often be distinguished from one another by their iconographies . These forms include men and women ( anthropomorphism ) , animals ( <unk> ) , and , more rarely , inanimate objects . Combinations of forms , such as gods with human bodies and animal heads , are common . New forms and increasingly complex combinations arose in the course of history . Some gods can only be distinguished from others if they are labeled in writing , as with Isis and Hathor . Because of the close connection between these goddesses , they could both wear the cow @-@ horn headdress that was originally Hathor 's alone . \\n\",\n",
       " ' Certain features of divine images are more useful than others in determining a god \\'s identity . The head of a given divine image is particularly significant . In a hybrid image , the head represents the original form of the being depicted , so that , as the Egyptologist Henry Fischer put it , \" a lion @-@ headed goddess is a lion @-@ goddess in human form , while a royal sphinx , conversely , is a man who has assumed the form of a lion . \" Divine headdresses , which range from the same types of crowns used by human kings to large hieroglyphs worn on gods \\' heads , are another important indicator . In contrast , the objects held in gods \\' hands tend to be generic . Male deities hold was staffs , goddesses hold stalks of papyrus , and both sexes carry ankh signs , representing the Egyptian word for \" life \" , to symbolize their life @-@ giving power . \\n',\n",
       " ' The forms in which the gods are shown , although diverse , are limited in many ways . Many creatures that are widespread in Egypt were never used in divine iconography , whereas a few , such as falcons , cobras , and cattle , can each represent many deities . Animals that were absent from Egypt in the early stages of its history were not used as divine images . For instance , the horse , which was only introduced in the Second Intermediate Period ( c . 1650 – 1550 BC ) , never represented a god . Similarly , the clothes worn by anthropomorphic deities in all periods changed little from the styles used in the Old Kingdom : a kilt , false beard , and often a shirt for male gods and a long , tight @-@ fitting dress for goddesses . \\n',\n",
       " ' The basic anthropomorphic form varies . Child gods are depicted nude , as are some adult gods when their procreative powers are emphasized . Certain male deities are given heavy bellies and breasts , signifying either androgyny or prosperity and abundance . Whereas most male gods have red skin and most goddesses are yellow — the same colors used to depict Egyptian men and women — some are given unusual , symbolic skin colors . Thus the blue skin and paunchy figure of the god <unk> alludes to the Nile flood he represents and the nourishing fertility it brought . A few deities , such as Osiris , Ptah , and Min , have a \" <unk> \" appearance , with their limbs tightly swathed in cloth . Although these gods resemble mummies , the earliest examples predate the cloth @-@ wrapped style of mummification , and this form may instead hark back to the earliest , limbless depictions of deities . \\n',\n",
       " '',\n",
       " ' = = Interactions with humans = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = Relationship with the pharaoh = = = \\n',\n",
       " '',\n",
       " ' In official writings , pharaohs are said to be divine , and they are constantly depicted in the company of the deities of the pantheon . Each pharaoh and his predecessors were considered the successors of the gods who had ruled Egypt in mythic prehistory . Living kings were equated with Horus and called the \" son \" of many deities , particularly Osiris and Ra ; deceased kings were equated with these elder gods . Pharaohs had their own mortuary temples where rituals were performed for them during their lives and after their deaths . But few pharaohs were worshipped as gods long after their lifetimes , and non @-@ official texts portray kings in a human light . For these reasons , scholars disagree about how genuinely most Egyptians believed the king to be a god . He may only have been considered divine when he was performing ceremonies . \\n',\n",
       " \" However much it was believed , the king 's divine status was the rationale for his role as Egypt 's representative to the gods , as he formed a link between the divine and human realms . The Egyptians believed the gods needed temples to dwell in , as well as the periodic performance of rituals and presentation of offerings to nourish them . These things were provided by the cults that the king oversaw , with their priests and laborers . Yet , according to royal ideology , temple @-@ building was exclusively the pharaoh 's work , as were the rituals that priests usually performed in his stead . These acts were a part of the king 's fundamental role : maintaining maat . The king and the nation he represented provided the gods with maat so they could continue to perform their functions , which maintained maat in the cosmos so humans could continue to live . \\n\",\n",
       " '',\n",
       " ' = = = Presence in the human world = = = \\n',\n",
       " '',\n",
       " ' Although the Egyptians believed their gods to be present in the world around them , contact between the human and divine realms was mostly limited to specific circumstances . In literature , gods may appear to humans in a physical form , but in real life the Egyptians were limited to more indirect means of communication . \\n',\n",
       " ' The ba of a god was said to periodically leave the divine realm to dwell in the images of that god . By inhabiting these images , the gods left their concealed state and took on a physical form . To the Egyptians , a place or object that was <unk> — \" sacred \" — was isolated and ritually pure , and thus fit for a god to inhabit . Temple statues and reliefs , as well as particular sacred animals , like the Apis bull , served as divine intermediaries in this way . Dreams and trances provided a very different venue for interaction . In these states , it was believed , people could come close to the gods and sometimes receive messages from them . Finally , according to Egyptian afterlife beliefs , human souls pass into the divine realm after death . The Egyptians therefore believed that in death they would exist on the same level as the gods and fully understand their mysterious nature . \\n',\n",
       " \" Temples , where the state rituals were carried out , were filled with images of the gods . The most important temple image was the cult statue in the inner sanctuary . These statues were usually less than life @-@ size , and made of the same precious materials that were said to form the gods ' bodies . Many temples had several sanctuaries , each with a cult statue representing one of the gods in a group such as a family triad . The city 's primary god was envisioned as its lord , employing many of the residents as servants in the divine household that the temple represented . The gods residing in the temples of Egypt collectively represented the entire pantheon . But many deities — including some important gods as well as those that were minor or hostile — were never given temples of their own , although some were represented in the temples of other gods . \\n\",\n",
       " ' To insulate the sacred power in the sanctuary from the impurities of the outside world , the Egyptians enclosed temple sanctuaries and greatly restricted access to them . People other than kings and high priests were thus denied contact with cult statues . The only exception was during festival processions , when the statue was carried out of the temple but still enclosed in a portable shrine . People did have less direct means of interaction . The more public parts of temples often incorporated small places for prayer , from doorways to freestanding chapels near the back of the temple building . Communities also built and managed small chapels for their own use , and some families had shrines inside their homes . Despite the gulf that separated humanity from the divine , the Egyptians were surrounded by opportunities to approach their gods . \\n',\n",
       " '',\n",
       " ' = = = Intervention in human lives = = = \\n',\n",
       " '',\n",
       " \" Egyptian gods were involved in human lives as well as in the overarching order of nature . This divine influence applied mainly to Egypt , as foreign peoples were traditionally believed to be outside the divine order . But in the New Kingdom , when other nations were under Egyptian control , foreigners were said to be under the sun god 's benign rule in the same way that Egyptians were . \\n\",\n",
       " ' Thoth , as the overseer of time , was said to allot fixed lifespans to both humans and gods . Other gods were also said to govern the length of human lives , including <unk> , who presided over birth , and Shai , the personification of fate . Thus the time and manner of death was the main meaning of the Egyptian concept of fate , although to some extent these deities governed other events in life as well . Several texts refer to gods influencing or inspiring human decisions , working through a person \\'s \" heart \" — the seat of emotion and intellect in Egyptian belief . Deities were also believed to give commands , instructing the king in the governance of his realm and regulating the management of their temples . Egyptian texts rarely mention direct commands given to private persons , and these commands never evolved into a set of divinely enforced moral codes . Morality in ancient Egypt was based on the concept of maat , which , when applied to human society , meant that everyone should live in an orderly way that did not interfere with the well @-@ being of other people . Because deities were the upholders of maat , morality was connected with them . For example , the gods judged humans \\' moral righteousness after death , and by the New Kingdom , a verdict of innocence in this judgment was believed to be necessary for admittance into the afterlife . But in general , morality was based on practical ways to uphold maat in daily life , rather than on strict rules that the gods laid out . \\n',\n",
       " \" Humans had free will to ignore divine guidance and the behavior required by maat , but by doing so they could bring divine punishment upon themselves . A deity carried out this punishment using its ba , the force that manifested the god 's power in the human world . Natural disasters and human ailments were seen as the work of angry divine bas . Conversely , the gods could cure righteous people of illness or even extend their lifespans . Both these types of intervention were eventually represented by deities : Shed , who emerged in the New Kingdom to represent divine rescue from harm , and <unk> , an apotropaic god from the late eras of Egyptian history who was believed to avenge wrongdoing . \\n\",\n",
       " \" Egyptian texts take different views on whether the gods are responsible when humans suffer unjustly . Misfortune was often seen as a product of <unk> , the cosmic disorder that was the opposite of maat , and therefore the gods were not guilty of causing evil events . Some deities who were closely connected with <unk> , such as Set , could be blamed for disorder within the world without placing guilt on the other gods . But some writings do accuse the deities of causing human misery , while others give theodicies in the gods ' defense . Beginning in the Middle Kingdom , several texts connected the issue of evil in the world with a myth in which the creator god fights a human rebellion against his rule and then withdraws from the earth . Because of this human misbehavior , the creator is distant from his creation , allowing suffering to exist . New Kingdom writings do not question the just nature of the gods as strongly as those of the Middle Kingdom . They emphasize humans ' direct , personal relationships with deities and the gods ' power to intervene in human events . People in this era put faith in specific gods who they hoped would help and protect them through their lives . As a result , upholding the ideals of maat grew less important than gaining the gods ' favor as a way to guarantee a good life . Even the pharaohs were regarded as dependent on divine aid , and after the New Kingdom came to an end , government was increasingly influenced by oracles communicating the gods ' will . \\n\",\n",
       " '',\n",
       " ' = = = Worship = = = \\n',\n",
       " '',\n",
       " \" Official religious practices , which maintained maat for the benefit of all Egypt , were related to , but distinct from , the religious practices of ordinary people , who sought the gods ' help for their personal problems . \\n\",\n",
       " \" Official religion involved a variety of rituals , based in temples . Some rites were performed every day , whereas others were festivals , taking place at longer intervals and often limited to a particular temple or deity . The gods received their offerings in daily ceremonies , in which their statues were clothed , anointed , and presented with food as hymns were recited in their honor . These offerings , in addition to maintaining maat for the gods , celebrated deities ' life @-@ giving generosity and encouraged them to remain benevolent rather than vengeful . \\n\",\n",
       " \" Festivals often involved a ceremonial procession in which a cult image was carried out of the temple in a barque @-@ shaped shrine . These processions served various purposes . In Roman times , when local deities of all kinds were believed to have power over the Nile inundation , processions in many communities carried temple images to the riverbanks so the gods could invoke a large and fruitful flood . Processions also traveled between temples , as when the image of Hathor from Dendera Temple visited her consort Horus at the Temple of Edfu . Rituals for a god were often based in that deity 's mythology . Such rituals were meant to be repetitions of the events of the mythic past , renewing the beneficial effects of the original events . In the Khoiak festival in honor of Osiris , his death and resurrection were ritually reenacted at a time when crops were beginning to sprout . The returning greenery symbolized the renewal of the god 's own life . \\n\",\n",
       " \" Personal interaction with the gods took many forms . People who wanted information or advice consulted oracles , run by temples , that were supposed to convey gods ' answers to questions . Amulets and other images of protective deities were used to ward off the demons that might threaten human well @-@ being or to impart the god 's positive characteristics to the wearer . Private rituals invoked the gods ' power to accomplish personal goals , from healing sickness to cursing enemies . These practices used heka , the same force of magic that the gods used , which the creator was said to have given to humans so they could fend off misfortune . The performer of a private rite often took on the role of a god in a myth , or even threatened a deity , to involve the gods in accomplishing the goal . Such rituals coexisted with private offerings and prayers , and all three were accepted means of obtaining divine help . \\n\",\n",
       " ' Prayer and private offerings are generally called \" personal piety \" : acts that reflect a close relationship between an individual and a god . Evidence of personal piety is scant before the New Kingdom . Votive offerings and personal names , many of which are <unk> , suggest that commoners felt some connection between themselves and their gods . But firm evidence of devotion to deities became visible only in the New Kingdom , reaching a peak late in that era . Scholars disagree about the meaning of this change — whether direct interaction with the gods was a new development or an outgrowth of older traditions . Egyptians now expressed their devotion through a new variety of activities in and around temples . They recorded their prayers and their thanks for divine help on stelae . They gave offerings of figurines that represented the gods they were praying to , or that symbolized the result they desired ; thus a relief image of Hathor and a statuette of a woman could both represent a prayer for fertility . Occasionally , a person took a particular god as a patron , dedicating his or her property or labor to the god \\'s cult . These practices continued into the latest periods of Egyptian history . These later eras saw more religious innovations , including the practice of giving animal mummies as offerings to deities depicted in animal form , such as the cat mummies given to the feline goddess Bastet . Some of the major deities from myth and official religion were rarely invoked in popular worship , but many of the great state gods were important in popular tradition . \\n',\n",
       " \" The worship of some Egyptian gods spread to neighboring lands , especially to Canaan and Nubia during the New Kingdom , when those regions were under pharaonic control . In Canaan , the exported deities , including Hathor , Amun , and Set , were often syncretized with native gods , who in turn spread to Egypt . The Egyptian deities may not have had permanent temples in Canaan , and their importance there waned after Egypt lost control of the region . In contrast , many temples to the major Egyptian gods and deified pharaohs were built in Nubia . After the end of Egyptian rule there , the imported gods , particularly Amun and Isis , were syncretized with local deities and remained part of the religion of Nubia 's independent Kingdom of Kush . These gods were incorporated into the Nubian ideology of kingship much as they were in Egypt , so that Amun was considered the divine father of the king and Isis and other goddesses were linked with the Nubian queen , the <unk> . Some deities reached farther . <unk> became a goddess in Minoan Crete , and Amun 's oracle at Siwa Oasis was known to and consulted by people across the Mediterranean region . \\n\",\n",
       " \" Under the Greek Ptolemaic Dynasty and then Roman rule , Greeks and Romans introduced their own deities to Egypt . These newcomers equated the Egyptian gods with their own , as part of the Greco @-@ Roman tradition of interpretatio graeca . But the worship of the native gods was not swallowed up by that of foreign ones . Instead , Greek and Roman gods were adopted as manifestations of Egyptian ones . Egyptian cults sometimes incorporated Greek language , philosophy , iconography , and even temple architecture . Meanwhile , the cults of several Egyptian deities — particularly Isis , Osiris , Anubis , the form of Horus named Harpocrates , and the fused Greco @-@ Egyptian god Serapis — were adopted into Roman religion and spread across the Roman Empire . Roman emperors , like Ptolemaic kings before them , invoked Isis and Serapis to endorse their authority , inside and outside Egypt . In the empire 's complex mix of religious traditions , Thoth was transmuted into the legendary esoteric teacher Hermes Trismegistus , and Isis , who was venerated from Britain to Mesopotamia , became the focus of a Greek @-@ style mystery cult . Isis and Hermes Trismegistus were both prominent in the Western esoteric tradition that grew from the Roman religious world . \\n\",\n",
       " \" Temples and cults in Egypt itself declined as the Roman economy deteriorated in the third century AD , and beginning in the fourth century , Christians suppressed the veneration of Egyptian deities . The last formal cults , at Philae , died out in the fifth or sixth century . Most beliefs surrounding the gods themselves disappeared within a few hundred years , remaining in magical texts into the seventh and eighth centuries . But many of the practices involved in their worship , such as processions and oracles , were adapted to fit Christian ideology and persisted as part of the Coptic Church . Given the great changes and diverse influences in Egyptian culture since that time , scholars disagree about whether any modern Coptic practices are descended from those of pharaonic religion . But many festivals and other traditions of modern Egyptians , both Christian and Muslim , resemble the worship of their ancestors ' gods . \\n\",\n",
       " '',\n",
       " '',\n",
       " ' = South of Heaven = \\n',\n",
       " '',\n",
       " \" South of Heaven is the fourth studio album by American thrash metal band Slayer . Released on July 5 , 1988 , the album was the band 's second collaboration with record producer Rick Rubin , whose production skills on Slayer 's previous album Reign in Blood had helped the band 's sound evolve . \\n\",\n",
       " \" South of Heaven was Slayer 's second album to enter the Billboard 200 , and its last to be released by Def Jam Recordings , although the album became an American Recordings album after Rick Rubin ended his partnership with Russell Simmons . It was one of only two Def Jam titles to be distributed by Geffen Records through Warner Bros. Records because of original distributor Columbia Records ' refusal to release work by the band . The release peaked at number 57 and in 1992 was awarded a gold certification by the Recording Industry Association of America . \\n\",\n",
       " ' In order to offset the pace of the group \\'s previous album , Slayer deliberately slowed down the album \\'s tempo . In contrast to their previous albums , the band utilized undistorted guitars and toned @-@ down vocals . While some critics praised this musical change , others — more accustomed to the style of earlier releases — were disappointed . The songs \" Mandatory Suicide \" and the title track , however , have become permanent features of the band \\'s live setlist . \\n',\n",
       " '',\n",
       " ' = = Background = = \\n',\n",
       " '',\n",
       " ' South of Heaven was recorded in Los Angeles , California with Reign in Blood producer Rick Rubin . PopMatters reviewer Adrien Begrand observed that Rubin \\'s production \" shoves [ Dave ] Lombardo \\'s drumming right up front in the mix . \" Guitarist Jeff Hanneman has since said that South of Heaven was the only album the band members discussed before writing the music . Aware that they \" couldn \\'t top Reign in Blood \" , and that whatever they recorded would be \" compared to that album \" , he believed they \" had to slow down \" , something Slayer had never done on albums before , or since . Guitarist Kerry King cited the need to \" keep people guessing \" as another reason for the musical shift . \" In order to contrast the aggressive assault put forth on Reign in Blood , Slayer consciously slowed down the tempo of the album as a whole \" , according to Slayer \\'s official biography . \" They also added elements like undistorted guitars and toned @-@ down vocal styles not heard on previous albums . \" \\n',\n",
       " ' King has since been critical of his performance , which he describes as his \" most lackluster . \" King attributes this to the fact he had recently married , and moved to Phoenix , Arizona . Describing himself as \" probably the odd man out at that point \" , he admitted he \" didn ’ t participate as much because of that . \" Hanneman said : \" We go through dry spells sometimes , but the good thing about having two guitar players that can write music is that you are never gonna go without . I guess at that time , Kerry was hitting a dry spell . \" King has also been critical of the album in general , describing it as one of his least favorite Slayer albums . He feels vocalist Tom Araya moved too far away from his regular vocal style , and \" added too much singing . \" Drummer Dave Lombardo has since observed : \" There was fire on all the records , but it started dimming when South of Heaven came into the picture . And that \\'s me personally . Again , I was probably wanting something else . \" \\n',\n",
       " ' Judas Priest \\'s \" Dissident Aggressor \" is the only cover version to appear on a Slayer studio album . The song was chosen due to its war @-@ themed lyrics . Hanneman described the track as \" more just like one of those odd songs that a lot of people didn \\'t know , but it was a favorite of Kerry and I , so we just picked that one . \" Meanwhile , \" <unk> the Soul \" has been heavily criticized by King who said that he hates the track : \" That \\'s one of the black marks in our history , in my book . I just fucking think it \\'s horrible . [ Laughs ] I hate the opening riff . It \\'s what we call a \\' happy riff . \\' It \\'s just like \\' la @-@ <unk> @-@ la @-@ la @-@ la . \\' I can \\'t see myself playing it , but after that , where it gets heavier , I like that section . If we ever did a medley , I \\'d put part of that in there . \" The Slayer boxset Soundtrack to the Apocalypse featured , along with four songs of the album , an early version of the title track , recorded at Hanneman \\'s home . \\n',\n",
       " '',\n",
       " ' = = Photography and illustration = = \\n',\n",
       " '',\n",
       " ' Artist Larry Carroll and Illustrator Howard Schwartzberg designed the cover artwork for South of Heaven , having designed the artwork for Slayer \\'s previous album Reign in Blood . Photographer Glen E. Friedman took the promotional shot which surfaced as the back cover of South of Heaven around the time of 1986 \\'s Reign in Blood . Lombardo felt it made Slayer seem as though they \" had matured a little bit \" , while Friedman himself deemed it \" a really cool back cover \" and \" one of the most classic shots of them [ Slayer ] ever . \" \\n',\n",
       " '',\n",
       " ' = = Critical reception = = \\n',\n",
       " '',\n",
       " ' South of Heaven was released on July 5 , 1988 , and was the final Slayer album distributed via Def Jam Records . When label co @-@ founders Russell Simmons and Rubin parted ways , Slayer signed to Rubin \\'s newly founded Def American Recordings label . The album peaked at number 57 on the Billboard 200 album chart , and on November 20 , 1992 , became Slayer \\'s second album to be certified gold in the United States . South of Heaven was awarded silver certification in the United Kingdom on January 1 , 1993 , Slayer \\'s first record to do so in that country . Slayer \\'s official biography states that \" some critics praised the album as demonstrating Slayer \\'s desire to grow musically and avoid repeating themselves . \" Alex Henderson of AllMusic described the record as \" disturbing and powerful , \" while Joe Matera of Ultimate Guitar deemed the album a slight departure ; he wrote that while the pace was slowed down , it \" didn \\'t sacrifice any of the heaviness inherent in Slayer \\'s music . \" \\n',\n",
       " ' Reviewing the 2003 Slayer box set Soundtrack to the Apocalypse , Adrien Begrand of PopMatters described the album as \" their most underrated , and on this set , its five selections show how highly the band thinks of the record . \" KNAC.com \\'s Peter Atkinson was also positive , saying the album has a \" grandiosity and imposing presence \" which makes the record \" so magnificent . \" Grave \\'s Ola Lindgren and Bolt Thrower \\'s Karl Willetts both rate South of Heaven as amongst the top five albums of all time , while Max <unk> of Brazilian death metal group <unk> remembers hearing the song \" Silent Scream \" for the first time : \" It just blew me away . It was like fast double @-@ bass , fast kicks during the whole song . That was very inspiring for me . \" When discussing Slayer in an October 2007 interview , Evile frontman Matt Drake stated that while Reign in Blood \" was just speed \" , South of Heaven proved that the group could write \" slow material as well . \" Metal Forces reviewer gives \" the band credit for at least making an effort to try something new and not being afraid to experiment at such a crucial stage of their career \" , creating \" one of the more original sounding thrash / speed metal albums he heard in a long while \" . He remarks , however , that \" if you ’ re expecting to hear Reign in Blood Part Two , you ’ ll be in for a major disappointment \" . \\n',\n",
       " ' Kim Neely of Rolling Stone dismissed the album as \" genuinely offensive satanic drivel . \" Slayer \\'s official biography states : \" The new sounds disappointed some of the band \\'s fans who were more accustomed to the style of earlier releases . \" Michael Roberts of Westworld Online said this was due to some of the numbers moving \" at the <unk> speed of Black Sabbath . \" Araya commented that the \" album was a late bloomer — it wasn \\'t really received well , but it kind of grew on everybody later . \" \\n',\n",
       " '',\n",
       " ' = = Cover versions = = \\n',\n",
       " '',\n",
       " ' The title track and the song \" Mandatory Suicide \" have received various cover interpretations , particularly on Slayer tribute albums . Toni Ferguson recorded string quartet adaptations of both tracks on the album The String Quartet Tribute to Slayer : The Evil You Dread , with the former cover being described as having \" menacing chord shifts \" by AllMusic \\'s Johnny Loftus . \\n',\n",
       " ' 1995 Slayer tribute album Slatanic Slaughter featured three tracks which originally appeared on South of Heaven , with the title track , \" Mandatory Suicide \" and \" Spill the Blood \" interpreted by <unk> , Crown of Thorns and Grope respectively . Its 1998 follow up Slatanic Slaughter , Vol . 2 only featured two tracks originally from the album ; namely \" Silent Scream \" arranged by Vader and \" Read Between the Lies \" interpreted by Anathema . 1999 \\'s Straight to Hell : A Tribute to Slayer collected four Slayer renditions which originated on the album , with versions of South of Heaven performed by Abaddon ( Venom ) and Electric Hellfire Club , \" Mandatory Suicide \" cut by Chapter 7 and \" Behind the Crooked Cross \" adapted by <unk> . 2006 Argentine tribute album Al Sur Del Abismo ( Tributo Argentino A Slayer ) saw <unk> and Climatic Terra also respectively cover \" South of Heaven \" and \" Mandatory Suicide \" . Hatebreed covered the song \" Ghosts of War \" for their 2009 cover album For the Lions . They released a music video for it also . Korn has covered the title track at least twice live , once with Kid Rock on vocals and another using the intro to follow into one of their songs live . \\n',\n",
       " ' The title track itself has also been covered by Integrity 2000 , Modest Mouse and <unk> , Pro @-@ Pain , and Universe Eye . Polish death metal band Decapitated covered the song \" Mandatory Suicide \" on their first full @-@ length album Winds of Creation . In 2003 , \" Silent Scream \" was covered by Children of Bodom for their album Hate Crew <unk> in his UK version . Hardcore Punk band , The <unk> opened their set with the beginning of \" South of Heaven \" at <unk> 7 on May 4 , 2013 \\n',\n",
       " '',\n",
       " ' = = Live performances = = \\n',\n",
       " '',\n",
       " ' Two songs taken from the album ( \" Mandatory Suicide \" and \" South of Heaven \" ) have become near constant fixtures in the band \\'s live setlist , notching up appearances on the following : the live DVDs Live Intrusion , War at the Warfield , Still Reigning , Soundtrack to the Apocalypse \\'s deluxe edition \\'s bonus live disc , and the live double album Decade of Aggression . Lombardo guested with Finnish cellist group Apocalyptica on a live medley of the two tracks at 1998 \\'s Headbanger \\'s Heaven festival in the Netherlands . Adrien Begrand of PopMatters described \" South of Heaven \" as \" an unorthodox set opener in theory \" , noting \" the song went over like a megaton bomb detonating the place : dozens of inverted crosses projected behind the high drum riser , the sinewy opening notes kicked in , followed by an overture of bass , cymbal crashes , and tom fills , leading up to the slowly building crescendo \" in a concert review . Lombardo remembers listening to a live rendition of \" South of Heaven \" and thinking \" ‘ Man ! There \\'s just so much groove in that song . ’ To my kids I was saying , ‘ Listen to that ! Listen to how groovy that is ! ’ And it \\'s heavy . \" A rare live version of the track featured on the <unk> Rarities 2004 promotional CD , given away to attendees at the Spring 2004 Jägermeister Music Tour . A live rendition of \" South of Heaven \" was also included on a bonus DVD which came with the group \\'s 2007 re @-@ release of ninth studio album Christ Illusion , shot in Vancouver , British Columbia during 2006 \\'s Unholy Alliance tour . \\n',\n",
       " ' \" Behind the Crooked Cross \" is rarely played live as Hanneman hates the track , though King has always wanted to play it \" because it \\'s got a cool intro \" despite it not being his favorite song . King said \" that \\'s fine \" when speaking of the situation , noting \" there are songs that he wants to play that I always shoot down . \" \" Ghosts of War \" isn \\'t King \\'s favorite song either , which he attests \" everybody always wants to hear \" performed live . He confessed ; \" I like the ending , you know , I like the big heavy part and I always say , ‘ Let \\'s put the heavy ending at the end of \" Chemical Warfare \" and just do the last half . ’ But I could never make that fly . \" \\n',\n",
       " ' Slayer has toyed with the idea of creating a live set mixed with selections from the album and 1990 \\'s Seasons in the Abyss , though Hanneman said it \\'s something which hasn \\'t been \" seriously considered . \" Metal Maniacs asked Slayer in a 2006 interview whether they would consider playing South of Heaven in the footsteps of the Still Reigning tour , to which Araya replied , \" It \\'s becoming a trendy thing now . I don \\'t know . We have some really cool albums , but I don \\'t think we \\'ll ever do that again . \" King was equally unsure , commenting , \" Probably not . And I just don \\'t like enough songs off South of Heaven . \" \\n',\n",
       " '',\n",
       " ' = = Track listing = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = Personnel = = \\n',\n",
       " '',\n",
       " '',\n",
       " ' = = = Slayer = = = \\n',\n",
       " '',\n",
       " ' Tom Araya – bass , lead vocals \\n',\n",
       " ' Jeff Hanneman – lead and rhythm guitar \\n',\n",
       " ' Kerry King – lead and rhythm guitar , backing vocals \\n',\n",
       " ' Dave Lombardo – drums \\n',\n",
       " '',\n",
       " ' = = Charts and certifications = = \\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' = General aviation in the United Kingdom = \\n',\n",
       " '',\n",
       " ' General aviation in the United Kingdom has been defined as a civil aircraft operation other than a commercial air transport flight operating to a schedule or military aviation . Although the International Civil Aviation Organization ( ICAO ) excludes any form of remunerated aviation from its definition , some commercial operations are often included within the scope of general aviation ( GA ) in the UK . The sector operates business jets , rotorcraft , piston and jet @-@ engined fixed @-@ wing aircraft , gliders of all descriptions , and lighter than air craft . Public transport operations include business ( or corporate ) aviation and air taxi services , and account for nearly half of the economic contribution made by the sector . Other commercial GA activities are aerial work , such as surveying and air ambulances , and flight training , which plays an important role in the supply of pilots to the commercial air transport ( CAT ) industry . Private flying is conducted for personal transport and recreation . It includes a strong vintage aircraft movement , and encompasses a range of air sports , such as racing , aerobatics , and parachuting , at which British teams and individuals have succeeded in international competition . \\n',\n",
       " ' Of the 21 @,@ 000 civil aircraft registered in the UK , 96 per cent are engaged in GA operations , and annually the GA fleet accounts for between 1 @.@ 25 and 1 @.@ 35 million hours flown . The single most common class of aircraft is the fixed @-@ wing light aircraft associated with traditional GA , but the main area of growth over the last 20 years has been in the use of more affordable aircraft , such as microlights , amateur built aeroplanes , and smaller helicopters . There are 28 @,@ 000 Private Pilot Licence holders , and 10 @,@ 000 certified glider pilots . Some of the 19 @,@ 000 pilots who hold professional licences are also engaged in GA activities . Although GA operates from more than 1 @,@ 800 aerodromes and landing sites , ranging in size from large regional airports to farm strips , over 80 per cent of GA activity is conducted at 134 of the larger aerodromes . The GA industry , which is around 7 per cent the size of its CAT cousin , employs 12 @,@ 000 people , and contributes £ 1 @.@ 4 billion to the UK economy . \\n',\n",
       " ' GA is regulated by the Civil Aviation Authority ( CAA ) , although regulatory powers are being increasingly transferred to the European Aviation Safety Agency ( EASA ) . The main focus is on standards of airworthiness and pilot licensing , and the objective is to promote high standards of safety . At the lighter end of the GA spectrum some regulatory authority is devolved to representative bodies , and gliding is in transition from a self @-@ regulatory model to more formal governance by EASA . Airspace regulation necessary to protect an increasing number of CAT operations has reduced the area in which GA flights can be freely conducted . The growth in CAT is also making access to larger airports more difficult for the GA sector , and smaller aerodromes are vulnerable to closure and re @-@ development for more profitable uses . The UK planning system has no remit to consider the national significance of GA public transport operations , and generally does not favour the development of smaller aerodromes catering to the GA market . The planning process has become a mechanism for addressing local aerodrome @-@ related environmental issues which , particularly regarding noise , are the main subjects of public criticism levelled at GA . \\n',\n",
       " '',\n",
       " ' = = Definitions = = \\n',\n",
       " '',\n",
       " ' The International Civil Aviation Organization ( ICAO ) defines general aviation ( GA ) as \" an aircraft operation other than a commercial air transport operation or an aerial work operation . \" It defines commercial air transport ( CAT ) as \" an aircraft operation involving the transport of passengers , cargo or mail for remuneration or hire \" , and aerial work as \" an aircraft operation in which an aircraft is used for specialized services such as agriculture , construction , photography , surveying , observation and patrol , search and rescue , aerial advertisement , etc . \" \\n',\n",
       " ' Organisations in the United Kingdom ( UK ) describe GA in less restrictive terms that include elements of commercial aviation . The British Business and General Aviation Association interprets it to be \" all aeroplane and helicopter flying except that performed by the major airlines and the Armed Services \" . The General Aviation Awareness Council applies the description \" all Civil Aviation operations other than scheduled air services and non @-@ scheduled air transport operations for remuneration or hire \" . For the purposes of a strategic review of GA in the UK , the Civil Aviation Authority ( CAA ) defined the scope of GA as \" a civil aircraft operation other than a commercial air transport flight operating to a schedule \" , and considered it necessary to depart from the ICAO definition and include aerial work and minor CAT operations . \\n',\n",
       " '',\n",
       " ' = = History = = \\n',\n",
       " '',\n",
       " ' The first aerodrome in the UK was established by the Aero Club at Muswell Manor on the Isle of Sheppey , and in May 1909 it was the venue of the first flight conducted in the country by a British pilot , John Moore @-@ Brabazon . In 1910 the Aero Club was granted the Royal prefix , took responsibility for controlling all private flying in the UK , and started issuing the first British pilot licences . The introduction of the de Havilland DH.60 Moth in 1925 revolutionised light aviation , and the Royal Aero Club , recognising the \" vital necessity of promoting civil flying \" , formed the Light Aeroplane Club scheme . Between 1925 and 1939 around 60 flying clubs were started , and more than 5 @,@ 000 pilots were trained . \\n',\n",
       " ' During World War II civil aerodromes were taken over for military use , existing military airfields were expanded , and new ones were built . This resulted in a significant inventory of facilities becoming available after the war . Pre @-@ war civil aerodromes , for example Sywell , were returned to civilian use . Surplus military airfields were closed , and in some cases , for example <unk> , subsequently re @-@ opened as civil aerodromes . The Ministry of Civil Aviation was created to regulate all civil aviation in the UK , and this task remained the responsibility of government departments until the establishment of the independent CAA in 1972 . \\n',\n",
       " ' With an expanded infrastructure in place , GA became established after the war when manufacturers such as Cessna and Piper introduced light aircraft designed for the private market . The Cessna 172 , developed from the late 1940s Cessna 170 , was introduced in 1956 , and became the world \\'s best selling single @-@ engine aeroplane . Single piston @-@ engine aircraft are still the most common class of aircraft in the UK GA fleet . The development of the <unk> wing in the 1950s fostered the development of hang @-@ gliding during the 1960s and 1970s . The 1960s also saw experiments with motorised hang gliders , but it was not until the 1970s that this blend of technologies started to mature , resulting in the birth of the microlight movement . Another milestone in the development of GA was the 1964 introduction of the Learjet 23 . Although it was not the first business jet , it popularised corporate aviation , and established the personal jet as a \" whole new class of aircraft \" . \\n',\n",
       " '',\n",
       " ' = = Activities = = \\n',\n",
       " '',\n",
       " ' The GA sector operates a range of aircraft , including balloons and airships , gliders , hang gliders , paragliders , microlights , <unk> , helicopters , amateur built and mass @-@ produced light aircraft , ex @-@ military aircraft , and business jets . Flights can be broadly categorised as public transport , aerial work , and private flying , the first two of which are commercial activities . \\n',\n",
       " '',\n",
       " ' = = = Commercial operations = = = \\n',\n",
       " '',\n",
       " \" Commercial operations are remunerated activities which fall within the ICAO definition of CAT . Some are , however , closely aligned to , and considered part of , the GA sector . Public transport operations are non @-@ scheduled , on @-@ demand services flying between points specified by the customer , providing a more flexible service than airline travel . Air taxi operations offer charter services for third parties , and business or corporate aviation uses company @-@ owned aircraft to transport employees and clients . Aircraft used in these operations include business jets , helicopters , and twin piston @-@ engine aeroplanes carrying between six and ten people . An example of this type of operation is the transport by helicopter of spectators to the British Formula One grand prix at Silverstone . This involves so many flights that , according to Cranfield Aviation Services , on race day the heliport is temporarily the world 's busiest airport . Aerial work is a small but important component of the commercial GA sector , characterised in its simplest form as remunerated non @-@ transport activities , such as surveying , crop spraying , and emergency services work ( air ambulance and police ) . \\n\",\n",
       " '',\n",
       " ' = = = Flying schools = = = \\n',\n",
       " '',\n",
       " ' Flying schools are commercial businesses engaged in the training of pilots , both for recreational purposes and for those intending to fly professionally . They make widespread use of fixed @-@ wing light aircraft associated with traditional GA , not only for flying lessons but also as club aircraft rented out to qualified pilots for recreational flights . School @-@ owned aircraft account for a significant amount of GA activity , both in terms of hours flown and aircraft movements . The pilot training element is regarded by the GA community as a key benefit that is critical to the supply of pilots for the airline industry . It is claimed by the General Aviation Awareness Council that 60 – 70 per cent of professional pilots have self @-@ financed their flight training at GA schools , and one UK airline operator has stated that the industry must rely on 70 – 80 per cent of new pilots coming from the GA sector . The CAA estimates that between 1996 and 2006 the number of new professional pilots following the unsponsored training route rose from 48 per cent to 59 per cent . The counter argument to this claim is that pilots can be trained outside of the UK , and that the airline industry is not therefore dependent on a healthy GA sector in the UK for its supply of pilots . The CAA concludes that a severe reduction in GA would give \" some merit to the argument that pilot recruitment would be threatened \" , but that the data on flying hours \" does not support such a gloomy outlook . \" Of course , reliance on other countries for pilot training means that the UK foregoes the economic benefit of the training activity . \\n',\n",
       " '',\n",
       " ' = = = Private flying = = = \\n',\n",
       " '',\n",
       " ' Private flying can be for both recreational purposes and personal transport , using aircraft that are owned individually , collectively as part of a syndicate , or rented from a flying club . A survey of pilots conducted between 2001 and 2002 indicated that the most common purposes of recreational flights were local flights near the base aerodrome , visits to other aerodromes , and day trips away . Half of all flights landed at the same aerodrome they departed from , and only 9 per cent involved an overnight stay away from home . \\n',\n",
       " \" Private flying is most associated with the traditional form of factory @-@ produced two and four @-@ seater , single piston @-@ engine training and touring aircraft . Examples of these are the Cessna 152 , Cessna 172 , and Piper <unk> Cherokee , all with their origins in the 1950s , and the more modern designs of Cirrus . The average cost per hour to fly such aircraft has been estimated to be £ 133 , compared to an estimated £ 77 per hour for gliders , and a reported £ 35 per hour for microlights . Recent trends have seen an increase in the use of microlights , and also in recreational helicopter flying following the introduction of smaller and cheaper machines such as the Robinson <unk> and R44 . Another growth area in private flying in recent years has been in the use of amateur built aircraft , such as the Van 's Aircraft RV @-@ 4 and the Europa . \\n\",\n",
       " ' There is a strong vintage aircraft movement in the UK , with two @-@ thirds of the 500 registered historic aircraft active . These cover the whole spectrum of civil and military aviation , examples being the de Havilland Dragon Rapide airliner of the 1930s , and the World War II ( WWII ) Spitfire fighter . There are many post @-@ WWII aircraft which could also be considered historic under a looser definition , including for example 60 ex @-@ military jets such as the Hawker Hunter . Historic aircraft are regular exhibits at air displays , which are claimed to be the second most popular spectator activity after football in the UK . \\n',\n",
       " '',\n",
       " ' = = = Sports = = = \\n',\n",
       " '',\n",
       " \" Competitive gliding in the UK takes place between May and September . Regionals are local competitions , organised and run by one of the bigger gliding clubs in the region , and represent the entry level to glider racing . Races are handicapped according to glider performance , and normally take place over nine days . Success in the regionals allows pilots to progress to the nationals , where there are five classes of competition . These are based on glider performance , the lowest being club class , and then progressing through standard ( maximum 15 metres ( 49 ft ) wingspan , and flaps not permitted ) , 15 metres ( 49 ft ) ( as standard , but flaps are permitted ) , 18 metres ( 59 ft ) ( maximum 18 metres ( 59 ft ) wingspan ) , and finally open @-@ class ( no restrictions ) . Success at national level can lead to a place in the national team and competition at international level . In 2007 the British gliding team was ranked number one , and British pilots took two women 's world championships and the open class European championship . \\n\",\n",
       " ' Handicapped air racing is open to any propeller @-@ driven aircraft capable of maintaining a minimum speed of 100 miles ( 160 km ) per hour in level flight . Races are a case of \" fly low , fly fast , turn left \" , consisting of 4 – 5 laps round a 20 – 25 mile ( 32 – 40 km ) circuit . Faster aircraft are handicapped by starting after slower aircraft , the intention being that the race concludes with all aircraft diving for the finish line together . There are up to 16 races per year , conducted at airfields in the UK , France and the Channel Islands , for prizes that include the Schneider Trophy and King \\'s Cup , and the season culminates with the British Air Racing and European Air Racing Championships . \\n',\n",
       " ' Aerobatic competitions take place for both powered aircraft and gliders , with up to 30 events each year in the UK and Ireland . Starting at the Beginner level , pilots can move up to Standard ( powered aircraft ) or Sports ( glider ) levels , and then on to Intermediate , Advanced , and finally Unlimited classes . Each step up requires a wider repertoire of aerobatic figures and progressively more performance from the aircraft . National championships are awarded annually at Standard / Sports , Intermediate , Advanced ( powered aircraft only ) , and Unlimited levels , and pilots who have reached Advanced and Unlimited levels are eligible for selection to represent the UK in international competition . \\n',\n",
       " \" Parachute competitions are held at club , regional , national and international levels , and include the disciplines of accuracy landings , freefall gymnastics , formation skydiving , canopy formation , freestyle and <unk> , and skysurfing . British teams consistently win medals in canopy formation world championships , and a British team took the 2006 world championship in women 's 4 @-@ way formation skydiving . \\n\",\n",
       " '',\n",
       " ' = = Aerodromes = = \\n',\n",
       " '',\n",
       " ' Aerodrome is a collective term for any location from which flying operations take place , although more specific terminology can be used to characterise its purpose . The CAA strategic review of GA applies the term airport to locations which predominantly support large scale commercial operations , and airfield to locations which predominantly support GA operations . The General Aviation Small Aerodrome Research Study ( GASAR ) analysed 687 aerodromes in England which come under the scope of GA , classifying 374 into six types . These range in size from regional airports to the smallest farm strip , although 84 per cent of GA flights operate from 134 of the larger aerodromes in the first four categories . \\n',\n",
       " '',\n",
       " ' = = = GASAR aerodrome classification = = = \\n',\n",
       " '',\n",
       " ' The factors used in determining how an individual aerodrome is categorised by the GASAR study are based broadly on size and facilities . The six types of aerodrome are described , in size order , as : regional airports ( e.g. East Midlands ) ; major GA airports ( e.g. Oxford ) ; developed GA airfields ( e.g. <unk> ) ; basic GA airfields ( e.g. <unk> ) ; developed airstrips ( e.g. <unk> ) ; and basic airstrips ( e.g. <unk> in Hampshire ) . The actual criteria used to categorise aerodromes were complex , using 28 different parameters , backed up with a peer review by experienced GA pilots . \\n',\n",
       " ' Airports generally have long , fully lit , hard @-@ surfaced runways , full air traffic control , and navigation and landing aids . They are usually located on urban fringes , support commercial and business operations , and often exclude certain types of light aircraft . At the more <unk> located airfields , the lighter end of aviation , such as microlight and gliding activities , becomes increasingly prevalent , and there are few or no commercial operations other than flying schools . At this level runways are generally shorter , and grass surfaces are increasingly common . Navigation aids are increasingly scarce , being more basic where they are available , and informal ground to air radio communication replaces air traffic control . The smallest airfields are too small to feature on general purpose Ordnance Survey ( OS ) maps , and lack basic facilities such as fuel and maintenance . The majority of airstrips are basically single short grass runways with no supporting facilities , although the presence of a hangar is not uncommon at the larger examples . They do not feature on OS maps , and are owned by private clubs or , more commonly , individuals . \\n',\n",
       " '',\n",
       " ' = = = Aerodrome licensing = = = \\n',\n",
       " '',\n",
       " ' Most aerodromes used for public transport operations are required to be licensed by the CAA . To be granted a licence an aerodrome operator must satisfy the CAA that : the physical conditions at the aerodrome , and its environs , are acceptable ; the scale of equipment , and facilities provided , are adequate for the flying activities which are expected to take place ; an effective safety management system is in place ; and that staff are competent and , where necessary , suitably qualified . Aerodromes classified as developed GA airfields or larger by the GASAR study are , with few exceptions , licensed . Only two basic GA airfields , Silverstone and Duxford , are licensed , and all airstrips are unlicensed . The Light Aviation Airports Study Group , a joint CAA @-@ industry initiative , was established in 2005 to review the regulation of light aviation aerodromes . A particular focus of this group was a review of the restrictions placed on unlicensed aerodromes . The group concluded that the requirement for public transport operations to be conducted only from licensed aerodromes should be further reviewed in the context of corresponding international and European requirements . It also recommended that restrictions on flight training at unlicensed aerodromes should be lifted , and this was permitted from April 2010 \\n',\n",
       " '',\n",
       " ' = = Scale of the sector = = \\n',\n",
       " '',\n",
       " ' There are an estimated 27 @,@ 000 civil aircraft registered in the UK , 96 per cent of which are engaged in GA activities . In 2005 the GA fleet comprised 9 @,@ 000 fixed @-@ wing aircraft , 4 @,@ 100 microlights , 1 @,@ 300 helicopters , 1 @,@ 800 airships / balloons , 2 @,@ 500 gliders and some 7 @,@ 000 hang gliders . Estimates put the number of foreign @-@ registered GA aircraft based in the UK at 900 . \\n',\n",
       " ' The number of pilots licensed by the CAA to fly powered aircraft in 2005 was 47 @,@ 000 , of whom 28 @,@ 000 held a Private Pilot Licence . The remainder held professional pilot licences , either a Commercial Pilot Licence or an Airline Transport Pilot Licence , although not all of these would be engaged in GA activities . In addition , there are 10 @,@ 000 active glider pilots , and estimates put the membership of aviation @-@ related sport and recreational associations at 36 @,@ 000 . \\n',\n",
       " \" The number of aerodromes that support GA in the UK is difficult to establish with certainty . <unk> 2008 United Kingdom Flight Guide lists 355 , and the <unk> Flight Equipment UK VFR Flight Guide 2008 lists nearly 500 . <unk> Farm ' Strips ' and Private Airfields Flight Guide lists more than 300 landing sites . The GASAR study estimates 1 @,@ 100 formal flying sites in England alone , a figure which includes 400 sites known to planning authorities but not included in flight guides . It estimates another 759 informal sites known only to land owners , customs , and members of the enthusiast group Air @-@ Britain . \\n\",\n",
       " ' The sector was estimated to employ nearly 12 @,@ 000 people and directly contribute £ 1 @.@ 4 billion to the UK economy in 2005 , making it roughly seven per cent of the size of the CAT industry . Nearly half of the economic contribution was generated by business aviation . \\n',\n",
       " '',\n",
       " ' = = Trends = = \\n',\n",
       " '',\n",
       " ' Most sectors of GA for which data are available have experienced growth in aircraft numbers and hours flown over the last two decades . The lighter end of the GA spectrum : microlights , amateur built , and airships and balloons , have in particular shown strong growth , although the last of these activities was severely curtailed during the foot @-@ and @-@ mouth outbreak in 2001 , when access to farmland was denied . After strong growth in the late 1980s , traditional flying has shown a slight decline recently , reflecting a move amongst recreational flyers towards microlight aircraft , and increased numbers of foreign @-@ registered aircraft . Recreational helicopter usage has grown primarily due to the introduction of smaller and cheaper aircraft . Glider activity has remained relatively static , although there has been a gradual increase in the number of self @-@ launching motor gliders . \\n',\n",
       " ' Business aviation has shown strong growth , although the numbers of aircraft on the UK register have declined . This reflects a shift away from turboprop aircraft towards foreign @-@ registered business jets based in the UK , which are estimated to be growing in numbers . However , twin piston @-@ engined aircraft numbers have declined significantly , reflecting pressures on the light air @-@ taxi segment from increasingly flexible and cheaper scheduled services , and a more sophisticated corporate charter business . The amount of flight training conducted by UK schools has declined , largely at the hands of competition from foreign schools , which benefit from lower costs and better weather . \\n',\n",
       " ' Since 1990 the total number of hours flown annually by the GA sector has remained in the range 1 @.@ 25 – 1 @.@ 35 million , the dominant sector being traditional GA flying , which accounts for 0 @.@ 6 million per year . An overall increase in aircraft numbers combined with nil growth in hours flown has brought the annual average utilisation per aircraft down from 157 hours in 1984 to 103 hours in 2002 . The decline in asset utilisation has led to speculation that the economic health of the GA industry is weakening , though the lack of data on profitability makes this difficult to confirm . \\n',\n",
       " '',\n",
       " ' = = Regulation = = \\n',\n",
       " '',\n",
       " ' The objective of regulation is to \" promote high standards of safety in all aspects of aviation \" , and this is the main area of interaction between the CAA and the GA sector . Efforts focus on assuring appropriate standards of airworthiness , pilot qualification , the rules for the movement of aircraft , and equipment to be carried . The CAA was established as the primary regulatory body for all aviation in the UK in 1972 . In 1991 it started working within the Joint Aviation Authorities ( JAA ) framework to implement agreed common standards , known as the Joint Aviation Requirements ( JAR ) , throughout the European Union ( EU ) . In 2003 this was taken a step further when the European Aviation Safety Agency ( EASA ) was established as the central EU regulator , taking over responsibility for legislating airworthiness and environmental regulation from the national authorities . The CAA acts as an agency of EASA on these issues , retaining its original regulatory powers in areas not yet transferred to EASA . Proposed developments seek to establish EASA as the single authority throughout the EU , taking over from individual member states the power to regulate all aviation other than that specifically excluded from the scope of EASA . \\n',\n",
       " '',\n",
       " ' = = = Devolved and self @-@ regulation = = = \\n',\n",
       " '',\n",
       " ' Within this framework certain sectors of GA are governed on a devolved basis . In all cases the CAA / EASA retains responsibility for safety regulation , but representative bodies , particularly of sectors that are not included in the scope of EASA , are granted greater oversight of their activities . The majority of microlight aircraft are regulated by the British Microlight Aircraft Association ( BMAA ) , although a significant number are regulated by the Light Aircraft Association ( LAA ) , formerly known as the Popular Flying Association . The LAA is the primary regulator for amateur built aircraft , as well as vintage and classic aircraft . Parachuting is governed by the British Parachute Association , although the aircraft used in this activity are generally CAA @-@ regulated . Balloon and airship flying is overseen by the British Balloon and Airship Club . The UK @-@ specific National Private Pilot Licence ( NPPL ) is administered by the National Pilots Licensing Group Ltd . , supported by the LAA , the Aircraft Owners and Pilots Association UK , the British Gliding Association , and the British Microlight Aircraft Association . Separate from these devolved groups , gliding in the UK is self @-@ regulated . The British Gliding Association was until recently responsible for glider airworthiness , now formally regulated as a result of EASA legislation , and still retains control of pilot certification . Hang gliding and paragliding activities ( i.e. foot @-@ launched gliders ) are governed by the British Hang Gliding and <unk> Association . \\n',\n",
       " '',\n",
       " ' = = = Airworthiness = = = \\n',\n",
       " '',\n",
       " ' Under CAA and EASA rules , all aircraft are required to meet certain standards of airworthiness to fly safely and legally . Aircraft that meet these standards are issued with a Certificate of Airworthiness . However , British @-@ registered aircraft which are excluded from the scope of EASA , and which cannot satisfy the requirements for the issue of a Certificate of Airworthiness , may be issued with a Permit to Fly . This allows them to fly in UK airspace subject to certain limitations , for example being restricted to day @-@ time flights under visual flight rules only . A number of organisations ( e.g. the British Microlight Aircraft Association and the Light Aircraft Association ) have obtained a standing over @-@ flight permission for Permit to Fly aircraft within their area of interest with some European countries , notably France . Permits are typically issued to vintage and historic aircraft , amateur built aircraft , and microlights . \\n',\n",
       " '',\n",
       " ' = = = Pilot licensing = = = \\n',\n",
       " '',\n",
       " ' The pilot qualification most relevant to GA is the Private Pilot Licence ( PPL ) , which permits the holder to fly for recreational purposes without remuneration . In addition to the European @-@ wide Joint Aviation Regulations Flight Crew Licensing ( JAR @-@ <unk> ) standard , the CAA also issues UK @-@ specific national licences . In the absence of European standards for <unk> , balloon , and airship pilots , the CAA licenses these according to the original UK PPL standard . As a response to the perception that JAR pilot licensing standards are excessively bureaucratic and expensive for the purposes of recreational pilots , the National Private Pilot Licence ( NPPL ) was introduced in 2002 . The NPPL is easier to obtain than the JAR @-@ <unk> licence , has less stringent medical requirements , is more restrictive in the privileges it grants , and is valid only for flights in British @-@ registered aircraft flying in UK and French airspace . Although there are plans to bring glider pilot licensing within the regulatory framework of EASA , the gliding sector is currently self @-@ regulating in this respect . The British Gliding Association is responsible for defining the standards of initial training , and certifying , via a badge system , pilots who meet those standards . Pilots working in sectors of GA that are commercial operations , such as aerial work and business aviation , are required to hold a professional pilot licence which , at a minimum , is the Commercial Pilot Licence . \\n',\n",
       " '',\n",
       " ' = = Safety = = \\n',\n",
       " '',\n",
       " ' Between 1995 and 2004 there were 2 @,@ 630 accidents involving GA aircraft , of which 139 were fatal , resulting in the loss of 317 lives . The majority of accidents involved small fixed @-@ wing aircraft engaged in private flights , and analysis attributes the most common causes of these to : flight handling skills ; poor judgement or airmanship ; lack of training or experience ; and omission of , or inappropriate , action . \\n',\n",
       " ' There were 27 fatal accidents involving GA aircraft in 2007 , resulting in the loss of 48 lives . These compare with 16 accidents claiming a total of 19 lives the previous year , and although the 2007 statistics are higher than average , they are not exceptional . \\n',\n",
       " '',\n",
       " ' = = Issues = = \\n',\n",
       " '',\n",
       " ' The growth in Commercial Air Transport ( CAT ) has eroded the operational freedom of GA , both in the air and on the ground at larger airports . Difficulty with access to larger airports is compounded by a decline in the number of aerodromes generally , and existing sites are often threatened with closure and re @-@ development for more profitable uses . The UK planning system is designed to focus on local issues , and consideration of the national impact of GA operations is not within its remit . This makes aerodrome development difficult , often subjecting those that successfully negotiate the process to restrictions in use . \\n',\n",
       " '',\n",
       " ' = = = Airspace access = = = \\n',\n",
       " '',\n",
       " ' Airspace is shared by CAT , military and GA users . It is divided into controlled airspace , in which aircraft must always be under the control of an air traffic controller , and uncontrolled airspace , in which aircraft can operate autonomously . Although GA flights can under certain conditions enter controlled airspace , they operate mainly outside of it . \\n',\n",
       " ' Controlled airspace is essential for the provision of a known air traffic environment necessary for the safe operation of CAT . A CAA review found that \" mixing [ commercial ] operations with other users is considered undesirable , even untenable \" by commercial operators . However this position has resulted in extensive Class A controlled airspace with complex boundaries , including some running down to the ground , prohibiting VFR access to airspace , resulting in high numbers of GA flights operating close to the borders of controlled airspace who could not get formal receipt of an air traffic service . Coupled with pilot navigation errors , hundreds of airspace infringements have been recorded every year . \\n',\n",
       " ' Increases in the number of CAT operations , and in the number of airports they operate from , has resulted in a corresponding increase in Class A controlled airspace . Between 1997 and 2006 this area grew in size from 13 per cent of all airspace to 22 per cent nationally , and from 24 per cent to 43 per cent in airspace above England and Wales , leading to a perception within the GA community of being squeezed out . There are particular problems for GA around large airports , where Class A controlled airspace extends to ground level . The concentration of commercial operations and high demand for GA in the South East of England have also resulted in extensive areas of Class A controlled airspace there , which serve to channel uncontrolled GA operations through high @-@ collision @-@ risk hot spots . \\n',\n",
       " '',\n",
       " ' = = = Aerodrome access = = = \\n',\n",
       " '',\n",
       " ' Regional airports , such as Edinburgh Airport , have experienced strong growth in CAT operations in recent years . These operations are commercially and operationally incompatible with GA , and although there is no evidence of deliberate discrimination , the effect has been to discourage or exclude it . GA aircraft are being subject to significant increases in charges , including the imposition of handling fees in some cases . Some airports restrict or deny GA parking , and others limit or refuse certain GA activity . As a result , light GA aircraft are now rarely or never seen at large , busy international airports such as Heathrow , Stansted , Gatwick and Manchester . \\n',\n",
       " ' In addition to this de facto loss of facilities , the number of aerodromes in the UK has been in decline over the last 50 years , as a result of increasing urbanisation and the closure of airfields built during WWII . Alternative and more profitable uses for land can also lead to existing aerodromes being threatened with closure , for example North Weald , or actually being closed , as happened to Ipswich <unk> and Bristol Filton Airport . Referring to the importance of a \" functioning national network of GA airfields \" , especially where GA performs an air transport role , the CAA states that \" there could be cause for concern if a significant further loss of airfields were to continue , especially if crucial nodes on the transport network were to be lost . \" \\n',\n",
       " '',\n",
       " ' = = = Planning system = = = \\n',\n",
       " '',\n",
       " ' The planning system is critical to the viability and operation of GA aerodromes . With many cities lacking scheduled air transport services between them , and with GA access to commercial airports becoming increasingly difficult and expensive , a viable network of aerodromes supporting GA air transport operations is regarded as an important national issue . However , there is no unified national planning policy specific to GA aerodromes , and planning decisions relating to these are based on local issues that are not required to consider the national impact . Because aircraft are excluded from noise control legislation , the only recourse for people affected by aircraft noise is through the planning process , and this issue is the principal factor on which the majority of planning decisions relating to GA land use are made . GA is a specialist subject often unfamiliar to Local Planning Authorities , and most planning decisions relating to GA either refuse permission , or grant it with restrictive conditions . Little Gransden is just one example of a GA airfield required to comply with planning restrictions on the number of movements permitted , thereby inhibiting further development . Such restrictions , if poorly conceived , can make GA operations unviable or even unsafe . \\n',\n",
       " '',\n",
       " ' = = Criticism = = \\n',\n",
       " '',\n",
       " ' Public opinion towards aviation generally is worsening , based on increasing environmental concerns relating to emissions and noise , and private flying has been criticised by respondents to a government consultation on aircraft noise as a frivolous or selfish activity . In terms of environmental complaints and enquiries made to the CAA that relate specifically to GA , noise is \" by far \" the most common subject . Half of the 2 @,@ 000 noise complaints made annually to the CAA concern GA operations , most of which relate to aerobatics , helicopters using private sites , air balloon incidents , parachute dropping , and alleged low flying . \\n',\n",
       " ' Planning guidance on aircraft noise advises that \" in some circumstances the public perceive general aircraft noise levels as more disturbing than similar levels around major airports . \" This is a result of the tonal characteristics of light aircraft engines and the activities they are engaged in , including : repetitive circuit flying at low @-@ altitude near an aerodrome , during which aircraft are audible for long periods ; slow climbing aircraft engaged in parachute drop or glider tug activities concentrated around the drop zone or aerodrome , also audible for long periods ; erratic and repetitive engine noise from aircraft engaged in aerobatics ; and piston @-@ engines on full power in areas of low background noise , leading to the perception that such noise is more intrusive . In an attempt to alleviate these problems , the majority of aerodromes implement noise abatement procedures designed to route aircraft away from noise sensitive areas , and more than 50 are required by the government to provide consultative facilities in which local concerns can be raised with aerodrome operators . \\n',\n",
       " '',\n",
       " '',\n",
       " ' = SMS Zrínyi = \\n',\n",
       " '',\n",
       " ' SMS Zrínyi ( \" His Majesty \\'s ship Zrínyi \" ) was a Radetzky @-@ class pre @-@ dreadnought battleship ( Schlachtschiff ) of the Austro @-@ Hungarian Navy ( K.u.K. Kriegsmarine ) , named for the Zrinski , a noble Croatian family . Zrínyi and her sisters , Erzherzog Franz Ferdinand and Radetzky , were the last pre @-@ dreadnoughts built by the Austro @-@ Hungarian Navy . \\n',\n",
       " \" During World War I , Zrínyi saw action in the Adriatic Sea . She served with the Second Division of the Austro @-@ Hungarian Navy 's battleships and shelled Senigallia as part of the bombardment of the key seaport of Ancona , Italy , during May 1915 . However , Allied control of the Strait of Otranto meant that the Austro @-@ Hungarian Navy was , for all intents and purposes , effectively bottled up in the Adriatic . Nonetheless , the presence of the Zrínyi and other battleships tied down a substantial force of Allied ships . \\n\",\n",
       " ' With the war going against the Austrians by the end of 1918 , Zrínyi was prepared to be transferred to the new State of Slovenes , Croats and Serbs . On 10 November 1918 — just one day before the end of the war , navy officers sailed the battleship out of Pola ( Pula ) and eventually surrendered to a squadron of American submarine chasers . Following the handover to the United States Navy , she was briefly designated USS Zrínyi . In the Treaty of Saint @-@ Germain @-@ en @-@ Laye , the transfer was not recognized ; instead , Zrínyi was given to Italy and broken up for scrap . \\n',\n",
       " '',\n",
       " ' = = Design and construction = = \\n',\n",
       " '',\n",
       " \" Zrínyi was built at the Stabilimento Tecnico Triestino dockyard in Trieste , the same place where her sister ships were built earlier . She was laid down on 15 November 1908 and launched from the slipway on 12 April 1910 . The teak used on Zrínyi 's deck was the only material Austria @-@ Hungary had to purchase abroad to build the ship . The ship was completed by 15 July 1911 , and on 22 November 1911 she was commissioned into the fleet . She was the last ship of the class to be completed and had a crew of 880 to 890 officers and men . \\n\",\n",
       " ' Zrínyi was 138 @.@ 8 m ( 455 ft 4 in ) long , and had a beam of 24 @.@ 6 m ( 80 ft 8 in ) and a draft of 8 @.@ 1 m ( 26 ft 9 in ) . She displaced 14 @,@ 508 long tons ( 14 @,@ 741 t ) normally , and up to 15 @,@ 845 long tons ( 16 @,@ 099 t ) with a full combat load . She was powered by two @-@ shaft four @-@ cylinder vertical triple expansion engines rated at 19 @,@ 800 indicated horsepower . The ship had a top speed of 20 @.@ 5 knots ( 38 @.@ 0 km / h ; 23 @.@ 6 mph ) . Zrínyi was the first warship in the Austro @-@ Hungarian Navy to use fuel oil to supplement her 12 Yarrow @-@ type coal @-@ fired boilers . She had a maximum range of 4 @,@ 000 nautical miles ( 7 @,@ 400 km ; 4 @,@ 600 mi ) at a cruising speed of 10 knots ( 19 km / h ; 12 mph ) . \\n',\n",
       " \" The ship 's primary armament consisted of four 30 @.@ 5 cm ( 12 in ) 45 @-@ caliber guns in two twin gun turrets . This was augmented by a heavy secondary battery of eight 24 cm ( 9 @.@ 4 in ) guns in four wing turrets . The tertiary battery consisted of twenty 10 cm L / 50 guns in casemated single mounts , four 47 mm ( 1 @.@ 85 in ) L / 44 and one 47 mm L / 33 quick @-@ firing guns . Furthermore , the ship 's boats were equipped with two 66 mm ( 2 @.@ 6 in ) landing guns for operations shore . Three 45 cm ( 17 @.@ 7 in ) torpedo tubes were also carried , one on each broadside and one in the stern . \\n\",\n",
       " '',\n",
       " ' = = Service history = = \\n',\n",
       " '',\n",
       " \" The ship was assigned to the Austro @-@ Hungarian Fleet 's 1st Battle Squadron after her 1911 commissioning . In 1912 , Zrínyi and her two sister ships conducted two training cruises into the eastern Mediterranean Sea . On the second cruise into the Aegean Sea , conducted from November to December , Zrínyi and her sister ships were accompanied by the cruiser SMS Admiral Spaun and a pair of destroyers . After returning to Pola , the entire fleet mobilized for possible hostilities , as tensions flared in the Balkans . \\n\",\n",
       " ' In 1913 , Zrínyi participated in an international naval demonstration in the Ionian Sea to protest the Balkan Wars . Ships from other navies included in the demonstration were the British pre @-@ dreadnought HMS King Edward VII , the Italian pre @-@ dreadnought Ammiraglio di Saint Bon , the French armored cruiser Edgar Quinet , and the German light cruiser SMS Breslau . The most important action of the combined flotilla , which was under the command of British Admiral Cecil Burney , was to blockade the Montenegrin coast . The goal of the blockade was to prevent Serbian reinforcements from supporting the siege at Scutari , where Montenegro had besieged a combined force of Albanians and Ottomans . Pressured by the international blockade , Serbia withdrew its army from Scutari , which was subsequently occupied by a joint Allied ground force . \\n',\n",
       " ' During that year , the first of four new dreadnoughts , SMS Viribus Unitis , that made up the Tegetthoff class — the only dreadnoughts built for the Austro @-@ Hungarian Navy — came into active service . With the commissioning of these dreadnoughts , Zrínyi and her sisters were moved from the 1st Division to the 2nd Division of the 1st Battle Squadron . \\n',\n",
       " '',\n",
       " ' = = = World War I = = = \\n',\n",
       " '',\n",
       " ' At that time of the assassination of Archduke Franz Ferdinand of Austria on 28 June 1914 , the battleships in the Austro @-@ Hungarian Navy consisted of the Radetzky class , the Tegetthoff class ( which still had one ship , SMS Szent István , under construction ) , the Erzherzog Karl class and finally , the older Habsburg class . Along with the remainder of the Austro @-@ Hungarian Navy , Zrínyi was mobilized in late July 1914 to support the flight of SMS Goeben and SMS Breslau . The two German ships broke out of Messina , which was surrounded by the British navy and reached Turkey . The flotilla had advanced as far south as Brindisi in southeastern Italy when news of the successful breakout reached Vienna . The Austro @-@ Hungarian ships were then recalled before seeing action . \\n',\n",
       " ' On 23 May 1915 , between two and four hours after news of the Italian declaration of war reached the main Austro @-@ Hungarian naval base at Pola , Zrínyi and the rest of the fleet departed to bombard the Italian and Montenegrin coast . Their focus was on the important naval base at Ancona , and later the coast of Montenegro . The bombardment of Montenegro was part of the larger Austro @-@ Hungarian campaign against the Kingdoms of Montenegro and Serbia , who were members of the Entente , during the first half of 1915 . The attack on Ancona was an immense success , and the ships were unopposed during the operation . The bombardment of the province and the surrounding area resulted in the destruction of an Italian steamer in the port of Ancona itself , and an Italian destroyer , Turbine , was severely damaged further south . On the shore , the infrastructure of the port of Ancona , as well as the surrounding towns , were severely damaged . The railroad yard in Ancona , as well as the port facilities in the town , were damaged or destroyed . The local shore batteries were also suppressed . During the bombardment , Zrínyi also helped to destroy a train , a railway station , and a bridge at Senigallia . Additional targets that were damaged or destroyed included wharves , warehouses , oil tanks , radio stations , and the local barracks . Sixty @-@ three Italians , both civilians and military personnel , were killed in the bombardment . By the time Italian ships from Taranto and Brindisi arrived on the scene , the Austro @-@ Hungarians were safely back in Pola . \\n',\n",
       " ' The objective of the bombardment of Ancona was to delay the Italian Army from deploying its forces along the border with Austria @-@ Hungary by destroying critical transportation systems . The surprise attack on Ancona succeeded in delaying the Italian deployment to the Alps for two weeks . This delay gave Austria @-@ Hungary valuable time to strengthen its Italian border and re @-@ deploy some of its troops from the Eastern and Balkan fronts . \\n',\n",
       " ' Aside from the attack on Ancona , the Austro @-@ Hungarian battleships were largely confined to Pola for the duration of the war . Their operations were limited by Admiral Anton Haus , the commander of the Austro @-@ Hungarian Navy , who believed that he would need to husband his ships to counter any Italian attempt to seize the Dalmatian coast . Since coal was diverted to the newer Tegetthoff @-@ class battleships , the remainder of the war saw Zrínyi and the rest of the Austro @-@ Hungarian Navy acting as a fleet in being . This resulted in the Allied blockade of the Otranto Strait . With his fleet blockaded in the Adriatic Sea , and with a shortage of coal , Haus followed a strategy based on mines and submarines designed to reduce the numerical superiority of the Allied navies . \\n',\n",
       " '',\n",
       " ' = = = Post @-@ war fate = = = \\n',\n",
       " '',\n",
       " ' After the Austro @-@ Hungarian Empire collapsed in 1918 , the Austrians wanted to turn the fleet over to the newly created State of Slovenes , Croats and Serbs ( later to become a part of the Kingdom of Yugoslavia ) in order to prevent the Italians from claiming the ships as spoils of war . However , the victorious Allies refused to acknowledge the conversations between the Austrians and the south Slavs and , in due course , reallocated the ships . The ship had been boarded by a scratch Yugoslav crew on 10 November 1918 , one day before the Armistice , and had left Pola with her sister ship , Radetzky . They were soon spotted by heavy Italian ships , so the two battleships hoisted American flags and sailed south along the Adriatic coast to Castelli Bay near Spalato ( also known as Split ) . They appealed for American naval forces to meet them and accept their surrender , which a squadron of United States Navy ( USN ) submarine chasers in the area did . She had apparently been turned over to the fledgling south Slav state , as it was a Croat naval officer , Korvettenkapitän Marijan <unk> , who presented the ship as a prize of war to representatives of the United States Navy on the afternoon of 22 November 1919 at Spalato ( Split ) in Dalmatia . Simultaneously she was commissioned as USS Zrínyi and Lieutenant E.E. <unk> , USN , assumed command . The initial American complement consisted of four officers and 174 enlisted men — the latter entirely composed of United States Navy Reserve Force personnel . The ship remained at anchor at Spalato for nearly a year while the negotiations that would determine her ultimate fate dragged on . Only once did she apparently turn her engines over , and that occurred during a severe gale that struck Spalato on 9 February 1920 . \\n',\n",
       " ' On the morning of 7 November 1920 , Zrínyi was decommissioned . USS Chattanooga took her in tow and , assisted by Brooks and Hovey , towed the battleship to Italy . Under the terms of the treaties of Versailles and St. Germain , Zrínyi was ultimately turned over to the Italian government at Venice . She was broken up for scrap later that year and into 1921 . \\n',\n",
       " '',\n",
       " '',\n",
       " ' = Geopyxis carbonaria = \\n',\n",
       " '',\n",
       " ' Geopyxis carbonaria is a species of fungus in the genus Geopyxis , family Pyronemataceae . First described to science in 1805 , and given its current name in 1889 , the species is commonly known as the charcoal loving elf @-@ cup , dwarf acorn cup , stalked bonfire cup , or pixie cup . The small , goblet @-@ shaped fruitbodies of the fungus are reddish @-@ brown with a whitish fringe and measure up to 2 cm ( 0 @.@ 8 in ) across . They have a short , tapered stalk . Fruitbodies are commonly found on soil where brush has recently been burned , sometimes in great numbers . The fungus is distributed throughout many temperate regions of the Northern Hemisphere . It is found in Europe , Turkey , and North America . Although it is primarily a saprotrophic species , feeding on the decomposing organic matter remaining after a fire , it also forms biotrophic associations with the roots of Norway spruce . \\n',\n",
       " '',\n",
       " ' = = Taxonomy = = \\n',\n",
       " '',\n",
       " ' The fungus was first described scientifically in 1805 by Johannes Baptista von Albertini and Lewis David de Schweinitz as Peziza carbonaria . Mordecai Cubitt Cooke illustrated the fruitbodies , spores , and asci in his 1879 work <unk> , seu Icones fungorum . Figures of fungi from all parts of the world . In 1889 , Pier Andrea Saccardo transferred the fungus to the genus Geopyxis , giving the species its current name . <unk> carbonaria , published by Heinrich Rehm in 1884 , is a synonym of G. carbonaria . Louis @-@ Joseph <unk> proposed the variety Geopyxis carbonaria var. sessilis in 1937 , referring to forms producing fruitbodies without a stalk , but the taxon is not considered to have independent taxonomic significance . In 1860 Miles Berkeley and Moses Ashley Curtis described the species Peziza <unk> from collections made in Japan as part of the North Pacific Exploring and Surveying Expedition ( 1853 – 1856 ) . This taxon was synonymized with G. carbonaria by Mien Rifai in 1968 , a taxonomic opinion corroborated by Donald Pfister about a decade later . \\n',\n",
       " ' The specific epithet carbonaria derives from the Latin word for \" charcoal \" . Common names given to the fungus include \" charcoal loving elf @-@ cup \" , \" dwarf acorn cup \" , \" pixie cup \" , and the British Mycological Society approved \" stalked bonfire cup \" . \\n',\n",
       " '',\n",
       " ' = = Description = = \\n',\n",
       " '',\n",
       " ' The fruitbodies ( <unk> ) of Geopyxis <unk> are cup shaped , 1 – 2 cm wide , and have fringed whitish margins . The inner spore @-@ bearing surface of the cup , the hymenium , is brick red and smooth , while the exterior surface is a dull yellow , and may be either smooth or have blister @-@ like spots ( pustules ) . The stipe is small ( 1 – 1 @.@ 5 mm long and 1 – 2 mm wide ) , whitish in color , and expands abruptly into the cup . The brownish flesh of the fungus is thin and brittle . It does not have any distinctive taste , but has an unpleasant smell when crushed in water . The edibility of the fungus is not known , but the fruitbodies are insubstantial and unlikely to be harvested for eating . \\n',\n",
       " '',\n",
       " ' = = = Microscopic characteristics = = = \\n',\n",
       " '',\n",
       " ' In mass , the spores are whitish . The spores are elliptical , smooth , hyaline , devoid of oil droplets ( <unk> ) , and have dimensions of 13 – 18 by 7 – 9 µm . They are thin walled and germinate and grow rapidly in vitro in the absence of external stimuli . The asci are 190 – 225 by 9 – 10 µm . The paraphyses are slightly club @-@ shaped , unbranched , and have irregular orange @-@ brown granules , with tips up to 5 µm wide , and are not forked or lobed . The hypothecium , the layer of cells below the hymenium , is made of densely packed , small irregular cells . \\n',\n",
       " '',\n",
       " ' = = = Similar species = = = \\n',\n",
       " '',\n",
       " ' The closely related <unk> elf cup ( Geopyxis <unk> ) has a pale orange to yellowish fruitbody that is deeply cup shaped before flattening in maturity , and its crushed flesh often has an odor of sulfur . It may be distinguished microscopically by its paraphyses , which lack the orange @-@ brown granules characteristic of G. carbonaria . It also has larger spores , measuring 14 – 22 by 8 – 11 µm . Unlike G. carbonaria , it grows on substrates other than burned wood , including mosses , and needle duff . <unk> <unk> , which grows habitats similar to G. carbonaria , is distinguished microscopically by its spores that contain two oil droplets . Other genera with similar species with which G. carbonaria may be confused in the field include Aleuria , Caloscypha , <unk> , and <unk> . \\n',\n",
       " '',\n",
       " ' = = Habitat and distribution = = \\n',\n",
       " '',\n",
       " ' Geopyxis carbonaria is widespread on burned soil or charcoal in the spring and throughout the growing season . It is one of the most common pioneer species found on burned ground . The charred litter on the forest floor increases the underlying soil pH as well as the availability of minerals . Fruitbodies are produced from 16 to 139 weeks after a forest fire in areas with coniferous trees . Most fruitbodies are produced in the first year after a burn . The fungus prefers fruiting in microhabitats with thin <unk> duff near standing burned tree trunks . Geopyxis carbonaria fruitbodies are often found in the same post @-@ fire stands as morels , although the former is usually more abundant . Because the pixie cup fruits earlier than morels , it may serve as an indicator of imminent morel fruiting . Other cup fungi often found fruiting in the same area as G. carbonaria include those from the genera Aleuria , Anthracobia , Peziza , and <unk> . \\n',\n",
       " ' The fungus is found in Europe ( from where it was originally described ) , and is widespread throughout North America . The North American distribution extends north to Alaska . In 2010 , it was reported for the first time from Turkey . \\n',\n",
       " '',\n",
       " ' = = Ecology = = \\n',\n",
       " '',\n",
       " ' Although primarily a saprotrophic fungus involved in the post @-@ fire breakdown of duff and coniferous roots , Geopyxis carbonaria has been shown to be capable of forming ectomycorrhizae with Norway spruce ( Picea abies ) . It had been demonstrated earlier in laboratory experiments that the fungus has a biotrophic interaction with lodgepole pine ( Pinus contorta ) . The hyphae of G. carbonaria were able to infect the cortex of the tree seedling , but did not penetrate the <unk> . These traits suggest that the fungus is a moderate pathogen , with limited ability to cause reductions in seed germination . Additionally , the fungus produces the enzyme polyphenol oxidase , and can break down the complex organic polymer lignin — features characteristic of saprotrophic fungi . The formation of a rudimentary Hartig net , a characteristic of mycorrhizal fungi , indicated that G. carbonaria might be capable of forming mutualistic relationships under the right conditions . <unk> and colleagues suggest that its below @-@ ground association with spruce roots protects it from physical damage in the event of a fire , and the extensive fruitbody production after a fire may reflect \" a successful fungal escape from a dying host where the fungus no longer can maintain its biotrophic association \" . \\n',\n",
       " ' Large fruitings of the fungus are often associated with damage to the host tree , such as that which occurs with burning . A field study conducted in Norway demonstrated that fruit bodies were more likely to be found in areas that were heavily burned , compared to locations with light to moderate burning where the trees remained viable , or in clearcut areas . Fruiting was much denser in spruce forests — with up to 700 – 1000 fruitbodies per square meter — than in pine forests , where fruitbodies were sporadic . Fruitbodies grew by the millions in the year following the Yellowstone fires of 1988 . \\n',\n",
       " '',\n",
       " '',\n",
       " ' = Gold dollar = \\n',\n",
       " '',\n",
       " ' The gold dollar or gold one @-@ dollar piece was a coin struck as a regular issue by the United States Bureau of the Mint from 1849 to 1889 . The coin had three types over its lifetime , all designed by Mint Chief Engraver James B. Longacre . The Type 1 issue had the smallest diameter of any United States coin ever minted . \\n',\n",
       " ' A gold dollar had been proposed several times in the 1830s and 1840s , but was not initially adopted . Congress was finally galvanized into action by the increased supply of bullion caused by the California gold rush , and in 1849 authorized a gold dollar . In its early years , silver coins were being hoarded or exported , and the gold dollar found a ready place in commerce . Silver again circulated after Congress in 1853 required that new coins of that metal be made lighter , and the gold dollar became a rarity in commerce even before federal coins vanished from circulation because of the economic disruption caused by the American Civil War . \\n',\n",
       " ' Gold did not again circulate in most of the nation until 1879 ; once it did , the gold dollar did not regain its place . In its final years , it was struck in small numbers , causing speculation by hoarders . It was also in demand to be mounted in jewelry . The regular issue gold dollar was last struck in 1889 ; the following year , Congress ended the series . \\n',\n",
       " '',\n",
       " ' = = Background = = \\n',\n",
       " '',\n",
       " \" In proposing his plan for a mint and a coinage system , Secretary of the Treasury Alexander Hamilton in 1791 proposed that the one @-@ dollar denomination be struck both as a gold coin , and as one of silver , representative of the two metals which he proposed be made legal tender . Congress followed Hamilton 's recommendation only in part , authorizing a silver dollar , but no coin of that denomination in gold . \\n\",\n",
       " \" In 1831 , the first gold dollar was minted , at the private mint of Christopher Bechtler in North Carolina . Much of the gold then being produced in the United States came from the mountains of North Carolina and Georgia , and the dollars and other small gold coins issued by Bechtler circulated through that region , and were now and then seen further away . Additional one @-@ dollar pieces were struck by August Bechtler , Christopher 's son . \\n\",\n",
       " ' Soon after the <unk> began to strike their private issues , Secretary of the Treasury Levi Woodbury became an advocate of having the Mint of the United States ( \" Mint \" , when described as an institution ) strike the one @-@ dollar denomination in gold . He was opposed by the Mint Director , Robert M. Patterson . Woodbury persuaded President Andrew Jackson to have pattern coins struck . In response , Patterson had Mint Second Engraver Christian Gobrecht break off work on the new design for the silver one @-@ dollar coin and work on a pattern for the gold dollar . Gobrecht \\'s design featured a Liberty cap surrounded by rays on one side , and a palm branch arranged in a circle with the denomination , date , and name of the country on the other . \\n',\n",
       " ' Consideration was given to including the gold dollar as an authorized denomination in the revisionary legislation that became the Mint Act of 1837 . The Philadelphia newspaper Public Ledger , in December 1836 , supported a gold dollar , stating that \" the dollar is the smallest gold coin that would be convenient , and as it would be eminently so , neither silver nor paper should be allowed to take its place . \" Nevertheless , after Mint Director Patterson appeared before a congressional committee , the provision authorizing the gold dollar was deleted from the bill . \\n',\n",
       " '',\n",
       " ' = = Inception = = \\n',\n",
       " '',\n",
       " \" In January 1844 , North Carolina Representative James Iver McKay , the chairman of the Committee on Ways and Means , solicited the views of Director Patterson on the gold dollar . Patterson had more of Gobrecht 's pattern dollar struck to show to committee members , again advising against a coin that if issued would be only about a half inch ( 13 mm ) in diameter . He told Treasury Secretary John C. Spencer that the only gold coins of that size in commerce , the Spanish and Colombian half @-@ escudos , were unpopular and had not been struck for more than twenty years . This seemed to satisfy the committee as nothing more was done for the time , and when a gold dollar was proposed again in 1846 , McKay 's committee recommended against it . \\n\",\n",
       " \" Even before 1848 , record amounts of gold were flowing to American mints to be struck into coin , but the California Gold Rush vastly increased these quantities . This renewed calls for a gold dollar , as well as for a higher denomination than the eagle ( $ 10 piece ) , then the largest gold coin . In January 1849 , McKay introduced a bill for a gold dollar , which was referred to his committee . There was much discussion in the press about the proposed coin ; one newspaper published a proposal for an annular gold dollar , that is , with a hole in the middle to increase its small diameter . McKay amended his legislation to provide for a double eagle ( $ 20 gold coin ) and wrote to Patterson , who replied stating that the annular gold dollar would not work , and neither would another proposal to have dollar piece consisting of a gold plug in a silver coin . Nevertheless , Gobrecht 's successor as chief engraver , James B. Longacre , prepared patterns , including some with a square hole in the middle . \\n\",\n",
       " \" McKay got his fellow Democrat , New Hampshire Senator Charles Atherton , to introduce the bill to authorize the gold dollar and the double eagle in the Senate on February 1 , 1849 — Atherton was chairman of the Senate Finance Committee . McKay introduced a version into the House on February 20 ; debate began the same day . The dollar was attacked by congressmen from the Whig Party , then in the minority , on the grounds that it would be too small , would be counterfeited and in bad light might be mistakenly spent as a half dime , the coins being similar in size . McKay did not respond substantively , but stated that if no one wanted these denominations , they would not be called for at the Mint , and would not be coined . Pennsylvania Representative Joseph Ingersoll , a Whig , spoke against the bill , noting that Patterson opposed the new denominations , and that the idea had been repeatedly turned down , whenever considered . Another Whig , Massachusetts 's Charles Hudson , related that Patterson had sent a real and a counterfeit gold dollar to his committee and the majority of members had been unable to tell the difference . McKay made no answer to these claims , but others did , including New York Congressman Henry Nicoll , who assured the House that the counterfeiting allegations were greatly exaggerated . The point was , he indicated , that the double eagle and gold dollar were wanted by the public , and , in the case of the gold dollar could help money circulate in small communities where banknotes were not accepted . Connecticut Representative John A. Rockwell , a Whig , tried to table the bill , but his motion was defeated . The bill passed easily , and met only minimal opposition in the Senate , becoming law on March 3 , 1849 . \\n\",\n",
       " '',\n",
       " ' = = Preparation = = \\n',\n",
       " '',\n",
       " ' The officers at the Philadelphia Mint , including Chief Coiner Franklin Peale , were mostly the friends and relations of Director Patterson . The outsider in their midst was Chief Engraver James B. Longacre , successor to Gobrecht ( who had died in 1844 ) . A former copper @-@ plate engraver , Longacre had been appointed through the political influence of South Carolina Senator John C. Calhoun . \\n',\n",
       " \" When Longacre began work on the two new coins in early 1849 , he had no one to assist him . Longacre wrote the following year that he had been warned by a Mint employee that one of the officers ( undoubtedly Peale ) planned to undermine the chief engraver 's position by having the work of preparing designs and dies done outside Mint premises . Accordingly , when the gold coin bill became law , Longacre apprised Patterson that he was ready to begin work on the gold dollar . The Mint Director agreed , and after viewing a model of the head on the obverse , authorized Longacre to proceed with preparation of dies . According to Longacre , \\n\",\n",
       " ' The engraving was unusually minute and required very close and incessant labor for several weeks . I made the original dies and hubs for making the working dies twice over , to secure their perfect adaptation to the coining machinery . I had a wish to execute this work single handed , that I might thus silently reply to those who had questioned my ability for the work . The result , I believe , was satisfactory . \\n',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_data = \" \".join(dataset[\"text\"])\n",
    "# train_loader = create_dataloader_v1(txt_data, batch_size=4, max_length=256, stride=128)\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(txt_data))\n",
    "train_data = txt_data[:split_idx]\n",
    "val_data = txt_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Input batch (tokens): tensor([[ 2422,  9964,  1028,  4849, 17072,   772,   981,   991,   739,  1886,\n",
      "          2162,  3977, 12444,   284, 21194,   625,  9964,   810, 26402,   373,\n",
      "           407,  1682,   287,  3877,   837, 41109,   284,  1277,   465, 13463,\n",
      "          3371,   262, 39785,  5822,  2138,   621, 34618, 22576,   764,  3977,\n",
      "           705,    82,  2106,   460,   307,  1775,   355,   281,  8453,   544,\n",
      "           837,   257, 16716,  3761,   837,   329,   262, 13239,   837,   290,\n",
      "           517,  5734,   329, 26402,   705,    82,  3896,   764,  2750,   262,\n",
      "          1367,  2154,    82,   290,  1367,  1795,    82,   837,  8830, 20006,\n",
      "           547, 18416,   284,  1104,   262, 13239,   837, 11476,   780,   340,\n",
      "           373,  1290,  1497,   290,   612,   547,   517, 12273,  4786,   287,\n",
      "          2031,   837,   475,   635,   780,   443,  1676,  1837,   373,  3221,\n",
      "          3177, 11871,  9837,   764,   220,   198,   220,  3977,   373, 20524,\n",
      "         21925,  1028,   262, 15188, 41741,   837,  4150,   339],\n",
      "        [  796,   220,   198,   220,   220,  5890,  9626,   272,  6866,  1878,\n",
      "           666,  8948,   366,  1867, 35337, 10383,   366,   329,  9110,   287,\n",
      "           262,  3806,  5646,  4128,   278,   329,   257, 30647,  7171,  6536,\n",
      "           379,   262,  8190,   358, 11460,  8079, 33953, 15434,   764,   679,\n",
      "           750,   407,  3328,   257, 11872,   764,   220,   198,   220,   220,\n",
      "           220,   796,  1610,  2007,   286, 21385,  2183,   796,   220,   198,\n",
      "           220,   220,   383, 46116,   286,   257,   781,  2007,   286, 21385,\n",
      "           284,  6405, 11886,   508,   460, 21192,   284,   407,  1719, 44652,\n",
      "           511,  4845,   329,   257,   614,   290,   257,  1110,   318,   281,\n",
      "          1468,  6761,   837,   262, 30468,   286,   543,   991,  7866,   287,\n",
      "           617, 16511,   287,  4492,   764,   383,  6761,   373,  9456,   379,\n",
      "          1279,  2954,    29,  1566,   379,  1551,   262, 45592,  4289,   475,\n",
      "           783,  3793,   691,   355,   257, 39510,   625,   262],\n",
      "        [10423,   351,  7212,   705,    82,  5156,   837,   475,  9329,   268,\n",
      "         10497,   284,  1104,   607,   290,   484,  1716, 11589,  7953,   764,\n",
      "         38921,   306,  5213,   326, 35136,    68,   743,   307,   257, 42306,\n",
      "         31866,   837,  9329,   268,  5651,   689,   511, 12352,   764, 35136,\n",
      "            68,   318,  1568,  7384,   416,   257, 16914,  2488,    12,    31,\n",
      "         28357,  5827,   837,   290,   618,  9329,   268,  6370,   284,  4404,\n",
      "           607,   837,   339,   318,  2277,   287,   262,  1182,   351,   257,\n",
      "          5405,  9294,   290, 10564,   764, 35136,    68, 21046,   257, 14608,\n",
      "           287,   262, 16280,   286,  9329,   268,   705,    82,  1918,   837,\n",
      "           290,   318,  6848,   284,   257, 19906,  4326,   764,  1375,  2116,\n",
      "          2488,    12,    31, 34859,   837,   290, 17567,   284,  2666,   262,\n",
      "          4326,   618,   673,  2925,   656, 10515,   837,  1566,  8445, 45837,\n",
      "           607,   326,  7212,   857,   407, 14765,   284,  1011],\n",
      "        [ 2102,   837,   262,  9513,   373,   257,  1598,  5373,   329,   262,\n",
      "          4141,  9906,   326,  8998,   284,  1011, 46522,   290,   555,  2617,\n",
      "          7100,  3594, 17091, 27580,  3592,   764,  3125,   621,  1802, 46522,\n",
      "           547,  2077,   837,   290,   546,  2319,  1411,   286,   262,  7404,\n",
      "          7777,   547,  6572,   764,   220,   198,   220,  4900, 11001,   837,\n",
      "           262,  9513, 11472,   968,  4492, 43430,   837,  2252, 23775,  1143,\n",
      "          2316,   351,   262,  4141,   290,   511, 12547,  1605,  7681,   837,\n",
      "           290,  2957,   284,   517,  2055,  1175,  5597,  1108,   287, 27580,\n",
      "         18573,   764,   383,  9513,   468,   587, 26156,  1143,   355,   257,\n",
      "           636,   286,   262,  1903,  1605, 27580,  1621,   837, 41889,  2233,\n",
      "           284,   262,  1848,   286,   530,   286,   663, 46522,   837,   262,\n",
      "          5416,    13,  1757,  6484,   764,   679,   290,   465,  1641,   547,\n",
      "          4137,   284,   787,   262,   890,   625,  1044,  7002]])\n",
      "Target batch (tokens): tensor([[ 9964,  1028,  4849, 17072,   772,   981,   991,   739,  1886,  2162,\n",
      "          3977, 12444,   284, 21194,   625,  9964,   810, 26402,   373,   407,\n",
      "          1682,   287,  3877,   837, 41109,   284,  1277,   465, 13463,  3371,\n",
      "           262, 39785,  5822,  2138,   621, 34618, 22576,   764,  3977,   705,\n",
      "            82,  2106,   460,   307,  1775,   355,   281,  8453,   544,   837,\n",
      "           257, 16716,  3761,   837,   329,   262, 13239,   837,   290,   517,\n",
      "          5734,   329, 26402,   705,    82,  3896,   764,  2750,   262,  1367,\n",
      "          2154,    82,   290,  1367,  1795,    82,   837,  8830, 20006,   547,\n",
      "         18416,   284,  1104,   262, 13239,   837, 11476,   780,   340,   373,\n",
      "          1290,  1497,   290,   612,   547,   517, 12273,  4786,   287,  2031,\n",
      "           837,   475,   635,   780,   443,  1676,  1837,   373,  3221,  3177,\n",
      "         11871,  9837,   764,   220,   198,   220,  3977,   373, 20524, 21925,\n",
      "          1028,   262, 15188, 41741,   837,  4150,   339,  4762],\n",
      "        [  220,   198,   220,   220,  5890,  9626,   272,  6866,  1878,   666,\n",
      "          8948,   366,  1867, 35337, 10383,   366,   329,  9110,   287,   262,\n",
      "          3806,  5646,  4128,   278,   329,   257, 30647,  7171,  6536,   379,\n",
      "           262,  8190,   358, 11460,  8079, 33953, 15434,   764,   679,   750,\n",
      "           407,  3328,   257, 11872,   764,   220,   198,   220,   220,   220,\n",
      "           796,  1610,  2007,   286, 21385,  2183,   796,   220,   198,   220,\n",
      "           220,   383, 46116,   286,   257,   781,  2007,   286, 21385,   284,\n",
      "          6405, 11886,   508,   460, 21192,   284,   407,  1719, 44652,   511,\n",
      "          4845,   329,   257,   614,   290,   257,  1110,   318,   281,  1468,\n",
      "          6761,   837,   262, 30468,   286,   543,   991,  7866,   287,   617,\n",
      "         16511,   287,  4492,   764,   383,  6761,   373,  9456,   379,  1279,\n",
      "          2954,    29,  1566,   379,  1551,   262, 45592,  4289,   475,   783,\n",
      "          3793,   691,   355,   257, 39510,   625,   262, 44250],\n",
      "        [  351,  7212,   705,    82,  5156,   837,   475,  9329,   268, 10497,\n",
      "           284,  1104,   607,   290,   484,  1716, 11589,  7953,   764, 38921,\n",
      "           306,  5213,   326, 35136,    68,   743,   307,   257, 42306, 31866,\n",
      "           837,  9329,   268,  5651,   689,   511, 12352,   764, 35136,    68,\n",
      "           318,  1568,  7384,   416,   257, 16914,  2488,    12,    31, 28357,\n",
      "          5827,   837,   290,   618,  9329,   268,  6370,   284,  4404,   607,\n",
      "           837,   339,   318,  2277,   287,   262,  1182,   351,   257,  5405,\n",
      "          9294,   290, 10564,   764, 35136,    68, 21046,   257, 14608,   287,\n",
      "           262, 16280,   286,  9329,   268,   705,    82,  1918,   837,   290,\n",
      "           318,  6848,   284,   257, 19906,  4326,   764,  1375,  2116,  2488,\n",
      "            12,    31, 34859,   837,   290, 17567,   284,  2666,   262,  4326,\n",
      "           618,   673,  2925,   656, 10515,   837,  1566,  8445, 45837,   607,\n",
      "           326,  7212,   857,   407, 14765,   284,  1011,   511],\n",
      "        [  837,   262,  9513,   373,   257,  1598,  5373,   329,   262,  4141,\n",
      "          9906,   326,  8998,   284,  1011, 46522,   290,   555,  2617,  7100,\n",
      "          3594, 17091, 27580,  3592,   764,  3125,   621,  1802, 46522,   547,\n",
      "          2077,   837,   290,   546,  2319,  1411,   286,   262,  7404,  7777,\n",
      "           547,  6572,   764,   220,   198,   220,  4900, 11001,   837,   262,\n",
      "          9513, 11472,   968,  4492, 43430,   837,  2252, 23775,  1143,  2316,\n",
      "           351,   262,  4141,   290,   511, 12547,  1605,  7681,   837,   290,\n",
      "          2957,   284,   517,  2055,  1175,  5597,  1108,   287, 27580, 18573,\n",
      "           764,   383,  9513,   468,   587, 26156,  1143,   355,   257,   636,\n",
      "           286,   262,  1903,  1605, 27580,  1621,   837, 41889,  2233,   284,\n",
      "           262,  1848,   286,   530,   286,   663, 46522,   837,   262,  5416,\n",
      "            13,  1757,  6484,   764,   679,   290,   465,  1641,   547,  4137,\n",
      "           284,   787,   262,   890,   625,  1044,  7002,   284]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (input_batch, target_batch) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(\"Input batch (tokens):\", input_batch)\n",
    "    print(\"Target batch (tokens):\", target_batch)\n",
    "    break  # Only printing the first batch as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Note:\n",
    "# # Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# # which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# # However, the resulting loss values may be slightly different.\n",
    "\n",
    "# #if torch.cuda.is_available():\n",
    "# #    device = torch.device(\"cuda\")\n",
    "# #elif torch.backends.mps.is_available():\n",
    "# #    device = torch.device(\"mps\")\n",
    "# #else:\n",
    "# #    device = torch.device(\"cpu\")\n",
    "# #\n",
    "# # print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "# model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "# torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "# with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "#     train_loss = calc_loss_loader(train_loader, model, device)\n",
    "#     val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "# print(\"Training loss:\", train_loss)\n",
    "# print(\"Validation loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import deepspeed\n",
    "from tqdm import tqdm  # tqdm을 함수처럼 사용\n",
    "\n",
    "# Setup logging configuration\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "    level=logging.INFO,  # You can adjust the level to DEBUG, INFO, WARNING, etc.\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training_log.txt\"),  # Logs will be saved to this file\n",
    "        logging.StreamHandler()  # Also log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now use logging instead of print\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeed\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer, deepspeed_config):\n",
    "\n",
    "    # Initialize DeepSpeed\n",
    "    model, optimizer, _, _ = deepspeed.initialize(args=None, model=model, optimizer=optimizer, config_params=deepspeed_config)\n",
    "\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        logger.info(f\"Starting Epoch {epoch+1}...\")\n",
    "\n",
    "        for input_batch, target_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\"):\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "            # print(loss)\n",
    "            # print(f\"Loss type: {loss.dtype}\")\n",
    "            # print(f\"Loss scale type: {model.loss_scale.dtype}\")\n",
    "            # if not isinstance(loss, torch.Tensor):\n",
    "            #     loss = torch.tensor(loss).to(device)\n",
    "\n",
    "\n",
    "            # Backward pass and step the optimizer\n",
    "            model.backward(loss)  # Backward pass with DeepSpeed\n",
    "            model.step()  # Step optimizer using DeepSpeed\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                logger.info(f\"Epoch {epoch+1} (Step {global_step:06d}): \"\n",
    "                            f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "                \n",
    "                \n",
    "        logger.info(f\"Epoch {epoch+1} completed. Generating a sample...\")\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    logger.info(f\"Generated Text: {decoded_text}\")  # Log the generated text\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:35:32,917] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown\n",
      "[2025-04-02 19:35:32,919] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n",
      "[2025-04-02 19:35:32,924] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-04-02 19:35:32,925] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-04-02 19:35:32,925] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-04-02 19:35:32,926] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-04-02 19:35:32,926] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2025-04-02 19:35:32,926] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2025-04-02 19:35:32,926] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000\n",
      "[2025-04-02 19:35:32,927] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000\n",
      "[2025-04-02 19:35:32,927] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2025-04-02 19:35:32,927] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2025-04-02 19:35:34,159] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-04-02 19:35:34,160] [INFO] [utils.py:782:see_memory_usage] MA 9.52 GB         Max_MA 9.66 GB         CA 10.4 GB         Max_CA 10 GB \n",
      "[2025-04-02 19:35:34,161] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 15.94 GB, percent = 12.7%\n",
      "[2025-04-02 19:35:34,867] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-04-02 19:35:34,868] [INFO] [utils.py:782:see_memory_usage] MA 9.52 GB         Max_MA 9.63 GB         CA 10.4 GB         Max_CA 10 GB \n",
      "[2025-04-02 19:35:34,869] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 15.94 GB, percent = 12.7%\n",
      "[2025-04-02 19:35:34,869] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized\n",
      "[2025-04-02 19:35:35,611] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-04-02 19:35:35,613] [INFO] [utils.py:782:see_memory_usage] MA 9.52 GB         Max_MA 9.52 GB         CA 10.4 GB         Max_CA 10 GB \n",
      "[2025-04-02 19:35:35,614] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 15.94 GB, percent = 12.7%\n",
      "[2025-04-02 19:35:35,616] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-04-02 19:35:35,616] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-04-02 19:35:35,616] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-04-02 19:35:35,616] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0004], mom=[(0.9, 0.999)]\n",
      "[2025-04-02 19:35:35,617] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:\n",
      "[2025-04-02 19:35:35,617] [INFO] [config.py:1004:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-04-02 19:35:35,617] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-04-02 19:35:35,618] [INFO] [config.py:1004:print]   amp_enabled .................. False\n",
      "[2025-04-02 19:35:35,618] [INFO] [config.py:1004:print]   amp_params ................... False\n",
      "[2025-04-02 19:35:35,618] [INFO] [config.py:1004:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-04-02 19:35:35,618] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False\n",
      "[2025-04-02 19:35:35,619] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-04-02 19:35:35,619] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-04-02 19:35:35,619] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-04-02 19:35:35,619] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-04-02 19:35:35,619] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7671b0ee00>\n",
      "[2025-04-02 19:35:35,620] [INFO] [config.py:1004:print]   communication_data_type ...... None\n",
      "[2025-04-02 19:35:35,620] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-04-02 19:35:35,620] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False\n",
      "[2025-04-02 19:35:35,620] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False\n",
      "[2025-04-02 19:35:35,620] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-04-02 19:35:35,620] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False\n",
      "[2025-04-02 19:35:35,621] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False\n",
      "[2025-04-02 19:35:35,621] [INFO] [config.py:1004:print]   disable_allgather ............ False\n",
      "[2025-04-02 19:35:35,621] [INFO] [config.py:1004:print]   dump_state ................... False\n",
      "[2025-04-02 19:35:35,621] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-04-02 19:35:35,621] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False\n",
      "[2025-04-02 19:35:35,622] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-04-02 19:35:35,622] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-04-02 19:35:35,622] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-04-02 19:35:35,622] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-04-02 19:35:35,622] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-04-02 19:35:35,623] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-04-02 19:35:35,623] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False\n",
      "[2025-04-02 19:35:35,623] [INFO] [config.py:1004:print]   elasticity_enabled ........... False\n",
      "[2025-04-02 19:35:35,623] [INFO] [config.py:1004:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-04-02 19:35:35,623] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False\n",
      "[2025-04-02 19:35:35,624] [INFO] [config.py:1004:print]   fp16_enabled ................. True\n",
      "[2025-04-02 19:35:35,624] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-04-02 19:35:35,625] [INFO] [config.py:1004:print]   global_rank .................. 0\n",
      "[2025-04-02 19:35:35,625] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None\n",
      "[2025-04-02 19:35:35,625] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 2\n",
      "[2025-04-02 19:35:35,625] [INFO] [config.py:1004:print]   gradient_clipping ............ 0.0\n",
      "[2025-04-02 19:35:35,626] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-04-02 19:35:35,626] [INFO] [config.py:1004:print]   graph_harvesting ............. False\n",
      "[2025-04-02 19:35:35,626] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-04-02 19:35:35,626] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-04-02 19:35:35,626] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False\n",
      "[2025-04-02 19:35:35,627] [INFO] [config.py:1004:print]   loss_scale ................... 0\n",
      "[2025-04-02 19:35:35,627] [INFO] [config.py:1004:print]   memory_breakdown ............. False\n",
      "[2025-04-02 19:35:35,627] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False\n",
      "[2025-04-02 19:35:35,627] [INFO] [config.py:1004:print]   mics_shard_size .............. -1\n",
      "[2025-04-02 19:35:35,628] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-04-02 19:35:35,628] [INFO] [config.py:1004:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-04-02 19:35:35,628] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-04-02 19:35:35,628] [INFO] [config.py:1004:print]   optimizer_name ............... None\n",
      "[2025-04-02 19:35:35,628] [INFO] [config.py:1004:print]   optimizer_params ............. None\n",
      "[2025-04-02 19:35:35,628] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-04-02 19:35:35,629] [INFO] [config.py:1004:print]   pld_enabled .................. False\n",
      "[2025-04-02 19:35:35,629] [INFO] [config.py:1004:print]   pld_params ................... False\n",
      "[2025-04-02 19:35:35,629] [INFO] [config.py:1004:print]   prescale_gradients ........... False\n",
      "[2025-04-02 19:35:35,629] [INFO] [config.py:1004:print]   scheduler_name ............... None\n",
      "[2025-04-02 19:35:35,630] [INFO] [config.py:1004:print]   scheduler_params ............. None\n",
      "[2025-04-02 19:35:35,630] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-04-02 19:35:35,630] [INFO] [config.py:1004:print]   sparse_attention ............. None\n",
      "[2025-04-02 19:35:35,630] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False\n",
      "[2025-04-02 19:35:35,631] [INFO] [config.py:1004:print]   steps_per_print .............. None\n",
      "[2025-04-02 19:35:35,631] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-04-02 19:35:35,631] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-04-02 19:35:35,631] [INFO] [config.py:1004:print]   train_batch_size ............. 8\n",
      "[2025-04-02 19:35:35,631] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  4\n",
      "[2025-04-02 19:35:35,632] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False\n",
      "[2025-04-02 19:35:35,632] [INFO] [config.py:1004:print]   use_node_local_storage ....... False\n",
      "[2025-04-02 19:35:35,632] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False\n",
      "[2025-04-02 19:35:35,632] [INFO] [config.py:1004:print]   weight_quantization_config ... None\n",
      "[2025-04-02 19:35:35,633] [INFO] [config.py:1004:print]   world_size ................... 1\n",
      "[2025-04-02 19:35:35,633] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False\n",
      "[2025-04-02 19:35:35,633] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-04-02 19:35:35,633] [INFO] [config.py:1004:print]   zero_enabled ................. True\n",
      "[2025-04-02 19:35:35,634] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-04-02 19:35:35,634] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2\n",
      "[2025-04-02 19:35:35,634] [INFO] [config.py:990:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"gradient_accumulation_steps\": 2, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:35:35,634 - INFO - Starting Epoch 1...\n",
      "Epoch 1:   0%|          | 0/210244 [00:00<?, ?batch/s]2025-04-02 19:35:35,733 - INFO - Epoch 1 (Step 000000): Train loss 10.962, Val loss 10.964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:35:35,746] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 2/210244 [00:00<3:15:37, 17.91batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:35:35,775] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2025-04-02 19:35:35,794] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "[2025-04-02 19:35:35,814] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\n",
      "[2025-04-02 19:35:35,833] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\n",
      "[2025-04-02 19:35:35,852] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 12/210244 [00:00<56:20, 62.19batch/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:35:35,871] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\n",
      "[2025-04-02 19:35:35,889] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\n",
      "[2025-04-02 19:35:35,908] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\n",
      "[2025-04-02 19:35:35,927] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\n",
      "[2025-04-02 19:35:35,947] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 23/210244 [00:00<42:48, 81.86batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:35:35,967] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576\n",
      "[2025-04-02 19:35:35,986] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288\n",
      "[2025-04-02 19:35:36,005] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144\n",
      "[2025-04-02 19:35:36,024] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072\n",
      "[2025-04-02 19:35:36,043] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 998/210244 [00:12<43:10, 80.78batch/s]2025-04-02 19:35:48,115 - INFO - Epoch 1 (Step 001000): Train loss 6.612, Val loss 6.774\n",
      "Epoch 1:   1%|          | 1993/210244 [00:26<43:04, 80.57batch/s]  2025-04-02 19:36:01,885 - INFO - Epoch 1 (Step 002000): Train loss 6.179, Val loss 6.484\n",
      "Epoch 1:   1%|          | 2028/210244 [00:26<45:48, 75.76batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:36:02,292] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▏         | 2995/210244 [00:38<42:37, 81.03batch/s]2025-04-02 19:36:14,347 - INFO - Epoch 1 (Step 003000): Train loss 5.915, Val loss 6.286\n",
      "Epoch 1:   2%|▏         | 3996/210244 [00:51<43:20, 79.30batch/s]2025-04-02 19:36:27,038 - INFO - Epoch 1 (Step 004000): Train loss 5.982, Val loss 6.175\n",
      "Epoch 1:   2%|▏         | 4030/210244 [00:51<47:24, 72.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:36:27,491] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|▏         | 4996/210244 [01:05<42:18, 80.84batch/s]  2025-04-02 19:36:40,880 - INFO - Epoch 1 (Step 005000): Train loss 5.952, Val loss 6.035\n",
      "Epoch 1:   3%|▎         | 5992/210244 [01:17<44:24, 76.66batch/s]2025-04-02 19:36:53,375 - INFO - Epoch 1 (Step 006000): Train loss 5.710, Val loss 5.974\n",
      "Epoch 1:   3%|▎         | 6034/210244 [01:18<45:04, 75.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:36:53,841] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|▎         | 7000/210244 [01:31<44:01, 76.93batch/s]  2025-04-02 19:37:07,207 - INFO - Epoch 1 (Step 007000): Train loss 5.779, Val loss 5.883\n",
      "Epoch 1:   4%|▍         | 7995/210244 [01:43<42:01, 80.22batch/s]2025-04-02 19:37:19,767 - INFO - Epoch 1 (Step 008000): Train loss 5.609, Val loss 5.805\n",
      "Epoch 1:   4%|▍         | 8031/210244 [01:44<43:26, 77.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:37:20,243] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▍         | 8993/210244 [01:56<41:10, 81.45batch/s]2025-04-02 19:37:32,170 - INFO - Epoch 1 (Step 009000): Train loss 5.456, Val loss 5.731\n",
      "Epoch 1:   5%|▍         | 9997/210244 [02:10<39:58, 83.49batch/s]  2025-04-02 19:37:45,995 - INFO - Epoch 1 (Step 010000): Train loss 5.433, Val loss 5.659\n",
      "Epoch 1:   5%|▍         | 10041/210244 [02:10<42:38, 78.26batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:37:46,503] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   5%|▌         | 10998/210244 [02:22<42:31, 78.10batch/s]2025-04-02 19:37:58,436 - INFO - Epoch 1 (Step 011000): Train loss 5.447, Val loss 5.661\n",
      "Epoch 1:   6%|▌         | 11999/210244 [02:35<41:33, 79.52batch/s]2025-04-02 19:38:10,886 - INFO - Epoch 1 (Step 012000): Train loss 5.377, Val loss 5.597\n",
      "Epoch 1:   6%|▌         | 12039/210244 [02:35<44:19, 74.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:38:11,442] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   6%|▌         | 12997/210244 [02:49<40:41, 80.80batch/s]  2025-04-02 19:38:24,806 - INFO - Epoch 1 (Step 013000): Train loss 5.536, Val loss 5.524\n",
      "Epoch 1:   7%|▋         | 14000/210244 [03:01<40:33, 80.64batch/s]2025-04-02 19:38:37,254 - INFO - Epoch 1 (Step 014000): Train loss 5.370, Val loss 5.480\n",
      "Epoch 1:   7%|▋         | 14043/210244 [03:02<42:19, 77.26batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:38:37,817] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   7%|▋         | 14994/210244 [03:15<40:19, 80.69batch/s]  2025-04-02 19:38:50,961 - INFO - Epoch 1 (Step 015000): Train loss 5.252, Val loss 5.414\n",
      "Epoch 1:   8%|▊         | 15995/210244 [03:27<39:17, 82.39batch/s]2025-04-02 19:39:03,467 - INFO - Epoch 1 (Step 016000): Train loss 5.098, Val loss 5.375\n",
      "Epoch 1:   8%|▊         | 16039/210244 [03:28<41:40, 77.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:39:04,050] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   8%|▊         | 16996/210244 [03:40<39:57, 80.61batch/s]2025-04-02 19:39:15,946 - INFO - Epoch 1 (Step 017000): Train loss 5.140, Val loss 5.380\n",
      "Epoch 1:   9%|▊         | 17993/210244 [03:53<39:19, 81.49batch/s]  2025-04-02 19:39:29,738 - INFO - Epoch 1 (Step 018000): Train loss 5.149, Val loss 5.352\n",
      "Epoch 1:   9%|▊         | 18044/210244 [03:54<41:26, 77.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:39:30,350] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   9%|▉         | 18997/210244 [04:06<38:25, 82.95batch/s]2025-04-02 19:39:42,219 - INFO - Epoch 1 (Step 019000): Train loss 5.005, Val loss 5.326\n",
      "Epoch 1:  10%|▉         | 19993/210244 [04:20<39:16, 80.74batch/s]  2025-04-02 19:39:56,151 - INFO - Epoch 1 (Step 020000): Train loss 4.980, Val loss 5.261\n",
      "Epoch 1:  10%|▉         | 20051/210244 [04:21<40:48, 77.69batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:39:56,804] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|▉         | 20992/210244 [04:32<38:38, 81.63batch/s]2025-04-02 19:40:08,640 - INFO - Epoch 1 (Step 021000): Train loss 5.099, Val loss 5.248\n",
      "Epoch 1:  10%|█         | 21998/210244 [04:45<38:38, 81.20batch/s]2025-04-02 19:40:21,090 - INFO - Epoch 1 (Step 022000): Train loss 5.113, Val loss 5.227\n",
      "Epoch 1:  10%|█         | 22049/210244 [04:46<40:09, 78.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:40:21,748] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  11%|█         | 22998/210244 [04:59<38:42, 80.62batch/s]  2025-04-02 19:40:35,064 - INFO - Epoch 1 (Step 023000): Train loss 4.841, Val loss 5.220\n",
      "Epoch 1:  11%|█▏        | 23994/210244 [05:11<39:37, 78.35batch/s]2025-04-02 19:40:47,508 - INFO - Epoch 1 (Step 024000): Train loss 4.993, Val loss 5.173\n",
      "Epoch 1:  11%|█▏        | 24052/210244 [05:12<40:10, 77.23batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:40:48,208] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  12%|█▏        | 25000/210244 [05:24<37:56, 81.39batch/s]2025-04-02 19:40:59,967 - INFO - Epoch 1 (Step 025000): Train loss 5.161, Val loss 5.166\n",
      "Epoch 1:  12%|█▏        | 26000/210244 [05:38<37:44, 81.37batch/s]  2025-04-02 19:41:13,892 - INFO - Epoch 1 (Step 026000): Train loss 4.877, Val loss 5.150\n",
      "Epoch 1:  12%|█▏        | 26051/210244 [05:38<39:18, 78.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:41:14,603] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 26994/210244 [05:50<38:07, 80.12batch/s]2025-04-02 19:41:26,317 - INFO - Epoch 1 (Step 027000): Train loss 4.948, Val loss 5.116\n",
      "Epoch 1:  13%|█▎        | 27998/210244 [06:04<37:25, 81.15batch/s]  2025-04-02 19:41:40,029 - INFO - Epoch 1 (Step 028000): Train loss 4.870, Val loss 5.108\n",
      "Epoch 1:  13%|█▎        | 28059/210244 [06:05<38:18, 79.26batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:41:40,762] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  14%|█▍        | 28994/210244 [06:16<37:40, 80.17batch/s]2025-04-02 19:41:52,522 - INFO - Epoch 1 (Step 029000): Train loss 4.804, Val loss 5.073\n",
      "Epoch 1:  14%|█▍        | 29996/210244 [06:29<37:14, 80.68batch/s]2025-04-02 19:42:04,974 - INFO - Epoch 1 (Step 030000): Train loss 5.028, Val loss 5.061\n",
      "Epoch 1:  14%|█▍        | 30057/210244 [06:30<37:44, 79.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:42:05,733] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▍        | 30997/210244 [06:42<36:36, 81.60batch/s]  2025-04-02 19:42:18,738 - INFO - Epoch 1 (Step 031000): Train loss 5.032, Val loss 5.033\n",
      "Epoch 1:  15%|█▌        | 31993/210244 [06:55<36:33, 81.26batch/s]2025-04-02 19:42:31,141 - INFO - Epoch 1 (Step 032000): Train loss 4.833, Val loss 5.016\n",
      "Epoch 1:  15%|█▌        | 32060/210244 [06:56<41:46, 71.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:42:31,988] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  16%|█▌        | 32992/210244 [07:09<39:06, 75.53batch/s]  2025-04-02 19:42:45,064 - INFO - Epoch 1 (Step 033000): Train loss 4.821, Val loss 5.018\n",
      "Epoch 1:  16%|█▌        | 33996/210244 [07:21<37:01, 79.35batch/s]2025-04-02 19:42:57,562 - INFO - Epoch 1 (Step 034000): Train loss 4.846, Val loss 4.977\n",
      "Epoch 1:  16%|█▌        | 34063/210244 [07:22<37:22, 78.56batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:42:58,376] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 35000/210244 [07:34<36:47, 79.37batch/s]2025-04-02 19:43:10,165 - INFO - Epoch 1 (Step 035000): Train loss 4.711, Val loss 4.998\n",
      "Epoch 1:  17%|█▋        | 35999/210244 [07:48<35:13, 82.46batch/s]  2025-04-02 19:43:23,924 - INFO - Epoch 1 (Step 036000): Train loss 4.912, Val loss 4.968\n",
      "Epoch 1:  17%|█▋        | 36060/210244 [07:49<36:47, 78.92batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:43:24,773] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  18%|█▊        | 36993/210244 [08:00<34:58, 82.56batch/s]2025-04-02 19:43:36,383 - INFO - Epoch 1 (Step 037000): Train loss 4.785, Val loss 4.957\n",
      "Epoch 1:  18%|█▊        | 37999/210244 [08:14<47:55, 59.90batch/s]  2025-04-02 19:43:50,219 - INFO - Epoch 1 (Step 038000): Train loss 4.701, Val loss 4.968\n",
      "Epoch 1:  18%|█▊        | 38061/210244 [08:15<36:43, 78.16batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:43:51,067] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▊        | 38994/210244 [08:26<35:14, 81.01batch/s]2025-04-02 19:44:02,625 - INFO - Epoch 1 (Step 039000): Train loss 4.842, Val loss 4.950\n",
      "Epoch 1:  19%|█▉        | 39998/210244 [08:39<35:46, 79.33batch/s]2025-04-02 19:44:15,363 - INFO - Epoch 1 (Step 040000): Train loss 4.646, Val loss 4.935\n",
      "Epoch 1:  19%|█▉        | 40065/210244 [08:40<36:01, 78.73batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:44:16,264] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▉        | 40995/210244 [08:53<34:28, 81.83batch/s]  2025-04-02 19:44:29,239 - INFO - Epoch 1 (Step 041000): Train loss 4.710, Val loss 4.920\n",
      "Epoch 1:  20%|█▉        | 42000/210244 [09:06<35:10, 79.72batch/s]2025-04-02 19:44:41,754 - INFO - Epoch 1 (Step 042000): Train loss 4.660, Val loss 4.893\n",
      "Epoch 1:  20%|██        | 42069/210244 [09:06<35:13, 79.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:44:42,669] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  20%|██        | 42997/210244 [09:18<34:15, 81.36batch/s]2025-04-02 19:44:54,360 - INFO - Epoch 1 (Step 043000): Train loss 4.679, Val loss 4.906\n",
      "Epoch 1:  21%|██        | 43995/210244 [09:32<33:48, 81.96batch/s]  2025-04-02 19:45:08,171 - INFO - Epoch 1 (Step 044000): Train loss 4.649, Val loss 4.914\n",
      "Epoch 1:  21%|██        | 44075/210244 [09:33<34:16, 80.79batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:45:09,095] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  21%|██▏       | 44995/210244 [09:44<33:54, 81.21batch/s]2025-04-02 19:45:20,584 - INFO - Epoch 1 (Step 045000): Train loss 4.527, Val loss 4.896\n",
      "Epoch 1:  22%|██▏       | 45998/210244 [09:58<34:37, 79.06batch/s]  2025-04-02 19:45:34,309 - INFO - Epoch 1 (Step 046000): Train loss 4.762, Val loss 4.895\n",
      "Epoch 1:  22%|██▏       | 46069/210244 [09:59<33:54, 80.71batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:45:35,258] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  22%|██▏       | 46994/210244 [10:11<33:45, 80.60batch/s]2025-04-02 19:45:46,856 - INFO - Epoch 1 (Step 047000): Train loss 4.712, Val loss 4.890\n",
      "Epoch 1:  23%|██▎       | 47998/210244 [10:23<33:47, 80.03batch/s]2025-04-02 19:45:59,393 - INFO - Epoch 1 (Step 048000): Train loss 4.768, Val loss 4.876\n",
      "Epoch 1:  23%|██▎       | 48077/210244 [10:24<33:29, 80.70batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:46:00,370] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  23%|██▎       | 48992/210244 [10:37<33:14, 80.85batch/s]  2025-04-02 19:46:13,250 - INFO - Epoch 1 (Step 049000): Train loss 4.529, Val loss 4.884\n",
      "Epoch 1:  24%|██▍       | 49996/210244 [10:49<32:59, 80.96batch/s]2025-04-02 19:46:25,764 - INFO - Epoch 1 (Step 050000): Train loss 4.691, Val loss 4.877\n",
      "Epoch 1:  24%|██▍       | 50079/210244 [10:51<35:24, 75.38batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:46:26,820] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  24%|██▍       | 50999/210244 [11:03<32:59, 80.46batch/s]  2025-04-02 19:46:39,681 - INFO - Epoch 1 (Step 051000): Train loss 4.652, Val loss 4.830\n",
      "Epoch 1:  25%|██▍       | 51993/210244 [11:16<32:14, 81.79batch/s]2025-04-02 19:46:52,034 - INFO - Epoch 1 (Step 052000): Train loss 4.330, Val loss 4.842\n",
      "Epoch 1:  25%|██▍       | 52082/210244 [11:17<34:04, 77.36batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:46:53,067] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▌       | 52996/210244 [11:28<34:35, 75.75batch/s]2025-04-02 19:47:04,660 - INFO - Epoch 1 (Step 053000): Train loss 4.581, Val loss 4.839\n",
      "Epoch 1:  26%|██▌       | 53992/210244 [11:42<32:03, 81.25batch/s]  2025-04-02 19:47:18,596 - INFO - Epoch 1 (Step 054000): Train loss 4.668, Val loss 4.849\n",
      "Epoch 1:  26%|██▌       | 54077/210244 [11:43<32:38, 79.73batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:47:19,669] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  26%|██▌       | 54999/210244 [11:55<31:19, 82.58batch/s]2025-04-02 19:47:31,167 - INFO - Epoch 1 (Step 055000): Train loss 4.582, Val loss 4.838\n",
      "Epoch 1:  27%|██▋       | 55996/210244 [12:09<1:11:42, 35.85batch/s]2025-04-02 19:47:45,049 - INFO - Epoch 1 (Step 056000): Train loss 4.525, Val loss 4.845\n",
      "Epoch 1:  27%|██▋       | 56084/210244 [12:10<33:19, 77.10batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:47:46,119] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  27%|██▋       | 56996/210244 [12:21<31:16, 81.69batch/s]2025-04-02 19:47:57,500 - INFO - Epoch 1 (Step 057000): Train loss 4.449, Val loss 4.834\n",
      "Epoch 1:  28%|██▊       | 57998/210244 [12:34<31:50, 79.69batch/s]2025-04-02 19:48:09,947 - INFO - Epoch 1 (Step 058000): Train loss 4.551, Val loss 4.823\n",
      "Epoch 1:  28%|██▊       | 58084/210244 [12:35<31:54, 79.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:48:11,061] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  28%|██▊       | 58996/210244 [12:48<31:18, 80.50batch/s]  2025-04-02 19:48:23,770 - INFO - Epoch 1 (Step 059000): Train loss 4.794, Val loss 4.815\n",
      "Epoch 1:  29%|██▊       | 59997/210244 [13:00<30:20, 82.53batch/s]2025-04-02 19:48:36,163 - INFO - Epoch 1 (Step 060000): Train loss 4.634, Val loss 4.814\n",
      "Epoch 1:  29%|██▊       | 60086/210244 [13:01<30:36, 81.76batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:48:37,272] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  29%|██▉       | 60999/210244 [13:12<30:39, 81.12batch/s]2025-04-02 19:48:48,576 - INFO - Epoch 1 (Step 061000): Train loss 4.666, Val loss 4.815\n",
      "Epoch 1:  29%|██▉       | 61992/210244 [13:26<30:22, 81.35batch/s]  2025-04-02 19:49:02,327 - INFO - Epoch 1 (Step 062000): Train loss 4.376, Val loss 4.779\n",
      "Epoch 1:  30%|██▉       | 62092/210244 [13:27<31:02, 79.56batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:49:03,514] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  30%|██▉       | 62999/210244 [13:39<30:11, 81.29batch/s]2025-04-02 19:49:14,784 - INFO - Epoch 1 (Step 063000): Train loss 4.773, Val loss 4.751\n",
      "Epoch 1:  30%|███       | 64000/210244 [13:52<29:38, 82.22batch/s]  2025-04-02 19:49:28,519 - INFO - Epoch 1 (Step 064000): Train loss 4.492, Val loss 4.771\n",
      "Epoch 1:  30%|███       | 64090/210244 [13:53<30:08, 80.80batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:49:29,686] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  31%|███       | 64993/210244 [14:05<29:14, 82.81batch/s]2025-04-02 19:49:40,944 - INFO - Epoch 1 (Step 065000): Train loss 4.632, Val loss 4.757\n",
      "Epoch 1:  31%|███▏      | 65999/210244 [14:17<29:15, 82.17batch/s]2025-04-02 19:49:53,460 - INFO - Epoch 1 (Step 066000): Train loss 4.340, Val loss 4.775\n",
      "Epoch 1:  31%|███▏      | 66097/210244 [14:19<30:04, 79.88batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:49:54,669] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  32%|███▏      | 66998/210244 [14:31<30:08, 79.22batch/s]  2025-04-02 19:50:07,306 - INFO - Epoch 1 (Step 067000): Train loss 4.672, Val loss 4.786\n",
      "Epoch 1:  32%|███▏      | 67997/210244 [14:44<29:15, 81.01batch/s]2025-04-02 19:50:19,828 - INFO - Epoch 1 (Step 068000): Train loss 4.439, Val loss 4.800\n",
      "Epoch 1:  32%|███▏      | 68091/210244 [14:45<29:31, 80.24batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:50:21,066] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 68993/210244 [14:57<42:13, 55.74batch/s]  2025-04-02 19:50:33,600 - INFO - Epoch 1 (Step 069000): Train loss 4.607, Val loss 4.786\n",
      "Epoch 1:  33%|███▎      | 69994/210244 [15:10<28:44, 81.33batch/s]2025-04-02 19:50:46,100 - INFO - Epoch 1 (Step 070000): Train loss 4.379, Val loss 4.755\n",
      "Epoch 1:  33%|███▎      | 70100/210244 [15:11<29:41, 78.66batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:50:47,372] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  34%|███▍      | 71000/210244 [15:22<28:39, 81.00batch/s]2025-04-02 19:50:58,604 - INFO - Epoch 1 (Step 071000): Train loss 4.334, Val loss 4.774\n",
      "Epoch 1:  34%|███▍      | 71992/210244 [15:36<29:01, 79.38batch/s]  2025-04-02 19:51:12,352 - INFO - Epoch 1 (Step 072000): Train loss 4.508, Val loss 4.750\n",
      "Epoch 1:  34%|███▍      | 72095/210244 [15:37<29:05, 79.16batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:51:13,653] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  35%|███▍      | 72995/210244 [15:48<27:36, 82.87batch/s]2025-04-02 19:51:24,711 - INFO - Epoch 1 (Step 073000): Train loss 4.604, Val loss 4.755\n",
      "Epoch 1:  35%|███▌      | 73995/210244 [16:01<28:33, 79.52batch/s]2025-04-02 19:51:37,183 - INFO - Epoch 1 (Step 074000): Train loss 4.479, Val loss 4.765\n",
      "Epoch 1:  35%|███▌      | 74102/210244 [16:02<28:01, 80.94batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:51:38,468] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  36%|███▌      | 74998/210244 [16:15<27:25, 82.18batch/s]  2025-04-02 19:51:50,905 - INFO - Epoch 1 (Step 075000): Train loss 4.529, Val loss 4.734\n",
      "Epoch 1:  36%|███▌      | 75993/210244 [16:27<27:52, 80.27batch/s]2025-04-02 19:52:03,420 - INFO - Epoch 1 (Step 076000): Train loss 4.489, Val loss 4.728\n",
      "Epoch 1:  36%|███▌      | 76099/210244 [16:29<27:57, 79.99batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:52:04,749] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  37%|███▋      | 76996/210244 [16:41<27:38, 80.35batch/s]  2025-04-02 19:52:17,224 - INFO - Epoch 1 (Step 077000): Train loss 4.462, Val loss 4.748\n",
      "Epoch 1:  37%|███▋      | 77994/210244 [16:53<27:13, 80.97batch/s]2025-04-02 19:52:29,686 - INFO - Epoch 1 (Step 078000): Train loss 4.478, Val loss 4.719\n",
      "Epoch 1:  37%|███▋      | 78102/210244 [16:55<28:09, 78.20batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:52:31,082] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|███▊      | 78992/210244 [17:06<27:04, 80.81batch/s]2025-04-02 19:52:42,152 - INFO - Epoch 1 (Step 079000): Train loss 4.357, Val loss 4.732\n",
      "Epoch 1:  38%|███▊      | 79996/210244 [17:20<26:59, 80.41batch/s]  2025-04-02 19:52:55,939 - INFO - Epoch 1 (Step 080000): Train loss 4.399, Val loss 4.722\n",
      "Epoch 1:  38%|███▊      | 80111/210244 [17:21<27:06, 80.00batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:52:57,329] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  39%|███▊      | 80997/210244 [17:32<26:27, 81.43batch/s]2025-04-02 19:53:08,311 - INFO - Epoch 1 (Step 081000): Train loss 4.521, Val loss 4.719\n",
      "Epoch 1:  39%|███▉      | 81997/210244 [17:46<32:13, 66.32batch/s]  2025-04-02 19:53:22,032 - INFO - Epoch 1 (Step 082000): Train loss 4.346, Val loss 4.695\n",
      "Epoch 1:  39%|███▉      | 82113/210244 [17:47<26:05, 81.85batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:53:23,415] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  39%|███▉      | 82995/210244 [17:58<25:50, 82.07batch/s]2025-04-02 19:53:34,301 - INFO - Epoch 1 (Step 083000): Train loss 4.348, Val loss 4.694\n",
      "Epoch 1:  40%|███▉      | 83995/210244 [18:10<25:37, 82.13batch/s]2025-04-02 19:53:46,726 - INFO - Epoch 1 (Step 084000): Train loss 4.499, Val loss 4.695\n",
      "Epoch 1:  40%|████      | 84112/210244 [18:12<26:33, 79.17batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:53:48,139] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  40%|████      | 84999/210244 [18:24<25:18, 82.47batch/s]  2025-04-02 19:54:00,569 - INFO - Epoch 1 (Step 085000): Train loss 4.256, Val loss 4.664\n",
      "Epoch 1:  41%|████      | 85995/210244 [18:37<25:09, 82.32batch/s]2025-04-02 19:54:13,031 - INFO - Epoch 1 (Step 086000): Train loss 4.450, Val loss 4.653\n",
      "Epoch 1:  41%|████      | 86112/210244 [18:38<25:56, 79.73batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:54:14,476] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  41%|████▏     | 86995/210244 [18:49<25:16, 81.25batch/s]2025-04-02 19:54:25,455 - INFO - Epoch 1 (Step 087000): Train loss 4.291, Val loss 4.661\n",
      "Epoch 1:  42%|████▏     | 87995/210244 [19:03<24:54, 81.77batch/s]  2025-04-02 19:54:39,206 - INFO - Epoch 1 (Step 088000): Train loss 4.397, Val loss 4.657\n",
      "Epoch 1:  42%|████▏     | 88117/210244 [19:05<25:20, 80.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:54:40,691] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  42%|████▏     | 88994/210244 [19:15<25:01, 80.73batch/s]2025-04-02 19:54:51,571 - INFO - Epoch 1 (Step 089000): Train loss 4.351, Val loss 4.645\n",
      "Epoch 1:  43%|████▎     | 89998/210244 [19:29<25:18, 79.19batch/s]  2025-04-02 19:55:05,316 - INFO - Epoch 1 (Step 090000): Train loss 4.314, Val loss 4.642\n",
      "Epoch 1:  43%|████▎     | 90113/210244 [19:31<24:51, 80.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:55:06,824] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  43%|████▎     | 90996/210244 [19:42<23:37, 84.12batch/s]2025-04-02 19:55:17,756 - INFO - Epoch 1 (Step 091000): Train loss 4.346, Val loss 4.638\n",
      "Epoch 1:  44%|████▍     | 91996/210244 [19:54<24:37, 80.01batch/s]2025-04-02 19:55:30,210 - INFO - Epoch 1 (Step 092000): Train loss 4.276, Val loss 4.634\n",
      "Epoch 1:  44%|████▍     | 92120/210244 [19:56<25:35, 76.92batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:55:31,753] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  44%|████▍     | 92994/210244 [20:08<24:03, 81.21batch/s]  2025-04-02 19:55:44,050 - INFO - Epoch 1 (Step 093000): Train loss 4.628, Val loss 4.636\n",
      "Epoch 1:  45%|████▍     | 93995/210244 [20:20<23:44, 81.58batch/s]2025-04-02 19:55:56,492 - INFO - Epoch 1 (Step 094000): Train loss 4.392, Val loss 4.630\n",
      "Epoch 1:  45%|████▍     | 94119/210244 [20:22<24:03, 80.44batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:55:58,046] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  45%|████▌     | 94993/210244 [20:34<26:46, 71.75batch/s]  2025-04-02 19:56:10,260 - INFO - Epoch 1 (Step 095000): Train loss 4.410, Val loss 4.621\n",
      "Epoch 1:  46%|████▌     | 95994/210244 [20:46<23:36, 80.67batch/s]2025-04-02 19:56:22,721 - INFO - Epoch 1 (Step 096000): Train loss 4.362, Val loss 4.629\n",
      "Epoch 1:  46%|████▌     | 96125/210244 [20:48<23:11, 82.02batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:56:24,307] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  46%|████▌     | 96993/210244 [20:59<24:20, 77.53batch/s]2025-04-02 19:56:35,220 - INFO - Epoch 1 (Step 097000): Train loss 4.219, Val loss 4.591\n",
      "Epoch 1:  47%|████▋     | 97994/210244 [21:13<23:03, 81.11batch/s]  2025-04-02 19:56:49,047 - INFO - Epoch 1 (Step 098000): Train loss 4.196, Val loss 4.617\n",
      "Epoch 1:  47%|████▋     | 98125/210244 [21:14<22:46, 82.07batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:56:50,677] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  47%|████▋     | 98998/210244 [21:25<23:00, 80.61batch/s]2025-04-02 19:57:01,571 - INFO - Epoch 1 (Step 099000): Train loss 4.350, Val loss 4.574\n",
      "Epoch 1:  48%|████▊     | 100000/210244 [21:38<23:00, 79.85batch/s]2025-04-02 19:57:14,017 - INFO - Epoch 1 (Step 100000): Train loss 4.281, Val loss 4.594\n",
      "Epoch 1:  48%|████▊     | 100123/210244 [21:41<43:23, 42.30batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:57:16,984] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  48%|████▊     | 100993/210244 [21:51<22:32, 80.80batch/s]2025-04-02 19:57:27,773 - INFO - Epoch 1 (Step 101000): Train loss 4.573, Val loss 4.610\n",
      "Epoch 1:  49%|████▊     | 101992/210244 [22:04<22:14, 81.10batch/s]2025-04-02 19:57:40,194 - INFO - Epoch 1 (Step 102000): Train loss 4.372, Val loss 4.605\n",
      "Epoch 1:  49%|████▊     | 102126/210244 [22:06<22:31, 80.00batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:57:41,838] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  49%|████▉     | 102996/210244 [22:18<22:15, 80.28batch/s]  2025-04-02 19:57:53,962 - INFO - Epoch 1 (Step 103000): Train loss 4.291, Val loss 4.614\n",
      "Epoch 1:  49%|████▉     | 103996/210244 [22:30<22:20, 79.25batch/s]2025-04-02 19:58:06,500 - INFO - Epoch 1 (Step 104000): Train loss 4.492, Val loss 4.627\n",
      "Epoch 1:  50%|████▉     | 104128/210244 [22:32<22:06, 79.98batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:58:08,186] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|████▉     | 104995/210244 [22:43<21:37, 81.11batch/s]2025-04-02 19:58:18,997 - INFO - Epoch 1 (Step 105000): Train loss 4.350, Val loss 4.607\n",
      "Epoch 1:  50%|█████     | 105998/210244 [22:57<21:16, 81.68batch/s]  2025-04-02 19:58:32,827 - INFO - Epoch 1 (Step 106000): Train loss 4.377, Val loss 4.627\n",
      "Epoch 1:  50%|█████     | 106131/210244 [22:58<21:31, 80.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:58:34,529] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  51%|█████     | 107000/210244 [23:09<21:14, 81.00batch/s]2025-04-02 19:58:45,298 - INFO - Epoch 1 (Step 107000): Train loss 4.217, Val loss 4.609\n",
      "Epoch 1:  51%|█████▏    | 107996/210244 [23:23<21:18, 79.97batch/s]  2025-04-02 19:58:59,093 - INFO - Epoch 1 (Step 108000): Train loss 4.256, Val loss 4.615\n",
      "Epoch 1:  51%|█████▏    | 108139/210244 [23:25<21:06, 80.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:59:00,804] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  52%|█████▏    | 108992/210244 [23:35<20:27, 82.47batch/s]2025-04-02 19:59:11,525 - INFO - Epoch 1 (Step 109000): Train loss 4.303, Val loss 4.590\n",
      "Epoch 1:  52%|█████▏    | 109998/210244 [23:48<20:54, 79.91batch/s]2025-04-02 19:59:23,894 - INFO - Epoch 1 (Step 110000): Train loss 4.288, Val loss 4.595\n",
      "Epoch 1:  52%|█████▏    | 110133/210244 [23:49<20:42, 80.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:59:25,624] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  53%|█████▎    | 110998/210244 [24:01<20:01, 82.61batch/s]  2025-04-02 19:59:37,677 - INFO - Epoch 1 (Step 111000): Train loss 4.305, Val loss 4.596\n",
      "Epoch 1:  53%|█████▎    | 111999/210244 [24:14<20:10, 81.14batch/s]2025-04-02 19:59:50,180 - INFO - Epoch 1 (Step 112000): Train loss 4.257, Val loss 4.573\n",
      "Epoch 1:  53%|█████▎    | 112140/210244 [24:16<20:23, 80.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 19:59:51,949] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  54%|█████▎    | 112997/210244 [24:26<19:58, 81.12batch/s]2025-04-02 20:00:02,559 - INFO - Epoch 1 (Step 113000): Train loss 4.343, Val loss 4.576\n",
      "Epoch 1:  54%|█████▍    | 113481/210244 [24:34<20:02, 80.44batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:00:09,860] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  54%|█████▍    | 113999/210244 [24:40<19:31, 82.17batch/s]2025-04-02 20:00:16,347 - INFO - Epoch 1 (Step 114000): Train loss 4.192, Val loss 4.557\n",
      "Epoch 1:  55%|█████▍    | 114997/210244 [24:53<19:18, 82.21batch/s]2025-04-02 20:00:28,899 - INFO - Epoch 1 (Step 115000): Train loss 4.209, Val loss 4.595\n",
      "Epoch 1:  55%|█████▌    | 115999/210244 [25:06<19:32, 80.37batch/s]  2025-04-02 20:00:42,704 - INFO - Epoch 1 (Step 116000): Train loss 4.367, Val loss 4.579\n",
      "Epoch 1:  56%|█████▌    | 116992/210244 [25:19<19:17, 80.54batch/s]2025-04-02 20:00:55,257 - INFO - Epoch 1 (Step 117000): Train loss 4.347, Val loss 4.560\n",
      "Epoch 1:  56%|█████▌    | 117485/210244 [25:25<18:52, 81.88batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:01:01,211] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  56%|█████▌    | 117999/210244 [25:31<18:45, 81.93batch/s]2025-04-02 20:01:07,679 - INFO - Epoch 1 (Step 118000): Train loss 4.202, Val loss 4.559\n",
      "Epoch 1:  57%|█████▋    | 118998/210244 [25:45<18:48, 80.86batch/s]  2025-04-02 20:01:21,470 - INFO - Epoch 1 (Step 119000): Train loss 4.312, Val loss 4.527\n",
      "Epoch 1:  57%|█████▋    | 119485/210244 [25:51<18:31, 81.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:01:27,502] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  57%|█████▋    | 119994/210244 [25:58<19:02, 79.00batch/s]2025-04-02 20:01:33,912 - INFO - Epoch 1 (Step 120000): Train loss 4.417, Val loss 4.554\n",
      "Epoch 1:  58%|█████▊    | 120994/210244 [26:12<18:39, 79.73batch/s]  2025-04-02 20:01:47,828 - INFO - Epoch 1 (Step 121000): Train loss 4.340, Val loss 4.549\n",
      "Epoch 1:  58%|█████▊    | 121485/210244 [26:18<18:29, 80.01batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:01:53,959] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  58%|█████▊    | 121994/210244 [26:24<18:19, 80.26batch/s]2025-04-02 20:02:00,426 - INFO - Epoch 1 (Step 122000): Train loss 4.344, Val loss 4.544\n",
      "Epoch 1:  59%|█████▊    | 122997/210244 [26:37<17:53, 81.24batch/s]2025-04-02 20:02:12,975 - INFO - Epoch 1 (Step 123000): Train loss 4.234, Val loss 4.547\n",
      "Epoch 1:  59%|█████▊    | 123485/210244 [26:44<23:22, 61.86batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:02:20,392] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  59%|█████▉    | 123999/210244 [26:51<17:37, 81.57batch/s]2025-04-02 20:02:26,808 - INFO - Epoch 1 (Step 124000): Train loss 4.125, Val loss 4.556\n",
      "Epoch 1:  59%|█████▉    | 124994/210244 [27:03<17:53, 79.42batch/s]2025-04-02 20:02:39,321 - INFO - Epoch 1 (Step 125000): Train loss 4.280, Val loss 4.530\n",
      "Epoch 1:  60%|█████▉    | 125490/210244 [27:09<17:57, 78.65batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:02:45,430] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  60%|█████▉    | 125891/210244 [27:14<17:12, 81.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:02:50,381] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  60%|█████▉    | 125999/210244 [27:16<17:23, 80.74batch/s]2025-04-02 20:02:53,117 - INFO - Epoch 1 (Step 126000): Train loss 4.283, Val loss 4.529\n",
      "Epoch 1:  60%|██████    | 126995/210244 [27:29<17:09, 80.84batch/s]  2025-04-02 20:03:05,572 - INFO - Epoch 1 (Step 127000): Train loss 4.380, Val loss 4.510\n",
      "Epoch 1:  61%|██████    | 127999/210244 [27:42<17:23, 78.80batch/s]2025-04-02 20:03:18,093 - INFO - Epoch 1 (Step 128000): Train loss 4.239, Val loss 4.516\n",
      "Epoch 1:  61%|██████    | 128305/210244 [27:46<16:58, 80.42batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:03:21,961] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  61%|██████▏   | 128996/210244 [27:56<17:05, 79.23batch/s]  2025-04-02 20:03:31,901 - INFO - Epoch 1 (Step 129000): Train loss 4.342, Val loss 4.525\n",
      "Epoch 1:  62%|██████▏   | 129998/210244 [28:08<16:42, 80.08batch/s]2025-04-02 20:03:44,340 - INFO - Epoch 1 (Step 130000): Train loss 4.110, Val loss 4.502\n",
      "Epoch 1:  62%|██████▏   | 130994/210244 [28:21<16:05, 82.10batch/s]2025-04-02 20:03:56,798 - INFO - Epoch 1 (Step 131000): Train loss 4.219, Val loss 4.517\n",
      "Epoch 1:  62%|██████▏   | 131353/210244 [28:26<16:05, 81.70batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:04:02,565] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  63%|██████▎   | 131996/210244 [28:34<16:31, 78.93batch/s]2025-04-02 20:04:10,577 - INFO - Epoch 1 (Step 132000): Train loss 4.325, Val loss 4.531\n",
      "Epoch 1:  63%|██████▎   | 132998/210244 [28:47<16:24, 78.46batch/s]2025-04-02 20:04:23,074 - INFO - Epoch 1 (Step 133000): Train loss 4.126, Val loss 4.513\n",
      "Epoch 1:  63%|██████▎   | 133381/210244 [28:52<15:45, 81.27batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:04:27,887] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  64%|██████▎   | 133998/210244 [29:01<15:43, 80.80batch/s]  2025-04-02 20:04:36,867 - INFO - Epoch 1 (Step 134000): Train loss 4.285, Val loss 4.522\n",
      "Epoch 1:  64%|██████▍   | 134998/210244 [29:13<15:36, 80.38batch/s]2025-04-02 20:04:49,351 - INFO - Epoch 1 (Step 135000): Train loss 4.103, Val loss 4.530\n",
      "Epoch 1:  64%|██████▍   | 135505/210244 [29:19<15:03, 82.70batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:04:55,598] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  65%|██████▍   | 135994/210244 [29:26<16:07, 76.78batch/s]2025-04-02 20:05:01,816 - INFO - Epoch 1 (Step 136000): Train loss 4.263, Val loss 4.508\n",
      "Epoch 1:  65%|██████▌   | 136994/210244 [29:39<15:12, 80.28batch/s]  2025-04-02 20:05:15,640 - INFO - Epoch 1 (Step 137000): Train loss 4.292, Val loss 4.542\n",
      "Epoch 1:  65%|██████▌   | 137679/210244 [29:48<14:52, 81.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:05:24,183] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  66%|██████▌   | 137994/210244 [29:52<14:53, 80.91batch/s]2025-04-02 20:05:28,086 - INFO - Epoch 1 (Step 138000): Train loss 4.199, Val loss 4.551\n",
      "Epoch 1:  66%|██████▌   | 139000/210244 [30:06<20:04, 59.16batch/s]  2025-04-02 20:05:41,965 - INFO - Epoch 1 (Step 139000): Train loss 4.221, Val loss 4.522\n",
      "Epoch 1:  67%|██████▋   | 139995/210244 [30:18<14:17, 81.89batch/s]2025-04-02 20:05:54,386 - INFO - Epoch 1 (Step 140000): Train loss 4.259, Val loss 4.525\n",
      "Epoch 1:  67%|██████▋   | 140336/210244 [30:22<14:35, 79.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:05:58,574] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████▋   | 140999/210244 [30:31<14:06, 81.83batch/s]2025-04-02 20:06:06,846 - INFO - Epoch 1 (Step 141000): Train loss 3.837, Val loss 4.480\n",
      "Epoch 1:  68%|██████▊   | 141995/210244 [30:44<13:52, 81.96batch/s]  2025-04-02 20:06:20,536 - INFO - Epoch 1 (Step 142000): Train loss 4.263, Val loss 4.517\n",
      "Epoch 1:  68%|██████▊   | 142872/210244 [30:55<13:52, 80.96batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:06:31,420] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  68%|██████▊   | 142994/210244 [30:57<14:24, 77.80batch/s]2025-04-02 20:06:33,080 - INFO - Epoch 1 (Step 143000): Train loss 4.312, Val loss 4.532\n",
      "Epoch 1:  68%|██████▊   | 143992/210244 [31:09<13:45, 80.30batch/s]2025-04-02 20:06:45,614 - INFO - Epoch 1 (Step 144000): Train loss 4.257, Val loss 4.493\n",
      "Epoch 1:  69%|██████▉   | 144981/210244 [31:23<14:06, 77.10batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:06:59,184] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  69%|██████▉   | 144997/210244 [31:23<14:11, 76.63batch/s]2025-04-02 20:06:59,494 - INFO - Epoch 1 (Step 145000): Train loss 4.413, Val loss 4.495\n",
      "Epoch 1:  69%|██████▉   | 145992/210244 [31:36<13:09, 81.34batch/s]2025-04-02 20:07:11,910 - INFO - Epoch 1 (Step 146000): Train loss 4.238, Val loss 4.490\n",
      "Epoch 1:  70%|██████▉   | 146359/210244 [31:40<13:06, 81.23batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:07:16,366] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  70%|██████▉   | 146992/210244 [31:49<13:24, 78.66batch/s]  2025-04-02 20:07:25,674 - INFO - Epoch 1 (Step 147000): Train loss 4.103, Val loss 4.485\n",
      "Epoch 1:  70%|███████   | 147992/210244 [32:02<13:03, 79.48batch/s]2025-04-02 20:07:38,096 - INFO - Epoch 1 (Step 148000): Train loss 4.127, Val loss 4.504\n",
      "Epoch 1:  71%|███████   | 148520/210244 [32:08<12:31, 82.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:07:44,461] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  71%|███████   | 148992/210244 [32:14<13:00, 78.51batch/s]2025-04-02 20:07:50,462 - INFO - Epoch 1 (Step 149000): Train loss 4.260, Val loss 4.473\n",
      "Epoch 1:  71%|███████▏  | 149992/210244 [32:28<12:24, 80.95batch/s]2025-04-02 20:08:04,448 - INFO - Epoch 1 (Step 150000): Train loss 4.292, Val loss 4.487\n",
      "Epoch 1:  72%|███████▏  | 150983/210244 [32:41<13:09, 75.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:08:16,804] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  72%|███████▏  | 150992/210244 [32:41<12:41, 77.78batch/s]2025-04-02 20:08:16,971 - INFO - Epoch 1 (Step 151000): Train loss 4.385, Val loss 4.515\n",
      "Epoch 1:  72%|███████▏  | 152000/210244 [32:55<12:26, 78.06batch/s]2025-04-02 20:08:30,817 - INFO - Epoch 1 (Step 152000): Train loss 4.270, Val loss 4.503\n",
      "Epoch 1:  73%|███████▎  | 152993/210244 [33:07<11:51, 80.42batch/s]2025-04-02 20:08:43,310 - INFO - Epoch 1 (Step 153000): Train loss 4.166, Val loss 4.487\n",
      "Epoch 1:  73%|███████▎  | 153997/210244 [33:20<11:34, 81.00batch/s]2025-04-02 20:08:55,778 - INFO - Epoch 1 (Step 154000): Train loss 4.123, Val loss 4.491\n",
      "Epoch 1:  74%|███████▎  | 154997/210244 [33:33<11:03, 83.24batch/s]2025-04-02 20:09:09,603 - INFO - Epoch 1 (Step 155000): Train loss 4.220, Val loss 4.491\n",
      "Epoch 1:  74%|███████▍  | 155078/210244 [33:34<11:49, 77.80batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:09:10,653] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  74%|███████▍  | 156000/210244 [33:46<11:33, 78.17batch/s]2025-04-02 20:09:22,230 - INFO - Epoch 1 (Step 156000): Train loss 4.143, Val loss 4.486\n",
      "Epoch 1:  74%|███████▍  | 156142/210244 [33:48<11:21, 79.43batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:09:23,993] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  75%|███████▍  | 156996/210244 [33:58<11:01, 80.55batch/s]2025-04-02 20:09:34,590 - INFO - Epoch 1 (Step 157000): Train loss 4.221, Val loss 4.498\n",
      "Epoch 1:  75%|███████▌  | 157995/210244 [34:12<10:41, 81.42batch/s]2025-04-02 20:09:48,371 - INFO - Epoch 1 (Step 158000): Train loss 4.344, Val loss 4.476\n",
      "Epoch 1:  75%|███████▌  | 158724/210244 [34:21<10:30, 81.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:09:57,194] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  76%|███████▌  | 158994/210244 [34:24<10:33, 80.89batch/s]2025-04-02 20:10:00,641 - INFO - Epoch 1 (Step 159000): Train loss 4.057, Val loss 4.485\n",
      "Epoch 1:  76%|███████▌  | 159992/210244 [34:38<10:16, 81.45batch/s]2025-04-02 20:10:14,357 - INFO - Epoch 1 (Step 160000): Train loss 4.247, Val loss 4.454\n",
      "Epoch 1:  77%|███████▋  | 160995/210244 [34:50<09:57, 82.38batch/s]2025-04-02 20:10:26,703 - INFO - Epoch 1 (Step 161000): Train loss 4.317, Val loss 4.481\n",
      "Epoch 1:  77%|███████▋  | 161021/210244 [34:51<11:00, 74.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:10:26,969] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  77%|███████▋  | 161998/210244 [35:03<10:15, 78.36batch/s]2025-04-02 20:10:39,137 - INFO - Epoch 1 (Step 162000): Train loss 4.191, Val loss 4.476\n",
      "Epoch 1:  78%|███████▊  | 162995/210244 [35:17<09:40, 81.45batch/s]2025-04-02 20:10:53,034 - INFO - Epoch 1 (Step 163000): Train loss 4.144, Val loss 4.490\n",
      "Epoch 1:  78%|███████▊  | 164000/210244 [35:29<10:00, 76.95batch/s]2025-04-02 20:11:05,572 - INFO - Epoch 1 (Step 164000): Train loss 4.282, Val loss 4.493\n",
      "Epoch 1:  78%|███████▊  | 164197/210244 [35:32<09:19, 82.36batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:11:08,032] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  78%|███████▊  | 164994/210244 [35:43<09:38, 78.16batch/s]2025-04-02 20:11:19,255 - INFO - Epoch 1 (Step 165000): Train loss 4.175, Val loss 4.497\n",
      "Epoch 1:  79%|███████▉  | 165996/210244 [35:56<09:15, 79.67batch/s]2025-04-02 20:11:31,963 - INFO - Epoch 1 (Step 166000): Train loss 4.234, Val loss 4.481\n",
      "Epoch 1:  79%|███████▉  | 166253/210244 [35:59<08:52, 82.54batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:11:35,158] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  79%|███████▉  | 166999/210244 [36:08<09:15, 77.82batch/s]2025-04-02 20:11:44,457 - INFO - Epoch 1 (Step 167000): Train loss 4.142, Val loss 4.456\n",
      "Epoch 1:  80%|███████▉  | 167993/210244 [36:22<08:32, 82.50batch/s]2025-04-02 20:11:58,196 - INFO - Epoch 1 (Step 168000): Train loss 4.095, Val loss 4.463\n",
      "Epoch 1:  80%|████████  | 169000/210244 [36:34<08:28, 81.16batch/s]2025-04-02 20:12:10,546 - INFO - Epoch 1 (Step 169000): Train loss 4.263, Val loss 4.461\n",
      "Epoch 1:  80%|████████  | 169045/210244 [36:35<08:40, 79.14batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:12:11,096] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  81%|████████  | 169997/210244 [36:47<08:11, 81.96batch/s]2025-04-02 20:12:23,151 - INFO - Epoch 1 (Step 170000): Train loss 4.096, Val loss 4.446\n",
      "Epoch 1:  81%|████████▏ | 170996/210244 [37:01<08:05, 80.81batch/s]2025-04-02 20:12:37,000 - INFO - Epoch 1 (Step 171000): Train loss 4.281, Val loss 4.463\n",
      "Epoch 1:  82%|████████▏ | 171530/210244 [37:07<08:01, 80.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:12:43,594] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  82%|████████▏ | 171997/210244 [37:13<07:50, 81.26batch/s]2025-04-02 20:12:49,422 - INFO - Epoch 1 (Step 172000): Train loss 4.245, Val loss 4.477\n",
      "Epoch 1:  82%|████████▏ | 173000/210244 [37:27<07:41, 80.73batch/s]2025-04-02 20:13:03,359 - INFO - Epoch 1 (Step 173000): Train loss 4.140, Val loss 4.459\n",
      "Epoch 1:  83%|████████▎ | 173996/210244 [37:40<07:24, 81.46batch/s]2025-04-02 20:13:15,761 - INFO - Epoch 1 (Step 174000): Train loss 4.225, Val loss 4.441\n",
      "Epoch 1:  83%|████████▎ | 174749/210244 [37:49<07:10, 82.43batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:13:25,012] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 174992/210244 [37:52<07:15, 80.95batch/s]2025-04-02 20:13:28,142 - INFO - Epoch 1 (Step 175000): Train loss 4.225, Val loss 4.440\n",
      "Epoch 1:  84%|████████▎ | 175996/210244 [38:06<07:08, 79.97batch/s]2025-04-02 20:13:41,970 - INFO - Epoch 1 (Step 176000): Train loss 4.300, Val loss 4.456\n",
      "Epoch 1:  84%|████████▍ | 176999/210244 [38:18<06:59, 79.18batch/s]2025-04-02 20:13:54,477 - INFO - Epoch 1 (Step 177000): Train loss 4.186, Val loss 4.441\n",
      "Epoch 1:  84%|████████▍ | 177515/210244 [38:25<06:50, 79.76batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:14:01,085] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  85%|████████▍ | 177999/210244 [38:32<06:35, 81.48batch/s]2025-04-02 20:14:08,368 - INFO - Epoch 1 (Step 178000): Train loss 4.323, Val loss 4.413\n",
      "Epoch 1:  85%|████████▌ | 178994/210244 [38:44<06:26, 80.75batch/s]2025-04-02 20:14:20,771 - INFO - Epoch 1 (Step 179000): Train loss 4.092, Val loss 4.450\n",
      "Epoch 1:  85%|████████▌ | 179612/210244 [38:52<06:18, 80.84batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:14:28,377] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  86%|████████▌ | 179996/210244 [38:57<06:16, 80.37batch/s]2025-04-02 20:14:33,240 - INFO - Epoch 1 (Step 180000): Train loss 4.145, Val loss 4.437\n",
      "Epoch 1:  86%|████████▌ | 180992/210244 [39:11<06:00, 81.16batch/s]2025-04-02 20:14:47,082 - INFO - Epoch 1 (Step 181000): Train loss 4.102, Val loss 4.442\n",
      "Epoch 1:  86%|████████▋ | 181667/210244 [39:19<05:53, 80.78batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:14:55,359] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  87%|████████▋ | 181999/210244 [39:23<05:49, 80.84batch/s]2025-04-02 20:14:59,540 - INFO - Epoch 1 (Step 182000): Train loss 4.214, Val loss 4.463\n",
      "Epoch 1:  87%|████████▋ | 182994/210244 [39:37<16:34, 27.39batch/s]2025-04-02 20:15:13,360 - INFO - Epoch 1 (Step 183000): Train loss 3.946, Val loss 4.433\n",
      "Epoch 1:  87%|████████▋ | 183156/210244 [39:39<05:31, 81.76batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:15:15,346] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  88%|████████▊ | 183995/210244 [39:50<05:23, 81.13batch/s]2025-04-02 20:15:25,814 - INFO - Epoch 1 (Step 184000): Train loss 4.204, Val loss 4.476\n",
      "Epoch 1:  88%|████████▊ | 184999/210244 [40:02<05:08, 81.91batch/s]2025-04-02 20:15:38,288 - INFO - Epoch 1 (Step 185000): Train loss 4.144, Val loss 4.448\n",
      "Epoch 1:  88%|████████▊ | 186000/210244 [40:16<05:01, 80.53batch/s]2025-04-02 20:15:52,106 - INFO - Epoch 1 (Step 186000): Train loss 4.035, Val loss 4.482\n",
      "Epoch 1:  89%|████████▉ | 186997/210244 [40:28<04:40, 82.87batch/s]2025-04-02 20:16:04,613 - INFO - Epoch 1 (Step 187000): Train loss 4.159, Val loss 4.473\n",
      "Epoch 1:  89%|████████▉ | 187203/210244 [40:31<04:41, 81.82batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:16:07,192] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  89%|████████▉ | 187996/210244 [40:41<04:36, 80.39batch/s]2025-04-02 20:16:17,104 - INFO - Epoch 1 (Step 188000): Train loss 4.235, Val loss 4.452\n",
      "Epoch 1:  90%|████████▉ | 188999/210244 [40:55<04:24, 80.47batch/s]2025-04-02 20:16:30,976 - INFO - Epoch 1 (Step 189000): Train loss 4.231, Val loss 4.443\n",
      "Epoch 1:  90%|████████▉ | 189071/210244 [40:56<04:21, 80.83batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:16:31,921] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|█████████ | 189999/210244 [41:07<04:06, 82.03batch/s]2025-04-02 20:16:43,509 - INFO - Epoch 1 (Step 190000): Train loss 4.289, Val loss 4.421\n",
      "Epoch 1:  91%|█████████ | 190994/210244 [41:21<03:55, 81.61batch/s]2025-04-02 20:16:57,350 - INFO - Epoch 1 (Step 191000): Train loss 4.107, Val loss 4.451\n",
      "Epoch 1:  91%|█████████▏| 191995/210244 [41:34<03:40, 82.59batch/s]2025-04-02 20:17:09,820 - INFO - Epoch 1 (Step 192000): Train loss 4.054, Val loss 4.454\n",
      "Epoch 1:  92%|█████████▏| 192995/210244 [41:46<03:35, 80.01batch/s]2025-04-02 20:17:22,397 - INFO - Epoch 1 (Step 193000): Train loss 4.228, Val loss 4.429\n",
      "Epoch 1:  92%|█████████▏| 193173/210244 [41:48<03:27, 82.33batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:17:24,525] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  92%|█████████▏| 193994/210244 [42:00<03:21, 80.54batch/s]2025-04-02 20:17:36,205 - INFO - Epoch 1 (Step 194000): Train loss 4.147, Val loss 4.430\n",
      "Epoch 1:  93%|█████████▎| 194995/210244 [42:12<03:05, 82.37batch/s]2025-04-02 20:17:48,662 - INFO - Epoch 1 (Step 195000): Train loss 4.120, Val loss 4.431\n",
      "Epoch 1:  93%|█████████▎| 195273/210244 [42:16<03:02, 81.96batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:17:52,078] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  93%|█████████▎| 195994/210244 [42:26<03:08, 75.77batch/s]2025-04-02 20:18:02,432 - INFO - Epoch 1 (Step 196000): Train loss 4.123, Val loss 4.429\n",
      "Epoch 1:  94%|█████████▎| 196997/210244 [42:39<02:41, 81.85batch/s]2025-04-02 20:18:14,862 - INFO - Epoch 1 (Step 197000): Train loss 4.069, Val loss 4.424\n",
      "Epoch 1:  94%|█████████▍| 197550/210244 [42:46<02:36, 81.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:18:21,756] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  94%|█████████▍| 198000/210244 [42:51<02:31, 80.92batch/s]2025-04-02 20:18:27,263 - INFO - Epoch 1 (Step 198000): Train loss 4.153, Val loss 4.416\n",
      "Epoch 1:  95%|█████████▍| 198775/210244 [43:02<02:23, 80.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:18:38,299] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|█████████▍| 199000/210244 [43:05<02:21, 79.47batch/s]2025-04-02 20:18:41,104 - INFO - Epoch 1 (Step 199000): Train loss 4.318, Val loss 4.445\n",
      "Epoch 1:  95%|█████████▌| 199992/210244 [43:17<02:14, 76.24batch/s]2025-04-02 20:18:53,616 - INFO - Epoch 1 (Step 200000): Train loss 4.015, Val loss 4.442\n",
      "Epoch 1:  96%|█████████▌| 200999/210244 [43:30<01:52, 81.93batch/s]2025-04-02 20:19:05,959 - INFO - Epoch 1 (Step 201000): Train loss 4.083, Val loss 4.433\n",
      "Epoch 1:  96%|█████████▌| 201993/210244 [43:43<01:41, 81.32batch/s]2025-04-02 20:19:19,742 - INFO - Epoch 1 (Step 202000): Train loss 4.100, Val loss 4.436\n",
      "Epoch 1:  96%|█████████▋| 202778/210244 [43:53<01:35, 78.48batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:19:29,457] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  97%|█████████▋| 203000/210244 [43:56<01:34, 76.85batch/s]2025-04-02 20:19:32,260 - INFO - Epoch 1 (Step 203000): Train loss 4.092, Val loss 4.427\n",
      "Epoch 1:  97%|█████████▋| 203846/210244 [44:08<01:27, 72.93batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:19:44,311] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  97%|█████████▋| 203997/210244 [44:10<01:15, 82.22batch/s]2025-04-02 20:19:46,190 - INFO - Epoch 1 (Step 204000): Train loss 3.962, Val loss 4.436\n",
      "Epoch 1:  98%|█████████▊| 204998/210244 [44:23<01:05, 79.84batch/s]2025-04-02 20:19:58,753 - INFO - Epoch 1 (Step 205000): Train loss 4.196, Val loss 4.444\n",
      "Epoch 1:  98%|█████████▊| 205996/210244 [44:35<00:52, 80.44batch/s]2025-04-02 20:20:11,227 - INFO - Epoch 1 (Step 206000): Train loss 4.214, Val loss 4.437\n",
      "Epoch 1:  98%|█████████▊| 206482/210244 [44:42<00:47, 80.01batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:20:18,640] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  98%|█████████▊| 206995/210244 [44:49<00:39, 82.00batch/s]2025-04-02 20:20:25,128 - INFO - Epoch 1 (Step 207000): Train loss 4.157, Val loss 4.438\n",
      "Epoch 1:  99%|█████████▉| 207992/210244 [45:01<00:27, 80.97batch/s]2025-04-02 20:20:37,588 - INFO - Epoch 1 (Step 208000): Train loss 4.218, Val loss 4.452\n",
      "Epoch 1:  99%|█████████▉| 208530/210244 [45:08<00:21, 81.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:20:44,304] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 208998/210244 [45:15<00:15, 79.76batch/s]2025-04-02 20:20:51,471 - INFO - Epoch 1 (Step 209000): Train loss 4.140, Val loss 4.418\n",
      "Epoch 1: 100%|█████████▉| 209992/210244 [45:28<00:03, 81.53batch/s]2025-04-02 20:21:03,930 - INFO - Epoch 1 (Step 210000): Train loss 4.112, Val loss 4.425\n",
      "Epoch 1: 100%|██████████| 210244/210244 [45:31<00:00, 76.98batch/s]\n",
      "2025-04-02 20:21:06,958 - INFO - Epoch 1 completed. Generating a sample...\n",
      "2025-04-02 20:21:07,028 - INFO - Generated Text: Every effort moves you to get to the game . \" \n",
      "   = = = = = = = = = = = \n",
      "   The game was released in the United States on November 7 , 2010 , in the United States . The game was released in\n",
      "2025-04-02 20:21:07,030 - INFO - Starting Epoch 2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you to get to the game . \"     = = = = = = = = = = =     The game was released in the United States on November 7 , 2010 , in the United States . The game was released in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 755/210244 [00:09<43:05, 81.02batch/s]2025-04-02 20:21:16,538 - INFO - Epoch 2 (Step 211000): Train loss 4.095, Val loss 4.421\n",
      "Epoch 2:   1%|          | 1420/210244 [00:19<44:23, 78.40batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:21:26,311] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   1%|          | 1754/210244 [00:23<46:45, 74.33batch/s]2025-04-02 20:21:30,545 - INFO - Epoch 2 (Step 212000): Train loss 4.180, Val loss 4.399\n",
      "Epoch 2:   1%|▏         | 2752/210244 [00:35<42:57, 80.49batch/s]2025-04-02 20:21:43,150 - INFO - Epoch 2 (Step 213000): Train loss 3.975, Val loss 4.445\n",
      "Epoch 2:   2%|▏         | 3750/210244 [00:49<1:23:22, 41.28batch/s]2025-04-02 20:21:56,950 - INFO - Epoch 2 (Step 214000): Train loss 4.156, Val loss 4.442\n",
      "Epoch 2:   2%|▏         | 4752/210244 [01:02<43:04, 79.49batch/s]  2025-04-02 20:22:09,626 - INFO - Epoch 2 (Step 215000): Train loss 4.055, Val loss 4.437\n",
      "Epoch 2:   2%|▏         | 5057/210244 [01:06<42:42, 80.07batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:22:13,503] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   3%|▎         | 5748/210244 [01:14<41:05, 82.95batch/s]2025-04-02 20:22:22,103 - INFO - Epoch 2 (Step 216000): Train loss 3.959, Val loss 4.417\n",
      "Epoch 2:   3%|▎         | 6752/210244 [01:28<42:12, 80.35batch/s]  2025-04-02 20:22:35,889 - INFO - Epoch 2 (Step 217000): Train loss 4.126, Val loss 4.444\n",
      "Epoch 2:   4%|▎         | 7753/210244 [01:41<41:32, 81.23batch/s]2025-04-02 20:22:48,379 - INFO - Epoch 2 (Step 218000): Train loss 4.084, Val loss 4.409\n",
      "Epoch 2:   4%|▍         | 8238/210244 [01:47<41:53, 80.36batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:22:54,327] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   4%|▍         | 8490/210244 [01:50<41:50, 80.38batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:22:57,396] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   4%|▍         | 8750/210244 [01:53<41:29, 80.93batch/s]2025-04-02 20:23:00,694 - INFO - Epoch 2 (Step 219000): Train loss 4.027, Val loss 4.409\n",
      "Epoch 2:   5%|▍         | 9748/210244 [02:07<40:23, 82.71batch/s]  2025-04-02 20:23:14,462 - INFO - Epoch 2 (Step 220000): Train loss 4.153, Val loss 4.399\n",
      "Epoch 2:   5%|▌         | 10752/210244 [02:19<41:52, 79.38batch/s]2025-04-02 20:23:26,983 - INFO - Epoch 2 (Step 221000): Train loss 4.124, Val loss 4.412\n",
      "Epoch 2:   6%|▌         | 11752/210244 [02:33<40:48, 81.07batch/s]  2025-04-02 20:23:40,797 - INFO - Epoch 2 (Step 222000): Train loss 3.998, Val loss 4.428\n",
      "Epoch 2:   6%|▌         | 12599/210244 [02:44<40:25, 81.48batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:23:51,217] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   6%|▌         | 12751/210244 [02:46<40:29, 81.29batch/s]2025-04-02 20:23:53,206 - INFO - Epoch 2 (Step 223000): Train loss 4.074, Val loss 4.425\n",
      "Epoch 2:   7%|▋         | 13752/210244 [02:58<41:55, 78.11batch/s]2025-04-02 20:24:05,694 - INFO - Epoch 2 (Step 224000): Train loss 4.225, Val loss 4.412\n",
      "Epoch 2:   7%|▋         | 14697/210244 [03:11<40:29, 80.48batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:24:18,860] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   7%|▋         | 14751/210244 [03:12<40:06, 81.23batch/s]2025-04-02 20:24:19,602 - INFO - Epoch 2 (Step 225000): Train loss 4.201, Val loss 4.392\n",
      "Epoch 2:   7%|▋         | 15484/210244 [03:21<40:16, 80.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:24:28,699] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   7%|▋         | 15753/210244 [03:24<39:38, 81.76batch/s]2025-04-02 20:24:32,104 - INFO - Epoch 2 (Step 226000): Train loss 4.052, Val loss 4.408\n",
      "Epoch 2:   8%|▊         | 16748/210244 [03:38<51:50, 62.21batch/s]  2025-04-02 20:24:45,848 - INFO - Epoch 2 (Step 227000): Train loss 4.249, Val loss 4.424\n",
      "Epoch 2:   8%|▊         | 17755/210244 [03:51<41:29, 77.31batch/s]2025-04-02 20:24:58,361 - INFO - Epoch 2 (Step 228000): Train loss 4.283, Val loss 4.411\n",
      "Epoch 2:   9%|▊         | 18184/210244 [03:56<39:19, 81.40batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:25:03,661] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   9%|▉         | 18749/210244 [04:03<39:02, 81.74batch/s]2025-04-02 20:25:10,729 - INFO - Epoch 2 (Step 229000): Train loss 4.048, Val loss 4.408\n",
      "Epoch 2:   9%|▉         | 19750/210244 [04:17<39:14, 80.92batch/s]  2025-04-02 20:25:24,633 - INFO - Epoch 2 (Step 230000): Train loss 3.889, Val loss 4.383\n",
      "Epoch 2:  10%|▉         | 20755/210244 [04:29<38:46, 81.46batch/s]2025-04-02 20:25:37,091 - INFO - Epoch 2 (Step 231000): Train loss 3.896, Val loss 4.373\n",
      "Epoch 2:  10%|▉         | 20850/210244 [04:31<39:43, 79.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:25:38,347] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  10%|█         | 21756/210244 [04:42<38:52, 80.80batch/s]2025-04-02 20:25:49,662 - INFO - Epoch 2 (Step 232000): Train loss 4.264, Val loss 4.374\n",
      "Epoch 2:  11%|█         | 22748/210244 [04:56<38:57, 80.21batch/s]  2025-04-02 20:26:03,463 - INFO - Epoch 2 (Step 233000): Train loss 4.121, Val loss 4.360\n",
      "Epoch 2:  11%|█▏        | 23751/210244 [05:08<37:01, 83.94batch/s]2025-04-02 20:26:15,804 - INFO - Epoch 2 (Step 234000): Train loss 4.127, Val loss 4.381\n",
      "Epoch 2:  11%|█▏        | 23847/210244 [05:09<40:17, 77.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:26:17,008] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  12%|█▏        | 24755/210244 [05:22<38:54, 79.45batch/s]  2025-04-02 20:26:29,702 - INFO - Epoch 2 (Step 235000): Train loss 4.089, Val loss 4.359\n",
      "Epoch 2:  12%|█▏        | 25750/210244 [05:34<38:21, 80.17batch/s]2025-04-02 20:26:42,163 - INFO - Epoch 2 (Step 236000): Train loss 4.027, Val loss 4.391\n",
      "Epoch 2:  13%|█▎        | 26753/210244 [05:47<37:34, 81.37batch/s]2025-04-02 20:26:54,710 - INFO - Epoch 2 (Step 237000): Train loss 4.149, Val loss 4.376\n",
      "Epoch 2:  13%|█▎        | 27754/210244 [06:01<37:07, 81.92batch/s]  2025-04-02 20:27:08,508 - INFO - Epoch 2 (Step 238000): Train loss 3.954, Val loss 4.406\n",
      "Epoch 2:  13%|█▎        | 27903/210244 [06:03<37:17, 81.51batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:27:10,451] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  14%|█▎        | 28755/210244 [06:13<41:02, 73.69batch/s]2025-04-02 20:27:21,057 - INFO - Epoch 2 (Step 239000): Train loss 4.163, Val loss 4.386\n",
      "Epoch 2:  14%|█▍        | 29752/210244 [06:27<37:47, 79.61batch/s]  2025-04-02 20:27:34,882 - INFO - Epoch 2 (Step 240000): Train loss 4.123, Val loss 4.379\n",
      "Epoch 2:  15%|█▍        | 30628/210244 [06:38<36:58, 80.96batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:27:45,759] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  15%|█▍        | 30754/210244 [06:40<37:00, 80.84batch/s]2025-04-02 20:27:47,332 - INFO - Epoch 2 (Step 241000): Train loss 4.248, Val loss 4.379\n",
      "Epoch 2:  15%|█▌        | 31755/210244 [06:52<36:23, 81.73batch/s]2025-04-02 20:27:59,834 - INFO - Epoch 2 (Step 242000): Train loss 4.207, Val loss 4.386\n",
      "Epoch 2:  16%|█▌        | 32750/210244 [07:06<36:33, 80.90batch/s]  2025-04-02 20:28:13,623 - INFO - Epoch 2 (Step 243000): Train loss 4.112, Val loss 4.383\n",
      "Epoch 2:  16%|█▌        | 33089/210244 [07:10<35:37, 82.90batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:28:17,784] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  16%|█▌        | 33754/210244 [07:18<36:19, 80.99batch/s]2025-04-02 20:28:26,001 - INFO - Epoch 2 (Step 244000): Train loss 4.073, Val loss 4.393\n",
      "Epoch 2:  17%|█▋        | 34748/210244 [07:31<36:31, 80.08batch/s]2025-04-02 20:28:38,466 - INFO - Epoch 2 (Step 245000): Train loss 4.093, Val loss 4.357\n",
      "Epoch 2:  17%|█▋        | 35207/210244 [07:38<35:44, 81.61batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:28:45,532] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  17%|█▋        | 35756/210244 [07:45<36:24, 79.87batch/s]2025-04-02 20:28:52,291 - INFO - Epoch 2 (Step 246000): Train loss 4.129, Val loss 4.375\n",
      "Epoch 2:  17%|█▋        | 36751/210244 [07:57<35:04, 82.44batch/s]2025-04-02 20:29:04,707 - INFO - Epoch 2 (Step 247000): Train loss 4.070, Val loss 4.389\n",
      "Epoch 2:  18%|█▊        | 37260/210244 [08:03<35:22, 81.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:29:10,945] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  18%|█▊        | 37750/210244 [08:09<35:11, 81.70batch/s]2025-04-02 20:29:17,135 - INFO - Epoch 2 (Step 248000): Train loss 4.032, Val loss 4.380\n",
      "Epoch 2:  18%|█▊        | 37973/210244 [08:12<34:57, 82.14batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:29:19,899] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  18%|█▊        | 38753/210244 [08:24<35:51, 79.72batch/s]  2025-04-02 20:29:31,513 - INFO - Epoch 2 (Step 249000): Train loss 4.173, Val loss 4.377\n",
      "Epoch 2:  19%|█▉        | 39752/210244 [08:36<35:11, 80.75batch/s]2025-04-02 20:29:43,861 - INFO - Epoch 2 (Step 250000): Train loss 4.339, Val loss 4.355\n",
      "Epoch 2:  19%|█▉        | 40753/210244 [08:49<35:19, 79.97batch/s]2025-04-02 20:29:56,418 - INFO - Epoch 2 (Step 251000): Train loss 4.157, Val loss 4.367\n",
      "Epoch 2:  19%|█▉        | 40859/210244 [08:50<34:19, 82.26batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:29:57,707] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  20%|█▉        | 41756/210244 [09:03<34:39, 81.02batch/s]  2025-04-02 20:30:10,150 - INFO - Epoch 2 (Step 252000): Train loss 4.009, Val loss 4.378\n",
      "Epoch 2:  20%|██        | 42756/210244 [09:15<34:54, 79.98batch/s]2025-04-02 20:30:22,619 - INFO - Epoch 2 (Step 253000): Train loss 4.029, Val loss 4.379\n",
      "Epoch 2:  21%|██        | 43139/210244 [09:20<33:42, 82.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:30:27,356] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  21%|██        | 43749/210244 [09:27<34:59, 79.29batch/s]2025-04-02 20:30:35,078 - INFO - Epoch 2 (Step 254000): Train loss 4.075, Val loss 4.389\n",
      "Epoch 2:  21%|██▏       | 44748/210244 [09:41<33:54, 81.33batch/s]  2025-04-02 20:30:48,892 - INFO - Epoch 2 (Step 255000): Train loss 4.143, Val loss 4.405\n",
      "Epoch 2:  22%|██▏       | 45751/210244 [09:54<35:59, 76.18batch/s]2025-04-02 20:31:01,372 - INFO - Epoch 2 (Step 256000): Train loss 4.041, Val loss 4.420\n",
      "Epoch 2:  22%|██▏       | 46748/210244 [10:08<33:38, 81.01batch/s]  2025-04-02 20:31:15,254 - INFO - Epoch 2 (Step 257000): Train loss 4.253, Val loss 4.396\n",
      "Epoch 2:  23%|██▎       | 47658/210244 [10:19<33:45, 80.27batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:31:26,425] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  23%|██▎       | 47748/210244 [10:20<33:17, 81.33batch/s]2025-04-02 20:31:27,679 - INFO - Epoch 2 (Step 258000): Train loss 4.082, Val loss 4.407\n",
      "Epoch 2:  23%|██▎       | 48378/210244 [10:28<32:34, 82.83batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:31:35,405] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  23%|██▎       | 48755/210244 [10:32<33:20, 80.72batch/s]2025-04-02 20:31:40,078 - INFO - Epoch 2 (Step 259000): Train loss 4.097, Val loss 4.415\n",
      "Epoch 2:  24%|██▎       | 49750/210244 [10:46<33:05, 80.83batch/s]  2025-04-02 20:31:54,006 - INFO - Epoch 2 (Step 260000): Train loss 4.172, Val loss 4.399\n",
      "Epoch 2:  24%|██▍       | 50754/210244 [10:59<33:50, 78.54batch/s]2025-04-02 20:32:06,438 - INFO - Epoch 2 (Step 261000): Train loss 4.169, Val loss 4.407\n",
      "Epoch 2:  25%|██▍       | 51752/210244 [11:13<33:19, 79.25batch/s]  2025-04-02 20:32:20,246 - INFO - Epoch 2 (Step 262000): Train loss 4.196, Val loss 4.408\n",
      "Epoch 2:  25%|██▌       | 52681/210244 [11:24<31:36, 83.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:32:31,755] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  25%|██▌       | 52753/210244 [11:25<33:17, 78.85batch/s]2025-04-02 20:32:32,703 - INFO - Epoch 2 (Step 263000): Train loss 4.053, Val loss 4.408\n",
      "Epoch 2:  25%|██▌       | 52922/210244 [11:27<32:24, 80.90batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:32:34,764] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  26%|██▌       | 53752/210244 [11:37<32:35, 80.03batch/s]2025-04-02 20:32:45,148 - INFO - Epoch 2 (Step 264000): Train loss 4.147, Val loss 4.397\n",
      "Epoch 2:  26%|██▌       | 54756/210244 [11:51<32:24, 79.97batch/s]  2025-04-02 20:32:58,981 - INFO - Epoch 2 (Step 265000): Train loss 4.085, Val loss 4.387\n",
      "Epoch 2:  26%|██▋       | 55599/210244 [12:02<31:13, 82.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:33:09,348] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  27%|██▋       | 55752/210244 [12:04<31:36, 81.44batch/s]2025-04-02 20:33:11,289 - INFO - Epoch 2 (Step 266000): Train loss 3.942, Val loss 4.369\n",
      "Epoch 2:  27%|██▋       | 56753/210244 [12:18<1:51:16, 22.99batch/s]2025-04-02 20:33:25,148 - INFO - Epoch 2 (Step 267000): Train loss 3.936, Val loss 4.405\n",
      "Epoch 2:  27%|██▋       | 57751/210244 [12:30<31:17, 81.21batch/s]  2025-04-02 20:33:37,653 - INFO - Epoch 2 (Step 268000): Train loss 4.047, Val loss 4.405\n",
      "Epoch 2:  28%|██▊       | 58753/210244 [12:43<31:20, 80.54batch/s]2025-04-02 20:33:50,181 - INFO - Epoch 2 (Step 269000): Train loss 4.010, Val loss 4.402\n",
      "Epoch 2:  28%|██▊       | 59718/210244 [12:56<31:02, 80.82batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:34:03,533] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  28%|██▊       | 59754/210244 [12:56<31:02, 80.79batch/s]2025-04-02 20:34:04,029 - INFO - Epoch 2 (Step 270000): Train loss 4.130, Val loss 4.395\n",
      "Epoch 2:  28%|██▊       | 59858/210244 [12:58<31:41, 79.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:34:05,347] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  29%|██▉       | 60748/210244 [13:09<30:30, 81.66batch/s]2025-04-02 20:34:16,426 - INFO - Epoch 2 (Step 271000): Train loss 3.924, Val loss 4.402\n",
      "Epoch 2:  29%|██▉       | 61748/210244 [13:21<31:27, 78.69batch/s]2025-04-02 20:34:28,902 - INFO - Epoch 2 (Step 272000): Train loss 4.106, Val loss 4.398\n",
      "Epoch 2:  30%|██▉       | 62377/210244 [13:30<30:18, 81.32batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:34:38,059] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  30%|██▉       | 62748/210244 [13:35<30:47, 79.84batch/s]2025-04-02 20:34:42,795 - INFO - Epoch 2 (Step 273000): Train loss 4.072, Val loss 4.410\n",
      "Epoch 2:  30%|███       | 63756/210244 [13:48<30:20, 80.47batch/s]2025-04-02 20:34:55,448 - INFO - Epoch 2 (Step 274000): Train loss 4.069, Val loss 4.416\n",
      "Epoch 2:  31%|███       | 64390/210244 [13:56<30:38, 79.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:35:03,502] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  31%|███       | 64753/210244 [14:02<30:32, 79.38batch/s]  2025-04-02 20:35:09,507 - INFO - Epoch 2 (Step 275000): Train loss 4.182, Val loss 4.424\n",
      "Epoch 2:  31%|███▏      | 65750/210244 [14:14<29:43, 81.00batch/s]2025-04-02 20:35:21,884 - INFO - Epoch 2 (Step 276000): Train loss 3.850, Val loss 4.406\n",
      "Epoch 2:  31%|███▏      | 65973/210244 [14:17<28:49, 83.43batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:35:24,653] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  32%|███▏      | 66752/210244 [14:27<29:19, 81.53batch/s]2025-04-02 20:35:34,219 - INFO - Epoch 2 (Step 277000): Train loss 3.942, Val loss 4.405\n",
      "Epoch 2:  32%|███▏      | 67748/210244 [14:40<29:35, 80.27batch/s]  2025-04-02 20:35:48,034 - INFO - Epoch 2 (Step 278000): Train loss 4.127, Val loss 4.415\n",
      "Epoch 2:  33%|███▎      | 68750/210244 [14:53<29:29, 79.95batch/s]2025-04-02 20:36:00,418 - INFO - Epoch 2 (Step 279000): Train loss 4.119, Val loss 4.418\n",
      "Epoch 2:  33%|███▎      | 69756/210244 [15:07<33:26, 70.02batch/s]  2025-04-02 20:36:14,202 - INFO - Epoch 2 (Step 280000): Train loss 3.913, Val loss 4.382\n",
      "Epoch 2:  34%|███▎      | 70756/210244 [15:19<28:44, 80.91batch/s]2025-04-02 20:36:26,623 - INFO - Epoch 2 (Step 281000): Train loss 4.086, Val loss 4.348\n",
      "Epoch 2:  34%|███▍      | 71070/210244 [15:23<28:59, 80.00batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:36:30,566] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  34%|███▍      | 71749/210244 [15:31<28:30, 80.96batch/s]2025-04-02 20:36:39,167 - INFO - Epoch 2 (Step 282000): Train loss 4.092, Val loss 4.379\n",
      "Epoch 2:  35%|███▍      | 72753/210244 [15:45<28:05, 81.57batch/s]  2025-04-02 20:36:52,982 - INFO - Epoch 2 (Step 283000): Train loss 3.989, Val loss 4.403\n",
      "Epoch 2:  35%|███▍      | 73137/210244 [15:50<27:47, 82.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:36:57,853] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  35%|███▌      | 73753/210244 [15:58<27:32, 82.60batch/s]2025-04-02 20:37:05,503 - INFO - Epoch 2 (Step 284000): Train loss 4.147, Val loss 4.376\n",
      "Epoch 2:  36%|███▌      | 74752/210244 [16:10<28:13, 79.98batch/s]2025-04-02 20:37:18,080 - INFO - Epoch 2 (Step 285000): Train loss 3.940, Val loss 4.376\n",
      "Epoch 2:  36%|███▌      | 75708/210244 [16:24<28:08, 79.66batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:37:31,286] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  36%|███▌      | 75753/210244 [16:24<27:48, 80.60batch/s]2025-04-02 20:37:31,857 - INFO - Epoch 2 (Step 286000): Train loss 3.969, Val loss 4.412\n",
      "Epoch 2:  37%|███▋      | 76748/210244 [16:37<28:07, 79.09batch/s]2025-04-02 20:37:44,319 - INFO - Epoch 2 (Step 287000): Train loss 4.166, Val loss 4.413\n",
      "Epoch 2:  37%|███▋      | 77752/210244 [16:51<28:18, 78.02batch/s]  2025-04-02 20:37:58,159 - INFO - Epoch 2 (Step 288000): Train loss 4.086, Val loss 4.346\n",
      "Epoch 2:  37%|███▋      | 78017/210244 [16:54<27:28, 80.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:38:01,440] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  37%|███▋      | 78748/210244 [17:03<27:07, 80.82batch/s]2025-04-02 20:38:10,606 - INFO - Epoch 2 (Step 289000): Train loss 4.047, Val loss 4.386\n",
      "Epoch 2:  38%|███▊      | 79121/210244 [17:08<26:47, 81.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:38:15,313] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  38%|███▊      | 79754/210244 [17:16<26:57, 80.68batch/s]2025-04-02 20:38:23,189 - INFO - Epoch 2 (Step 290000): Train loss 4.254, Val loss 4.374\n",
      "Epoch 2:  38%|███▊      | 80756/210244 [17:29<26:50, 80.38batch/s]  2025-04-02 20:38:37,030 - INFO - Epoch 2 (Step 291000): Train loss 4.074, Val loss 4.358\n",
      "Epoch 2:  39%|███▉      | 81755/210244 [17:42<27:02, 79.21batch/s]2025-04-02 20:38:49,457 - INFO - Epoch 2 (Step 292000): Train loss 3.989, Val loss 4.399\n",
      "Epoch 2:  39%|███▉      | 82749/210244 [17:55<26:52, 79.06batch/s]  2025-04-02 20:39:03,138 - INFO - Epoch 2 (Step 293000): Train loss 4.157, Val loss 4.382\n",
      "Epoch 2:  40%|███▉      | 83756/210244 [18:08<26:17, 80.17batch/s]2025-04-02 20:39:15,757 - INFO - Epoch 2 (Step 294000): Train loss 4.100, Val loss 4.391\n",
      "Epoch 2:  40%|████      | 84749/210244 [18:21<26:09, 79.97batch/s]2025-04-02 20:39:28,298 - INFO - Epoch 2 (Step 295000): Train loss 4.121, Val loss 4.362\n",
      "Epoch 2:  41%|████      | 85163/210244 [18:26<25:27, 81.86batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:39:33,423] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  41%|████      | 85521/210244 [18:32<25:29, 81.53batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:39:39,167] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  41%|████      | 85754/210244 [18:34<25:20, 81.85batch/s]2025-04-02 20:39:42,063 - INFO - Epoch 2 (Step 296000): Train loss 4.011, Val loss 4.367\n",
      "Epoch 2:  41%|████▏     | 86754/210244 [18:47<27:03, 76.05batch/s]2025-04-02 20:39:54,532 - INFO - Epoch 2 (Step 297000): Train loss 3.959, Val loss 4.371\n",
      "Epoch 2:  42%|████▏     | 87748/210244 [18:59<25:23, 80.40batch/s]2025-04-02 20:40:06,914 - INFO - Epoch 2 (Step 298000): Train loss 4.206, Val loss 4.375\n",
      "Epoch 2:  42%|████▏     | 88752/210244 [19:13<24:53, 81.36batch/s]  2025-04-02 20:40:20,655 - INFO - Epoch 2 (Step 299000): Train loss 4.167, Val loss 4.376\n",
      "Epoch 2:  43%|████▎     | 89754/210244 [19:26<24:55, 80.55batch/s]2025-04-02 20:40:33,189 - INFO - Epoch 2 (Step 300000): Train loss 3.986, Val loss 4.363\n",
      "Epoch 2:  43%|████▎     | 89825/210244 [19:26<24:45, 81.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:40:34,132] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  43%|████▎     | 90751/210244 [19:39<24:25, 81.55batch/s]  2025-04-02 20:40:46,971 - INFO - Epoch 2 (Step 301000): Train loss 4.005, Val loss 4.358\n",
      "Epoch 2:  44%|████▎     | 91753/210244 [19:52<24:26, 80.82batch/s]2025-04-02 20:40:59,426 - INFO - Epoch 2 (Step 302000): Train loss 4.168, Val loss 4.376\n",
      "Epoch 2:  44%|████▎     | 91895/210244 [19:54<24:13, 81.40batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:41:01,174] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  44%|████▎     | 91931/210244 [19:54<24:15, 81.28batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:41:01,642] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  44%|████▍     | 92756/210244 [20:04<25:27, 76.93batch/s]2025-04-02 20:41:11,923 - INFO - Epoch 2 (Step 303000): Train loss 4.087, Val loss 4.363\n",
      "Epoch 2:  45%|████▍     | 93748/210244 [20:18<24:23, 79.62batch/s]  2025-04-02 20:41:25,827 - INFO - Epoch 2 (Step 304000): Train loss 4.174, Val loss 4.402\n",
      "Epoch 2:  45%|████▍     | 94027/210244 [20:22<23:31, 82.34batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:41:29,186] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  45%|████▌     | 94754/210244 [20:31<24:14, 79.38batch/s]2025-04-02 20:41:38,284 - INFO - Epoch 2 (Step 305000): Train loss 3.925, Val loss 4.386\n",
      "Epoch 2:  46%|████▌     | 95751/210244 [20:44<23:31, 81.12batch/s]  2025-04-02 20:41:52,114 - INFO - Epoch 2 (Step 306000): Train loss 4.205, Val loss 4.342\n",
      "Epoch 2:  46%|████▌     | 96751/210244 [20:57<23:15, 81.32batch/s]2025-04-02 20:42:04,570 - INFO - Epoch 2 (Step 307000): Train loss 4.130, Val loss 4.364\n",
      "Epoch 2:  46%|████▌     | 96910/210244 [20:59<22:57, 82.29batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:42:06,504] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  46%|████▋     | 97752/210244 [21:09<23:39, 79.24batch/s]2025-04-02 20:42:17,025 - INFO - Epoch 2 (Step 308000): Train loss 3.993, Val loss 4.396\n",
      "Epoch 2:  47%|████▋     | 98751/210244 [21:23<22:51, 81.27batch/s]  2025-04-02 20:42:30,903 - INFO - Epoch 2 (Step 309000): Train loss 4.187, Val loss 4.388\n",
      "Epoch 2:  47%|████▋     | 99539/210244 [21:33<22:59, 80.28batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:42:40,714] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  47%|████▋     | 99755/210244 [21:36<22:33, 81.64batch/s]2025-04-02 20:42:43,376 - INFO - Epoch 2 (Step 310000): Train loss 4.131, Val loss 4.351\n",
      "Epoch 2:  48%|████▊     | 100754/210244 [21:48<22:27, 81.24batch/s]2025-04-02 20:42:55,818 - INFO - Epoch 2 (Step 311000): Train loss 3.969, Val loss 4.349\n",
      "Epoch 2:  48%|████▊     | 101750/210244 [22:02<22:20, 80.94batch/s]  2025-04-02 20:43:09,618 - INFO - Epoch 2 (Step 312000): Train loss 4.130, Val loss 4.359\n",
      "Epoch 2:  49%|████▉     | 102576/210244 [22:12<21:53, 81.98batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:43:19,786] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  49%|████▉     | 102756/210244 [22:14<22:09, 80.86batch/s]2025-04-02 20:43:21,979 - INFO - Epoch 2 (Step 313000): Train loss 4.088, Val loss 4.349\n",
      "Epoch 2:  49%|████▉     | 103756/210244 [22:28<22:24, 79.20batch/s]  2025-04-02 20:43:35,720 - INFO - Epoch 2 (Step 314000): Train loss 4.037, Val loss 4.379\n",
      "Epoch 2:  50%|████▉     | 104753/210244 [22:41<21:18, 82.51batch/s]2025-04-02 20:43:48,164 - INFO - Epoch 2 (Step 315000): Train loss 3.946, Val loss 4.390\n",
      "Epoch 2:  50%|█████     | 105750/210244 [22:53<22:35, 77.12batch/s]2025-04-02 20:44:00,605 - INFO - Epoch 2 (Step 316000): Train loss 4.032, Val loss 4.392\n",
      "Epoch 2:  51%|█████     | 106660/210244 [23:06<21:36, 79.88batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:44:13,232] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  51%|█████     | 106722/210244 [23:06<21:50, 79.00batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:44:13,967] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  51%|█████     | 106755/210244 [23:07<21:49, 79.05batch/s]2025-04-02 20:44:14,456 - INFO - Epoch 2 (Step 317000): Train loss 3.818, Val loss 4.381\n",
      "Epoch 2:  51%|█████     | 106954/210244 [23:09<21:27, 80.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:44:17,017] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  51%|█████▏    | 107752/210244 [23:19<21:23, 79.84batch/s]2025-04-02 20:44:27,012 - INFO - Epoch 2 (Step 318000): Train loss 4.102, Val loss 4.371\n",
      "Epoch 2:  52%|█████▏    | 108754/210244 [23:33<20:40, 81.80batch/s]  2025-04-02 20:44:40,741 - INFO - Epoch 2 (Step 319000): Train loss 3.979, Val loss 4.363\n",
      "Epoch 2:  52%|█████▏    | 109755/210244 [23:46<20:27, 81.88batch/s]2025-04-02 20:44:53,185 - INFO - Epoch 2 (Step 320000): Train loss 4.200, Val loss 4.368\n",
      "Epoch 2:  53%|█████▎    | 110750/210244 [23:58<20:44, 79.95batch/s]2025-04-02 20:45:05,652 - INFO - Epoch 2 (Step 321000): Train loss 4.203, Val loss 4.354\n",
      "Epoch 2:  53%|█████▎    | 111665/210244 [24:11<21:50, 75.22batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:45:18,474] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  53%|█████▎    | 111753/210244 [24:12<20:19, 80.77batch/s]2025-04-02 20:45:19,575 - INFO - Epoch 2 (Step 322000): Train loss 4.081, Val loss 4.371\n",
      "Epoch 2:  54%|█████▎    | 112755/210244 [24:24<20:06, 80.79batch/s]2025-04-02 20:45:31,996 - INFO - Epoch 2 (Step 323000): Train loss 4.033, Val loss 4.394\n",
      "Epoch 2:  54%|█████▍    | 113750/210244 [24:37<19:48, 81.21batch/s]2025-04-02 20:45:44,339 - INFO - Epoch 2 (Step 324000): Train loss 3.963, Val loss 4.366\n",
      "Epoch 2:  55%|█████▍    | 114749/210244 [24:50<19:57, 79.72batch/s]  2025-04-02 20:45:58,121 - INFO - Epoch 2 (Step 325000): Train loss 4.008, Val loss 4.379\n",
      "Epoch 2:  55%|█████▌    | 115744/210244 [25:03<19:32, 80.61batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:46:10,481] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  55%|█████▌    | 115753/210244 [25:03<19:06, 82.45batch/s]2025-04-02 20:46:10,629 - INFO - Epoch 2 (Step 326000): Train loss 4.130, Val loss 4.354\n",
      "Epoch 2:  56%|█████▌    | 116752/210244 [25:17<19:05, 81.63batch/s]  2025-04-02 20:46:24,440 - INFO - Epoch 2 (Step 327000): Train loss 4.383, Val loss 4.368\n",
      "Epoch 2:  56%|█████▌    | 116779/210244 [25:17<20:49, 74.82batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:46:24,725] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  56%|█████▌    | 117754/210244 [25:29<19:34, 78.73batch/s]2025-04-02 20:46:36,825 - INFO - Epoch 2 (Step 328000): Train loss 4.083, Val loss 4.364\n",
      "Epoch 2:  56%|█████▋    | 118748/210244 [25:42<19:38, 77.66batch/s]2025-04-02 20:46:49,331 - INFO - Epoch 2 (Step 329000): Train loss 4.085, Val loss 4.373\n",
      "Epoch 2:  57%|█████▋    | 119750/210244 [25:56<18:49, 80.15batch/s]  2025-04-02 20:47:03,209 - INFO - Epoch 2 (Step 330000): Train loss 3.927, Val loss 4.390\n",
      "Epoch 2:  57%|█████▋    | 120231/210244 [26:02<18:12, 82.40batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:47:09,112] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  57%|█████▋    | 120753/210244 [26:08<18:16, 81.58batch/s]2025-04-02 20:47:15,574 - INFO - Epoch 2 (Step 331000): Train loss 3.906, Val loss 4.366\n",
      "Epoch 2:  58%|█████▊    | 121756/210244 [26:22<18:30, 79.72batch/s]  2025-04-02 20:47:29,469 - INFO - Epoch 2 (Step 332000): Train loss 4.113, Val loss 4.375\n",
      "Epoch 2:  58%|█████▊    | 122753/210244 [26:34<18:09, 80.28batch/s]2025-04-02 20:47:41,965 - INFO - Epoch 2 (Step 333000): Train loss 3.947, Val loss 4.378\n",
      "Epoch 2:  59%|█████▉    | 123756/210244 [26:47<17:29, 82.43batch/s]2025-04-02 20:47:54,601 - INFO - Epoch 2 (Step 334000): Train loss 4.118, Val loss 4.390\n",
      "Epoch 2:  59%|█████▉    | 124295/210244 [26:55<17:43, 80.85batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:48:02,599] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  59%|█████▉    | 124756/210244 [27:01<17:37, 80.82batch/s]2025-04-02 20:48:08,375 - INFO - Epoch 2 (Step 335000): Train loss 4.096, Val loss 4.377\n",
      "Epoch 2:  60%|█████▉    | 125749/210244 [27:13<17:15, 81.61batch/s]2025-04-02 20:48:20,941 - INFO - Epoch 2 (Step 336000): Train loss 4.137, Val loss 4.380\n",
      "Epoch 2:  60%|██████    | 126340/210244 [27:21<17:21, 80.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:48:28,252] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  60%|██████    | 126750/210244 [27:27<39:41, 35.06batch/s]  2025-04-02 20:48:34,739 - INFO - Epoch 2 (Step 337000): Train loss 4.047, Val loss 4.360\n",
      "Epoch 2:  60%|██████    | 126945/210244 [27:30<17:08, 80.95batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:48:37,087] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  61%|██████    | 127753/210244 [27:40<17:03, 80.57batch/s]2025-04-02 20:48:47,170 - INFO - Epoch 2 (Step 338000): Train loss 3.992, Val loss 4.360\n",
      "Epoch 2:  61%|██████    | 128755/210244 [27:52<16:43, 81.21batch/s]2025-04-02 20:48:59,744 - INFO - Epoch 2 (Step 339000): Train loss 4.227, Val loss 4.385\n",
      "Epoch 2:  62%|██████▏   | 129756/210244 [28:06<16:32, 81.06batch/s]  2025-04-02 20:49:13,530 - INFO - Epoch 2 (Step 340000): Train loss 3.948, Val loss 4.381\n",
      "Epoch 2:  62%|██████▏   | 130755/210244 [28:18<16:07, 82.17batch/s]2025-04-02 20:49:26,053 - INFO - Epoch 2 (Step 341000): Train loss 4.104, Val loss 4.379\n",
      "Epoch 2:  62%|██████▏   | 131016/210244 [28:22<16:15, 81.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:49:29,269] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  63%|██████▎   | 131748/210244 [28:31<16:20, 80.09batch/s]2025-04-02 20:49:38,481 - INFO - Epoch 2 (Step 342000): Train loss 4.128, Val loss 4.369\n",
      "Epoch 2:  63%|██████▎   | 132756/210244 [28:45<16:00, 80.66batch/s]  2025-04-02 20:49:52,153 - INFO - Epoch 2 (Step 343000): Train loss 4.078, Val loss 4.327\n",
      "Epoch 2:  63%|██████▎   | 133255/210244 [28:51<15:57, 80.41batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:49:58,429] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  63%|██████▎   | 133273/210244 [28:51<15:56, 80.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:49:58,703] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  64%|██████▎   | 133756/210244 [28:57<15:50, 80.47batch/s]2025-04-02 20:50:04,789 - INFO - Epoch 2 (Step 344000): Train loss 4.043, Val loss 4.345\n",
      "Epoch 2:  64%|██████▍   | 134755/210244 [29:11<15:29, 81.22batch/s]  2025-04-02 20:50:18,657 - INFO - Epoch 2 (Step 345000): Train loss 3.865, Val loss 4.343\n",
      "Epoch 2:  65%|██████▍   | 135755/210244 [29:23<15:16, 81.26batch/s]2025-04-02 20:50:31,081 - INFO - Epoch 2 (Step 346000): Train loss 4.046, Val loss 4.357\n",
      "Epoch 2:  65%|██████▍   | 135961/210244 [29:26<15:21, 80.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:50:33,659] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  65%|██████▍   | 136177/210244 [29:29<15:04, 81.88batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:50:36,317] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  65%|██████▌   | 136751/210244 [29:36<14:56, 82.00batch/s]2025-04-02 20:50:43,543 - INFO - Epoch 2 (Step 347000): Train loss 4.052, Val loss 4.348\n",
      "Epoch 2:  66%|██████▌   | 137756/210244 [29:50<15:59, 75.58batch/s]  2025-04-02 20:50:57,357 - INFO - Epoch 2 (Step 348000): Train loss 4.102, Val loss 4.367\n",
      "Epoch 2:  66%|██████▌   | 138748/210244 [30:02<14:52, 80.13batch/s]2025-04-02 20:51:09,741 - INFO - Epoch 2 (Step 349000): Train loss 4.128, Val loss 4.356\n",
      "Epoch 2:  66%|██████▋   | 139751/210244 [30:16<16:01, 73.31batch/s]  2025-04-02 20:51:23,522 - INFO - Epoch 2 (Step 350000): Train loss 4.180, Val loss 4.359\n",
      "Epoch 2:  67%|██████▋   | 140752/210244 [30:28<14:00, 82.71batch/s]2025-04-02 20:51:35,885 - INFO - Epoch 2 (Step 351000): Train loss 4.211, Val loss 4.381\n",
      "Epoch 2:  67%|██████▋   | 141019/210244 [30:32<15:03, 76.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:51:39,253] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  67%|██████▋   | 141748/210244 [30:41<13:54, 82.08batch/s]2025-04-02 20:51:48,332 - INFO - Epoch 2 (Step 352000): Train loss 3.933, Val loss 4.377\n",
      "Epoch 2:  68%|██████▊   | 142748/210244 [30:55<13:45, 81.74batch/s]  2025-04-02 20:52:02,244 - INFO - Epoch 2 (Step 353000): Train loss 3.871, Val loss 4.380\n",
      "Epoch 2:  68%|██████▊   | 143755/210244 [31:07<13:22, 82.88batch/s]2025-04-02 20:52:14,645 - INFO - Epoch 2 (Step 354000): Train loss 4.034, Val loss 4.373\n",
      "Epoch 2:  68%|██████▊   | 143895/210244 [31:09<13:57, 79.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:52:16,402] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  69%|██████▉   | 144750/210244 [31:19<13:43, 79.57batch/s]2025-04-02 20:52:27,135 - INFO - Epoch 2 (Step 355000): Train loss 4.172, Val loss 4.358\n",
      "Epoch 2:  69%|██████▉   | 145755/210244 [31:33<13:12, 81.37batch/s]  2025-04-02 20:52:40,963 - INFO - Epoch 2 (Step 356000): Train loss 3.964, Val loss 4.362\n",
      "Epoch 2:  70%|██████▉   | 146751/210244 [31:46<13:02, 81.18batch/s]2025-04-02 20:52:53,370 - INFO - Epoch 2 (Step 357000): Train loss 3.885, Val loss 4.363\n",
      "Epoch 2:  70%|███████   | 147752/210244 [32:00<12:53, 80.84batch/s]  2025-04-02 20:53:07,205 - INFO - Epoch 2 (Step 358000): Train loss 4.099, Val loss 4.365\n",
      "Epoch 2:  71%|███████   | 148386/210244 [32:07<12:58, 79.42batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:53:15,035] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  71%|███████   | 148751/210244 [32:12<12:38, 81.07batch/s]2025-04-02 20:53:19,638 - INFO - Epoch 2 (Step 359000): Train loss 3.917, Val loss 4.363\n",
      "Epoch 2:  71%|███████   | 149749/210244 [32:24<12:23, 81.33batch/s]2025-04-02 20:53:32,131 - INFO - Epoch 2 (Step 360000): Train loss 3.963, Val loss 4.360\n",
      "Epoch 2:  72%|███████▏  | 150703/210244 [32:38<12:09, 81.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:53:45,187] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  72%|███████▏  | 150748/210244 [32:38<12:23, 80.02batch/s]2025-04-02 20:53:45,900 - INFO - Epoch 2 (Step 361000): Train loss 4.081, Val loss 4.354\n",
      "Epoch 2:  72%|███████▏  | 151082/210244 [32:43<12:31, 78.74batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:53:50,073] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  72%|███████▏  | 151750/210244 [32:51<12:24, 78.53batch/s]2025-04-02 20:53:58,526 - INFO - Epoch 2 (Step 362000): Train loss 3.836, Val loss 4.372\n",
      "Epoch 2:  73%|███████▎  | 152754/210244 [33:05<12:42, 75.43batch/s]2025-04-02 20:54:12,395 - INFO - Epoch 2 (Step 363000): Train loss 4.050, Val loss 4.384\n",
      "Epoch 2:  73%|███████▎  | 153408/210244 [33:13<11:41, 81.03batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:54:20,513] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  73%|███████▎  | 153750/210244 [33:17<11:33, 81.48batch/s]2025-04-02 20:54:24,796 - INFO - Epoch 2 (Step 364000): Train loss 4.092, Val loss 4.399\n",
      "Epoch 2:  74%|███████▎  | 154755/210244 [33:30<11:12, 82.47batch/s]2025-04-02 20:54:37,145 - INFO - Epoch 2 (Step 365000): Train loss 3.977, Val loss 4.359\n",
      "Epoch 2:  74%|███████▍  | 155751/210244 [33:43<11:29, 79.06batch/s]2025-04-02 20:54:51,020 - INFO - Epoch 2 (Step 366000): Train loss 4.195, Val loss 4.377\n",
      "Epoch 2:  74%|███████▍  | 156363/210244 [33:51<11:02, 81.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:54:58,672] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  74%|███████▍  | 156408/210244 [33:52<11:18, 79.39batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:54:59,193] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  75%|███████▍  | 156749/210244 [33:56<10:54, 81.74batch/s]2025-04-02 20:55:03,470 - INFO - Epoch 2 (Step 367000): Train loss 3.995, Val loss 4.371\n",
      "Epoch 2:  75%|███████▌  | 157750/210244 [34:08<11:04, 78.96batch/s]2025-04-02 20:55:15,961 - INFO - Epoch 2 (Step 368000): Train loss 4.120, Val loss 4.383\n",
      "Epoch 2:  76%|███████▌  | 158748/210244 [34:23<10:36, 80.89batch/s]2025-04-02 20:55:30,215 - INFO - Epoch 2 (Step 369000): Train loss 3.983, Val loss 4.406\n",
      "Epoch 2:  76%|███████▌  | 159754/210244 [34:35<10:21, 81.24batch/s]2025-04-02 20:55:42,539 - INFO - Epoch 2 (Step 370000): Train loss 4.054, Val loss 4.368\n",
      "Epoch 2:  76%|███████▋  | 160753/210244 [34:49<10:04, 81.92batch/s]2025-04-02 20:55:56,368 - INFO - Epoch 2 (Step 371000): Train loss 3.939, Val loss 4.398\n",
      "Epoch 2:  77%|███████▋  | 161748/210244 [35:01<09:54, 81.54batch/s]2025-04-02 20:56:08,790 - INFO - Epoch 2 (Step 372000): Train loss 4.081, Val loss 4.374\n",
      "Epoch 2:  77%|███████▋  | 162448/210244 [35:10<10:07, 78.64batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:56:17,409] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  77%|███████▋  | 162754/210244 [35:14<10:09, 77.93batch/s]2025-04-02 20:56:21,270 - INFO - Epoch 2 (Step 373000): Train loss 4.175, Val loss 4.374\n",
      "Epoch 2:  78%|███████▊  | 163749/210244 [35:27<09:22, 82.63batch/s]2025-04-02 20:56:35,083 - INFO - Epoch 2 (Step 374000): Train loss 4.074, Val loss 4.424\n",
      "Epoch 2:  78%|███████▊  | 164497/210244 [35:37<09:20, 81.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:56:44,311] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  78%|███████▊  | 164695/210244 [35:39<09:19, 81.42batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:56:46,746] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  78%|███████▊  | 164749/210244 [35:40<09:21, 81.01batch/s]2025-04-02 20:56:47,531 - INFO - Epoch 2 (Step 375000): Train loss 3.937, Val loss 4.395\n",
      "Epoch 2:  79%|███████▉  | 165749/210244 [35:54<09:11, 80.64batch/s]2025-04-02 20:57:01,319 - INFO - Epoch 2 (Step 376000): Train loss 4.011, Val loss 4.388\n",
      "Epoch 2:  79%|███████▉  | 166748/210244 [36:06<08:57, 80.90batch/s]2025-04-02 20:57:13,783 - INFO - Epoch 2 (Step 377000): Train loss 4.005, Val loss 4.362\n",
      "Epoch 2:  79%|███████▉  | 166810/210244 [36:07<09:10, 78.87batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:57:14,462] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|███████▉  | 167748/210244 [36:18<08:51, 79.92batch/s]2025-04-02 20:57:26,191 - INFO - Epoch 2 (Step 378000): Train loss 4.130, Val loss 4.387\n",
      "Epoch 2:  80%|████████  | 168751/210244 [36:32<08:26, 81.97batch/s]2025-04-02 20:57:40,007 - INFO - Epoch 2 (Step 379000): Train loss 4.037, Val loss 4.380\n",
      "Epoch 2:  81%|████████  | 169748/210244 [36:45<08:16, 81.48batch/s]2025-04-02 20:57:52,577 - INFO - Epoch 2 (Step 380000): Train loss 4.063, Val loss 4.380\n",
      "Epoch 2:  81%|████████  | 170751/210244 [36:59<22:24, 29.37batch/s]2025-04-02 20:58:06,306 - INFO - Epoch 2 (Step 381000): Train loss 4.138, Val loss 4.375\n",
      "Epoch 2:  81%|████████  | 170821/210244 [37:00<08:58, 73.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:58:07,208] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  81%|████████▏ | 170943/210244 [37:01<08:02, 81.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:58:08,695] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  82%|████████▏ | 171754/210244 [37:11<08:04, 79.43batch/s]2025-04-02 20:58:18,886 - INFO - Epoch 2 (Step 382000): Train loss 3.969, Val loss 4.373\n",
      "Epoch 2:  82%|████████▏ | 172751/210244 [37:24<07:51, 79.46batch/s]2025-04-02 20:58:31,424 - INFO - Epoch 2 (Step 383000): Train loss 4.028, Val loss 4.406\n",
      "Epoch 2:  82%|████████▏ | 173397/210244 [37:33<07:57, 77.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:58:40,713] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  83%|████████▎ | 173756/210244 [37:38<07:33, 80.49batch/s]2025-04-02 20:58:45,117 - INFO - Epoch 2 (Step 384000): Train loss 3.982, Val loss 4.406\n",
      "Epoch 2:  83%|████████▎ | 174755/210244 [37:50<07:12, 82.08batch/s]2025-04-02 20:58:57,513 - INFO - Epoch 2 (Step 385000): Train loss 4.162, Val loss 4.391\n",
      "Epoch 2:  84%|████████▎ | 175750/210244 [38:02<07:09, 80.22batch/s]2025-04-02 20:59:09,975 - INFO - Epoch 2 (Step 386000): Train loss 4.045, Val loss 4.400\n",
      "Epoch 2:  84%|████████▍ | 176750/210244 [38:16<06:54, 80.89batch/s]2025-04-02 20:59:23,803 - INFO - Epoch 2 (Step 387000): Train loss 4.042, Val loss 4.384\n",
      "Epoch 2:  84%|████████▍ | 177581/210244 [38:26<06:43, 80.92batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 20:59:34,070] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  85%|████████▍ | 177752/210244 [38:29<06:40, 81.04batch/s]2025-04-02 20:59:36,198 - INFO - Epoch 2 (Step 388000): Train loss 4.026, Val loss 4.374\n",
      "Epoch 2:  85%|████████▌ | 178754/210244 [38:42<06:26, 81.49batch/s]2025-04-02 20:59:49,936 - INFO - Epoch 2 (Step 389000): Train loss 4.176, Val loss 4.371\n",
      "Epoch 2:  85%|████████▌ | 179672/210244 [38:54<06:14, 81.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:00:01,374] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  85%|████████▌ | 179753/210244 [38:55<06:09, 82.60batch/s]2025-04-02 21:00:02,427 - INFO - Epoch 2 (Step 390000): Train loss 4.079, Val loss 4.384\n",
      "Epoch 2:  86%|████████▌ | 180488/210244 [39:04<06:16, 79.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:00:11,492] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  86%|████████▌ | 180750/210244 [39:07<06:09, 79.88batch/s]2025-04-02 21:00:14,957 - INFO - Epoch 2 (Step 391000): Train loss 4.080, Val loss 4.384\n",
      "Epoch 2:  86%|████████▋ | 181751/210244 [39:21<05:51, 81.06batch/s]2025-04-02 21:00:28,820 - INFO - Epoch 2 (Step 392000): Train loss 4.076, Val loss 4.361\n",
      "Epoch 2:  87%|████████▋ | 182742/210244 [39:33<05:39, 81.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:00:41,066] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  87%|████████▋ | 182751/210244 [39:34<05:32, 82.69batch/s]2025-04-02 21:00:41,263 - INFO - Epoch 2 (Step 393000): Train loss 4.108, Val loss 4.379\n",
      "Epoch 2:  87%|████████▋ | 183754/210244 [39:47<07:53, 55.93batch/s]2025-04-02 21:00:55,104 - INFO - Epoch 2 (Step 394000): Train loss 4.013, Val loss 4.348\n",
      "Epoch 2:  88%|████████▊ | 184752/210244 [40:00<05:19, 79.71batch/s]2025-04-02 21:01:07,631 - INFO - Epoch 2 (Step 395000): Train loss 4.044, Val loss 4.339\n",
      "Epoch 2:  88%|████████▊ | 185753/210244 [40:12<05:05, 80.13batch/s]2025-04-02 21:01:20,072 - INFO - Epoch 2 (Step 396000): Train loss 4.024, Val loss 4.371\n",
      "Epoch 2:  88%|████████▊ | 186040/210244 [40:16<04:55, 81.80batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:01:23,569] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  89%|████████▊ | 186121/210244 [40:17<04:57, 81.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:01:24,653] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  89%|████████▉ | 186755/210244 [40:26<04:54, 79.73batch/s]2025-04-02 21:01:33,833 - INFO - Epoch 2 (Step 397000): Train loss 4.094, Val loss 4.359\n",
      "Epoch 2:  89%|████████▉ | 187756/210244 [40:39<04:38, 80.74batch/s]2025-04-02 21:01:46,526 - INFO - Epoch 2 (Step 398000): Train loss 4.104, Val loss 4.363\n",
      "Epoch 2:  90%|████████▉ | 188754/210244 [40:51<04:28, 80.15batch/s]2025-04-02 21:01:59,090 - INFO - Epoch 2 (Step 399000): Train loss 4.056, Val loss 4.383\n",
      "Epoch 2:  90%|█████████ | 189755/210244 [41:06<04:30, 75.84batch/s]2025-04-02 21:02:13,147 - INFO - Epoch 2 (Step 400000): Train loss 4.140, Val loss 4.373\n",
      "Epoch 2:  91%|█████████ | 190750/210244 [41:18<04:01, 80.66batch/s]2025-04-02 21:02:25,598 - INFO - Epoch 2 (Step 401000): Train loss 4.209, Val loss 4.348\n",
      "Epoch 2:  91%|█████████ | 191756/210244 [41:32<03:46, 81.54batch/s]2025-04-02 21:02:39,499 - INFO - Epoch 2 (Step 402000): Train loss 4.221, Val loss 4.365\n",
      "Epoch 2:  92%|█████████▏| 192572/210244 [41:42<03:38, 80.72batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:02:49,687] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  92%|█████████▏| 192753/210244 [41:44<03:33, 81.80batch/s]2025-04-02 21:02:51,957 - INFO - Epoch 2 (Step 403000): Train loss 4.164, Val loss 4.366\n",
      "Epoch 2:  92%|█████████▏| 193749/210244 [41:57<03:19, 82.56batch/s]2025-04-02 21:03:04,298 - INFO - Epoch 2 (Step 404000): Train loss 4.114, Val loss 4.339\n",
      "Epoch 2:  93%|█████████▎| 194754/210244 [42:10<03:10, 81.32batch/s]2025-04-02 21:03:18,082 - INFO - Epoch 2 (Step 405000): Train loss 4.097, Val loss 4.358\n",
      "Epoch 2:  93%|█████████▎| 194815/210244 [42:11<03:11, 80.61batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:03:18,881] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  93%|█████████▎| 195489/210244 [42:20<02:56, 83.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:03:27,109] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  93%|█████████▎| 195753/210244 [42:23<03:01, 80.03batch/s]2025-04-02 21:03:30,479 - INFO - Epoch 2 (Step 406000): Train loss 4.289, Val loss 4.345\n",
      "Epoch 2:  94%|█████████▎| 196753/210244 [42:37<02:47, 80.48batch/s]2025-04-02 21:03:44,219 - INFO - Epoch 2 (Step 407000): Train loss 4.192, Val loss 4.389\n",
      "Epoch 2:  94%|█████████▍| 197570/210244 [42:47<02:37, 80.41batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:03:54,449] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  94%|█████████▍| 197750/210244 [42:49<02:39, 78.54batch/s]2025-04-02 21:03:56,718 - INFO - Epoch 2 (Step 408000): Train loss 4.106, Val loss 4.356\n",
      "Epoch 2:  95%|█████████▍| 198756/210244 [43:01<02:22, 80.67batch/s]2025-04-02 21:04:09,047 - INFO - Epoch 2 (Step 409000): Train loss 4.190, Val loss 4.366\n",
      "Epoch 2:  95%|█████████▍| 199621/210244 [43:14<02:10, 81.46batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:04:21,106] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  95%|█████████▌| 199756/210244 [43:15<02:09, 80.80batch/s]2025-04-02 21:04:22,802 - INFO - Epoch 2 (Step 410000): Train loss 4.167, Val loss 4.346\n",
      "Epoch 2:  95%|█████████▌| 200752/210244 [43:28<01:58, 80.28batch/s]2025-04-02 21:04:35,337 - INFO - Epoch 2 (Step 411000): Train loss 3.916, Val loss 4.356\n",
      "Epoch 2:  96%|█████████▌| 201754/210244 [43:40<01:43, 82.07batch/s]2025-04-02 21:04:47,753 - INFO - Epoch 2 (Step 412000): Train loss 3.891, Val loss 4.369\n",
      "Epoch 2:  96%|█████████▋| 202753/210244 [43:54<01:32, 80.90batch/s]2025-04-02 21:05:01,518 - INFO - Epoch 2 (Step 413000): Train loss 4.165, Val loss 4.357\n",
      "Epoch 2:  96%|█████████▋| 202798/210244 [43:54<01:35, 77.57batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:05:02,047] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  97%|█████████▋| 203756/210244 [44:06<01:18, 82.96batch/s]2025-04-02 21:05:13,892 - INFO - Epoch 2 (Step 414000): Train loss 4.184, Val loss 4.362\n",
      "Epoch 2:  97%|█████████▋| 204753/210244 [44:20<01:07, 81.23batch/s]2025-04-02 21:05:27,771 - INFO - Epoch 2 (Step 415000): Train loss 4.067, Val loss 4.337\n",
      "Epoch 2:  98%|█████████▊| 205542/210244 [44:30<00:59, 79.44batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:05:37,731] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  98%|█████████▊| 205756/210244 [44:33<00:55, 81.56batch/s]2025-04-02 21:05:40,372 - INFO - Epoch 2 (Step 416000): Train loss 4.009, Val loss 4.368\n",
      "Epoch 2:  98%|█████████▊| 206751/210244 [44:45<00:43, 80.42batch/s]2025-04-02 21:05:52,930 - INFO - Epoch 2 (Step 417000): Train loss 4.249, Val loss 4.362\n",
      "Epoch 2:  99%|█████████▉| 207748/210244 [44:59<00:30, 82.48batch/s]2025-04-02 21:06:06,799 - INFO - Epoch 2 (Step 418000): Train loss 3.993, Val loss 4.386\n",
      "Epoch 2:  99%|█████████▉| 208755/210244 [45:12<00:18, 80.48batch/s]2025-04-02 21:06:19,343 - INFO - Epoch 2 (Step 419000): Train loss 4.145, Val loss 4.348\n",
      "Epoch 2: 100%|█████████▉| 209621/210244 [45:24<00:09, 62.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:06:31,454] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████▉| 209754/210244 [45:25<00:06, 80.58batch/s]2025-04-02 21:06:33,104 - INFO - Epoch 2 (Step 420000): Train loss 3.827, Val loss 4.362\n",
      "Epoch 2: 100%|██████████| 210244/210244 [45:32<00:00, 76.95batch/s]\n",
      "2025-04-02 21:06:39,136 - INFO - Epoch 2 completed. Generating a sample...\n",
      "2025-04-02 21:06:39,207 - INFO - Generated Text: Every effort moves you out of the game . \" \n",
      "   = = = = = = = = = = = = \n",
      "   The game was released on October 12 , 2008 , and was released on the Xbox 360 console . It was released on October\n",
      "2025-04-02 21:06:39,208 - INFO - Starting Epoch 3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you out of the game . \"     = = = = = = = = = = = =     The game was released on October 12 , 2008 , and was released on the Xbox 360 console . It was released on October\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 160/210244 [00:01<43:10, 81.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:06:41,254] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 509/210244 [00:06<42:31, 82.21batch/s]2025-04-02 21:06:45,566 - INFO - Epoch 3 (Step 421000): Train loss 4.064, Val loss 4.381\n",
      "Epoch 3:   1%|          | 1508/210244 [00:18<43:29, 79.99batch/s]2025-04-02 21:06:57,892 - INFO - Epoch 3 (Step 422000): Train loss 3.996, Val loss 4.374\n",
      "Epoch 3:   1%|          | 2510/210244 [00:32<42:50, 80.82batch/s]  2025-04-02 21:07:11,599 - INFO - Epoch 3 (Step 423000): Train loss 4.077, Val loss 4.379\n",
      "Epoch 3:   2%|▏         | 3507/210244 [00:44<44:11, 77.96batch/s]2025-04-02 21:07:24,024 - INFO - Epoch 3 (Step 424000): Train loss 4.048, Val loss 4.379\n",
      "Epoch 3:   2%|▏         | 4189/210244 [00:53<41:53, 81.99batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:07:32,412] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   2%|▏         | 4504/210244 [00:56<42:33, 80.56batch/s]2025-04-02 21:07:36,363 - INFO - Epoch 3 (Step 425000): Train loss 4.085, Val loss 4.355\n",
      "Epoch 3:   2%|▏         | 4852/210244 [01:02<43:57, 77.88batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:07:42,068] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   3%|▎         | 5509/210244 [01:11<42:30, 80.27batch/s]2025-04-02 21:07:50,339 - INFO - Epoch 3 (Step 426000): Train loss 4.134, Val loss 4.375\n",
      "Epoch 3:   3%|▎         | 6508/210244 [01:23<41:33, 81.70batch/s]2025-04-02 21:08:02,778 - INFO - Epoch 3 (Step 427000): Train loss 4.106, Val loss 4.349\n",
      "Epoch 3:   3%|▎         | 7181/210244 [01:33<1:06:41, 50.74batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:08:12,501] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   4%|▎         | 7512/210244 [01:37<42:02, 80.38batch/s]  2025-04-02 21:08:16,595 - INFO - Epoch 3 (Step 428000): Train loss 3.980, Val loss 4.356\n",
      "Epoch 3:   4%|▍         | 8510/210244 [01:49<40:46, 82.44batch/s]2025-04-02 21:08:28,926 - INFO - Epoch 3 (Step 429000): Train loss 4.018, Val loss 4.377\n",
      "Epoch 3:   5%|▍         | 9506/210244 [02:02<41:20, 80.94batch/s]2025-04-02 21:08:41,395 - INFO - Epoch 3 (Step 430000): Train loss 4.022, Val loss 4.367\n",
      "Epoch 3:   5%|▍         | 10504/210244 [02:15<41:05, 81.02batch/s] 2025-04-02 21:08:55,235 - INFO - Epoch 3 (Step 431000): Train loss 4.024, Val loss 4.380\n",
      "Epoch 3:   5%|▌         | 11358/210244 [02:26<40:44, 81.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:09:05,892] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   5%|▌         | 11512/210244 [02:28<41:27, 79.89batch/s]2025-04-02 21:09:07,740 - INFO - Epoch 3 (Step 432000): Train loss 4.031, Val loss 4.354\n",
      "Epoch 3:   6%|▌         | 12511/210244 [02:42<40:32, 81.30batch/s]  2025-04-02 21:09:21,597 - INFO - Epoch 3 (Step 433000): Train loss 4.104, Val loss 4.387\n",
      "Epoch 3:   6%|▋         | 13187/210244 [02:50<41:57, 78.28batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:09:29,982] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   6%|▋         | 13506/210244 [02:54<41:59, 78.08batch/s]2025-04-02 21:09:34,074 - INFO - Epoch 3 (Step 434000): Train loss 4.060, Val loss 4.369\n",
      "Epoch 3:   7%|▋         | 14507/210244 [03:07<40:12, 81.13batch/s]2025-04-02 21:09:46,775 - INFO - Epoch 3 (Step 435000): Train loss 3.869, Val loss 4.359\n",
      "Epoch 3:   7%|▋         | 15506/210244 [03:21<40:51, 79.42batch/s]  2025-04-02 21:10:00,661 - INFO - Epoch 3 (Step 436000): Train loss 4.194, Val loss 4.317\n",
      "Epoch 3:   8%|▊         | 16504/210244 [03:33<40:03, 80.60batch/s]2025-04-02 21:10:13,123 - INFO - Epoch 3 (Step 437000): Train loss 4.182, Val loss 4.366\n",
      "Epoch 3:   8%|▊         | 16608/210244 [03:35<40:49, 79.04batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:10:14,332] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   8%|▊         | 17505/210244 [03:46<39:12, 81.93batch/s]2025-04-02 21:10:26,827 - INFO - Epoch 3 (Step 438000): Train loss 4.223, Val loss 4.380\n",
      "Epoch 3:   9%|▉         | 18511/210244 [03:59<39:03, 81.81batch/s]  2025-04-02 21:10:39,204 - INFO - Epoch 3 (Step 439000): Train loss 3.961, Val loss 4.335\n",
      "Epoch 3:   9%|▉         | 19505/210244 [04:12<38:31, 82.53batch/s]2025-04-02 21:10:51,522 - INFO - Epoch 3 (Step 440000): Train loss 4.073, Val loss 4.370\n",
      "Epoch 3:  10%|▉         | 19993/210244 [04:18<39:10, 80.95batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:10:57,582] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  10%|▉         | 20181/210244 [04:21<1:06:32, 47.61batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:11:01,296] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  10%|▉         | 20509/210244 [04:26<38:57, 81.18batch/s]  2025-04-02 21:11:05,413 - INFO - Epoch 3 (Step 441000): Train loss 4.030, Val loss 4.366\n",
      "Epoch 3:  10%|█         | 21512/210244 [04:38<39:46, 79.09batch/s]2025-04-02 21:11:17,824 - INFO - Epoch 3 (Step 442000): Train loss 4.037, Val loss 4.386\n",
      "Epoch 3:  11%|█         | 22507/210244 [04:51<38:32, 81.17batch/s]2025-04-02 21:11:30,360 - INFO - Epoch 3 (Step 443000): Train loss 4.094, Val loss 4.361\n",
      "Epoch 3:  11%|█         | 23505/210244 [05:04<38:11, 81.51batch/s]  2025-04-02 21:11:44,200 - INFO - Epoch 3 (Step 444000): Train loss 4.181, Val loss 4.359\n",
      "Epoch 3:  12%|█▏        | 24510/210244 [05:17<39:23, 78.59batch/s]2025-04-02 21:11:56,745 - INFO - Epoch 3 (Step 445000): Train loss 4.080, Val loss 4.368\n",
      "Epoch 3:  12%|█▏        | 25509/210244 [05:31<37:34, 81.95batch/s]  2025-04-02 21:12:10,549 - INFO - Epoch 3 (Step 446000): Train loss 4.037, Val loss 4.366\n",
      "Epoch 3:  13%|█▎        | 26334/210244 [05:41<39:04, 78.46batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:12:20,788] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  13%|█▎        | 26505/210244 [05:43<37:27, 81.76batch/s]2025-04-02 21:12:22,980 - INFO - Epoch 3 (Step 447000): Train loss 4.009, Val loss 4.367\n",
      "Epoch 3:  13%|█▎        | 27302/210244 [05:53<37:49, 80.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:12:32,733] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  13%|█▎        | 27506/210244 [05:56<38:21, 79.40batch/s]2025-04-02 21:12:35,377 - INFO - Epoch 3 (Step 448000): Train loss 3.903, Val loss 4.342\n",
      "Epoch 3:  14%|█▎        | 28509/210244 [06:09<37:01, 81.79batch/s]  2025-04-02 21:12:49,177 - INFO - Epoch 3 (Step 449000): Train loss 4.218, Val loss 4.356\n",
      "Epoch 3:  14%|█▍        | 29508/210244 [06:22<37:37, 80.06batch/s]2025-04-02 21:13:01,646 - INFO - Epoch 3 (Step 450000): Train loss 4.011, Val loss 4.359\n",
      "Epoch 3:  15%|█▍        | 30509/210244 [06:34<36:40, 81.67batch/s]2025-04-02 21:13:14,099 - INFO - Epoch 3 (Step 451000): Train loss 4.033, Val loss 4.350\n",
      "Epoch 3:  15%|█▍        | 31218/210244 [06:45<36:59, 80.67batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:13:24,260] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  15%|█▍        | 31506/210244 [06:48<36:39, 81.26batch/s]2025-04-02 21:13:27,886 - INFO - Epoch 3 (Step 452000): Train loss 4.098, Val loss 4.389\n",
      "Epoch 3:  15%|█▌        | 32004/210244 [06:54<36:59, 80.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:13:34,086] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  15%|█▌        | 32512/210244 [07:01<36:39, 80.80batch/s]2025-04-02 21:13:40,372 - INFO - Epoch 3 (Step 453000): Train loss 3.913, Val loss 4.377\n",
      "Epoch 3:  16%|█▌        | 33508/210244 [07:14<36:45, 80.15batch/s]  2025-04-02 21:13:54,260 - INFO - Epoch 3 (Step 454000): Train loss 4.092, Val loss 4.352\n",
      "Epoch 3:  16%|█▋        | 34511/210244 [07:27<35:36, 82.25batch/s]2025-04-02 21:14:06,702 - INFO - Epoch 3 (Step 455000): Train loss 3.910, Val loss 4.374\n",
      "Epoch 3:  17%|█▋        | 35509/210244 [07:39<35:23, 82.27batch/s]2025-04-02 21:14:19,187 - INFO - Epoch 3 (Step 456000): Train loss 3.996, Val loss 4.382\n",
      "Epoch 3:  17%|█▋        | 36125/210244 [07:48<35:26, 81.89batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:14:28,146] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  17%|█▋        | 36512/210244 [07:53<35:50, 80.80batch/s]2025-04-02 21:14:32,993 - INFO - Epoch 3 (Step 457000): Train loss 3.861, Val loss 4.361\n",
      "Epoch 3:  18%|█▊        | 37445/210244 [08:05<35:55, 80.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:14:44,641] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  18%|█▊        | 37508/210244 [08:06<36:07, 79.70batch/s]2025-04-02 21:14:45,537 - INFO - Epoch 3 (Step 458000): Train loss 4.116, Val loss 4.364\n",
      "Epoch 3:  18%|█▊        | 38511/210244 [08:20<36:31, 78.37batch/s]  2025-04-02 21:14:59,358 - INFO - Epoch 3 (Step 459000): Train loss 4.014, Val loss 4.366\n",
      "Epoch 3:  19%|█▉        | 39504/210244 [08:32<35:07, 81.02batch/s]2025-04-02 21:15:11,792 - INFO - Epoch 3 (Step 460000): Train loss 3.903, Val loss 4.347\n",
      "Epoch 3:  19%|█▉        | 40507/210244 [08:44<35:42, 79.23batch/s]2025-04-02 21:15:24,252 - INFO - Epoch 3 (Step 461000): Train loss 4.188, Val loss 4.353\n",
      "Epoch 3:  20%|█▉        | 41511/210244 [08:58<34:54, 80.56batch/s]  2025-04-02 21:15:38,012 - INFO - Epoch 3 (Step 462000): Train loss 3.932, Val loss 4.355\n",
      "Epoch 3:  20%|█▉        | 41726/210244 [09:01<34:30, 81.39batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:15:40,719] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  20%|██        | 42505/210244 [09:11<34:26, 81.17batch/s]2025-04-02 21:15:50,470 - INFO - Epoch 3 (Step 463000): Train loss 4.167, Val loss 4.364\n",
      "Epoch 3:  21%|██        | 43510/210244 [09:25<42:45, 64.99batch/s]  2025-04-02 21:16:04,512 - INFO - Epoch 3 (Step 464000): Train loss 4.062, Val loss 4.357\n",
      "Epoch 3:  21%|██        | 44511/210244 [09:37<33:36, 82.20batch/s]2025-04-02 21:16:16,931 - INFO - Epoch 3 (Step 465000): Train loss 3.967, Val loss 4.369\n",
      "Epoch 3:  22%|██▏       | 45505/210244 [09:50<34:36, 79.35batch/s]2025-04-02 21:16:29,470 - INFO - Epoch 3 (Step 466000): Train loss 4.131, Val loss 4.380\n",
      "Epoch 3:  22%|██▏       | 45964/210244 [09:55<33:47, 81.01batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:16:35,085] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  22%|██▏       | 46504/210244 [10:03<34:06, 79.99batch/s]  2025-04-02 21:16:43,273 - INFO - Epoch 3 (Step 467000): Train loss 3.991, Val loss 4.375\n",
      "Epoch 3:  23%|██▎       | 47512/210244 [10:16<33:23, 81.24batch/s]2025-04-02 21:16:55,848 - INFO - Epoch 3 (Step 468000): Train loss 4.089, Val loss 4.371\n",
      "Epoch 3:  23%|██▎       | 47627/210244 [10:18<34:04, 79.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:16:57,352] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  23%|██▎       | 48509/210244 [10:28<33:24, 80.69batch/s]2025-04-02 21:17:08,341 - INFO - Epoch 3 (Step 469000): Train loss 4.112, Val loss 4.374\n",
      "Epoch 3:  24%|██▎       | 49509/210244 [10:42<32:53, 81.43batch/s]  2025-04-02 21:17:22,174 - INFO - Epoch 3 (Step 470000): Train loss 3.854, Val loss 4.359\n",
      "Epoch 3:  24%|██▍       | 50504/210244 [10:55<32:56, 80.82batch/s]2025-04-02 21:17:34,611 - INFO - Epoch 3 (Step 471000): Train loss 3.992, Val loss 4.361\n",
      "Epoch 3:  25%|██▍       | 51510/210244 [11:09<33:04, 79.98batch/s]  2025-04-02 21:17:48,438 - INFO - Epoch 3 (Step 472000): Train loss 4.073, Val loss 4.388\n",
      "Epoch 3:  25%|██▍       | 51687/210244 [11:11<33:05, 79.88batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:17:50,648] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  25%|██▍       | 52025/210244 [11:15<32:42, 80.61batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:17:54,925] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  25%|██▍       | 52512/210244 [11:21<33:27, 78.58batch/s]2025-04-02 21:18:01,065 - INFO - Epoch 3 (Step 473000): Train loss 4.086, Val loss 4.370\n",
      "Epoch 3:  25%|██▌       | 53504/210244 [11:34<32:49, 79.57batch/s]2025-04-02 21:18:13,585 - INFO - Epoch 3 (Step 474000): Train loss 4.079, Val loss 4.367\n",
      "Epoch 3:  26%|██▌       | 54506/210244 [11:47<32:14, 80.53batch/s]  2025-04-02 21:18:27,352 - INFO - Epoch 3 (Step 475000): Train loss 4.183, Val loss 4.372\n",
      "Epoch 3:  26%|██▌       | 54833/210244 [11:52<32:16, 80.27batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:18:31,486] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  26%|██▋       | 55506/210244 [12:00<32:12, 80.05batch/s]2025-04-02 21:18:39,958 - INFO - Epoch 3 (Step 476000): Train loss 4.109, Val loss 4.375\n",
      "Epoch 3:  27%|██▋       | 56511/210244 [12:14<32:00, 80.05batch/s]  2025-04-02 21:18:53,872 - INFO - Epoch 3 (Step 477000): Train loss 3.956, Val loss 4.328\n",
      "Epoch 3:  27%|██▋       | 57509/210244 [12:27<31:18, 81.30batch/s]2025-04-02 21:19:06,333 - INFO - Epoch 3 (Step 478000): Train loss 3.924, Val loss 4.338\n",
      "Epoch 3:  28%|██▊       | 58508/210244 [12:39<30:43, 82.32batch/s]2025-04-02 21:19:18,742 - INFO - Epoch 3 (Step 479000): Train loss 4.057, Val loss 4.337\n",
      "Epoch 3:  28%|██▊       | 59042/210244 [12:47<35:45, 70.49batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:19:26,757] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  28%|██▊       | 59506/210244 [12:53<32:02, 78.39batch/s]2025-04-02 21:19:32,557 - INFO - Epoch 3 (Step 480000): Train loss 4.079, Val loss 4.351\n",
      "Epoch 3:  29%|██▊       | 60047/210244 [12:59<30:39, 81.65batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:19:39,294] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  29%|██▉       | 60504/210244 [13:05<31:13, 79.95batch/s]2025-04-02 21:19:45,048 - INFO - Epoch 3 (Step 481000): Train loss 4.100, Val loss 4.308\n",
      "Epoch 3:  29%|██▉       | 61512/210244 [13:18<30:44, 80.64batch/s]2025-04-02 21:19:58,930 - INFO - Epoch 3 (Step 482000): Train loss 3.939, Val loss 4.321\n",
      "Epoch 3:  30%|██▉       | 62504/210244 [13:32<30:35, 80.47batch/s]  2025-04-02 21:20:11,667 - INFO - Epoch 3 (Step 483000): Train loss 3.890, Val loss 4.339\n",
      "Epoch 3:  30%|██▉       | 63020/210244 [13:38<30:00, 81.77batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:20:17,985] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  30%|███       | 63506/210244 [13:44<30:24, 80.45batch/s]2025-04-02 21:20:24,212 - INFO - Epoch 3 (Step 484000): Train loss 4.010, Val loss 4.349\n",
      "Epoch 3:  31%|███       | 64508/210244 [13:58<29:23, 82.62batch/s]  2025-04-02 21:20:38,095 - INFO - Epoch 3 (Step 485000): Train loss 4.034, Val loss 4.365\n",
      "Epoch 3:  31%|███       | 65080/210244 [14:05<30:47, 78.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:20:45,145] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  31%|███       | 65512/210244 [14:11<29:35, 81.52batch/s]2025-04-02 21:20:50,488 - INFO - Epoch 3 (Step 486000): Train loss 3.869, Val loss 4.339\n",
      "Epoch 3:  32%|███▏      | 66512/210244 [14:23<30:18, 79.06batch/s]2025-04-02 21:21:02,951 - INFO - Epoch 3 (Step 487000): Train loss 4.118, Val loss 4.365\n",
      "Epoch 3:  32%|███▏      | 67504/210244 [14:37<29:28, 80.72batch/s]  2025-04-02 21:21:16,766 - INFO - Epoch 3 (Step 488000): Train loss 3.958, Val loss 4.359\n",
      "Epoch 3:  33%|███▎      | 68509/210244 [14:49<29:13, 80.85batch/s]2025-04-02 21:21:29,225 - INFO - Epoch 3 (Step 489000): Train loss 3.993, Val loss 4.374\n",
      "Epoch 3:  33%|███▎      | 69462/210244 [15:03<28:55, 81.10batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:21:42,378] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 69506/210244 [15:03<29:28, 79.59batch/s]2025-04-02 21:21:42,980 - INFO - Epoch 3 (Step 490000): Train loss 4.083, Val loss 4.383\n",
      "Epoch 3:  34%|███▎      | 70504/210244 [15:16<29:39, 78.55batch/s]2025-04-02 21:21:55,381 - INFO - Epoch 3 (Step 491000): Train loss 3.965, Val loss 4.371\n",
      "Epoch 3:  34%|███▍      | 71472/210244 [15:28<28:36, 80.84batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:22:07,451] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  34%|███▍      | 71505/210244 [15:28<30:15, 76.43batch/s]2025-04-02 21:22:07,974 - INFO - Epoch 3 (Step 492000): Train loss 4.186, Val loss 4.373\n",
      "Epoch 3:  34%|███▍      | 72509/210244 [15:42<28:11, 81.44batch/s]  2025-04-02 21:22:21,847 - INFO - Epoch 3 (Step 493000): Train loss 3.907, Val loss 4.381\n",
      "Epoch 3:  35%|███▍      | 73155/210244 [15:50<28:00, 81.57batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:22:29,890] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  35%|███▍      | 73506/210244 [15:54<28:19, 80.45batch/s]2025-04-02 21:22:34,331 - INFO - Epoch 3 (Step 494000): Train loss 4.203, Val loss 4.371\n",
      "Epoch 3:  35%|███▌      | 74509/210244 [16:08<29:24, 76.95batch/s]  2025-04-02 21:22:48,162 - INFO - Epoch 3 (Step 495000): Train loss 4.156, Val loss 4.377\n",
      "Epoch 3:  36%|███▌      | 75512/210244 [16:21<27:37, 81.31batch/s]2025-04-02 21:23:00,666 - INFO - Epoch 3 (Step 496000): Train loss 4.122, Val loss 4.398\n",
      "Epoch 3:  36%|███▋      | 76505/210244 [16:33<29:03, 76.72batch/s]2025-04-02 21:23:13,192 - INFO - Epoch 3 (Step 497000): Train loss 4.154, Val loss 4.391\n",
      "Epoch 3:  37%|███▋      | 77400/210244 [16:46<27:35, 80.23batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:23:25,552] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  37%|███▋      | 77508/210244 [16:47<27:17, 81.06batch/s]2025-04-02 21:23:26,932 - INFO - Epoch 3 (Step 498000): Train loss 4.046, Val loss 4.383\n",
      "Epoch 3:  37%|███▋      | 78457/210244 [16:59<26:56, 81.51batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:23:38,680] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  37%|███▋      | 78510/210244 [17:00<28:18, 77.57batch/s]2025-04-02 21:23:39,444 - INFO - Epoch 3 (Step 499000): Train loss 4.183, Val loss 4.382\n",
      "Epoch 3:  38%|███▊      | 79512/210244 [17:12<26:56, 80.85batch/s]2025-04-02 21:23:51,812 - INFO - Epoch 3 (Step 500000): Train loss 3.955, Val loss 4.373\n",
      "Epoch 3:  38%|███▊      | 80511/210244 [17:26<26:10, 82.63batch/s]  2025-04-02 21:24:05,634 - INFO - Epoch 3 (Step 501000): Train loss 4.053, Val loss 4.380\n",
      "Epoch 3:  39%|███▉      | 81504/210244 [17:38<26:20, 81.45batch/s]2025-04-02 21:24:17,998 - INFO - Epoch 3 (Step 502000): Train loss 4.160, Val loss 4.372\n",
      "Epoch 3:  39%|███▉      | 82504/210244 [17:52<26:02, 81.74batch/s]  2025-04-02 21:24:31,665 - INFO - Epoch 3 (Step 503000): Train loss 4.086, Val loss 4.371\n",
      "Epoch 3:  39%|███▉      | 82576/210244 [17:53<26:36, 79.94batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:24:32,508] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  40%|███▉      | 83508/210244 [18:04<26:18, 80.27batch/s]2025-04-02 21:24:44,014 - INFO - Epoch 3 (Step 504000): Train loss 4.051, Val loss 4.357\n",
      "Epoch 3:  40%|████      | 84505/210244 [18:17<26:26, 79.25batch/s]2025-04-02 21:24:56,572 - INFO - Epoch 3 (Step 505000): Train loss 3.908, Val loss 4.364\n",
      "Epoch 3:  40%|████      | 84638/210244 [18:18<25:42, 81.45batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:24:58,133] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  40%|████      | 84755/210244 [18:20<24:52, 84.08batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:24:59,563] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  41%|████      | 85505/210244 [18:30<27:20, 76.06batch/s]  2025-04-02 21:25:10,323 - INFO - Epoch 3 (Step 506000): Train loss 4.029, Val loss 4.356\n",
      "Epoch 3:  41%|████      | 86508/210244 [18:43<25:23, 81.22batch/s]2025-04-02 21:25:22,812 - INFO - Epoch 3 (Step 507000): Train loss 3.966, Val loss 4.362\n",
      "Epoch 3:  42%|████▏     | 87508/210244 [18:57<26:21, 77.59batch/s]  2025-04-02 21:25:36,540 - INFO - Epoch 3 (Step 508000): Train loss 3.833, Val loss 4.354\n",
      "Epoch 3:  42%|████▏     | 88348/210244 [19:07<24:56, 81.45batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:25:46,894] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  42%|████▏     | 88510/210244 [19:09<25:09, 80.63batch/s]2025-04-02 21:25:48,908 - INFO - Epoch 3 (Step 509000): Train loss 4.123, Val loss 4.330\n",
      "Epoch 3:  43%|████▎     | 89506/210244 [19:21<24:37, 81.69batch/s]2025-04-02 21:26:01,288 - INFO - Epoch 3 (Step 510000): Train loss 4.074, Val loss 4.359\n",
      "Epoch 3:  43%|████▎     | 90505/210244 [19:35<24:26, 81.65batch/s]  2025-04-02 21:26:15,031 - INFO - Epoch 3 (Step 511000): Train loss 4.224, Val loss 4.374\n",
      "Epoch 3:  43%|████▎     | 90550/210244 [19:36<25:37, 77.87batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:26:15,534] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  43%|████▎     | 91099/210244 [19:43<24:25, 81.33batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:26:22,316] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  44%|████▎     | 91512/210244 [19:48<25:16, 78.28batch/s]2025-04-02 21:26:27,425 - INFO - Epoch 3 (Step 512000): Train loss 4.072, Val loss 4.342\n",
      "Epoch 3:  44%|████▍     | 92512/210244 [20:00<24:14, 80.95batch/s]2025-04-02 21:26:39,795 - INFO - Epoch 3 (Step 513000): Train loss 4.024, Val loss 4.354\n",
      "Epoch 3:  44%|████▍     | 93511/210244 [20:14<23:51, 81.53batch/s]  2025-04-02 21:26:53,697 - INFO - Epoch 3 (Step 514000): Train loss 4.066, Val loss 4.363\n",
      "Epoch 3:  45%|████▍     | 94510/210244 [20:26<24:05, 80.04batch/s]2025-04-02 21:27:06,066 - INFO - Epoch 3 (Step 515000): Train loss 3.993, Val loss 4.365\n",
      "Epoch 3:  45%|████▌     | 95506/210244 [20:40<24:04, 79.41batch/s]  2025-04-02 21:27:19,857 - INFO - Epoch 3 (Step 516000): Train loss 3.995, Val loss 4.381\n",
      "Epoch 3:  46%|████▌     | 96511/210244 [20:52<23:11, 81.74batch/s]2025-04-02 21:27:32,272 - INFO - Epoch 3 (Step 517000): Train loss 4.012, Val loss 4.351\n",
      "Epoch 3:  46%|████▋     | 97506/210244 [21:05<23:15, 80.78batch/s]2025-04-02 21:27:45,081 - INFO - Epoch 3 (Step 518000): Train loss 4.009, Val loss 4.356\n",
      "Epoch 3:  47%|████▋     | 98416/210244 [21:18<23:17, 80.00batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:27:57,672] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  47%|████▋     | 98506/210244 [21:19<23:08, 80.50batch/s]2025-04-02 21:27:58,901 - INFO - Epoch 3 (Step 519000): Train loss 4.196, Val loss 4.356\n",
      "Epoch 3:  47%|████▋     | 99055/210244 [21:26<22:47, 81.33batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:28:05,770] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  47%|████▋     | 99510/210244 [21:32<23:27, 78.65batch/s]2025-04-02 21:28:11,492 - INFO - Epoch 3 (Step 520000): Train loss 4.016, Val loss 4.344\n",
      "Epoch 3:  48%|████▊     | 100509/210244 [21:45<22:36, 80.89batch/s]  2025-04-02 21:28:25,274 - INFO - Epoch 3 (Step 521000): Train loss 3.673, Val loss 4.368\n",
      "Epoch 3:  48%|████▊     | 101508/210244 [21:58<22:27, 80.71batch/s]2025-04-02 21:28:37,755 - INFO - Epoch 3 (Step 522000): Train loss 4.105, Val loss 4.326\n",
      "Epoch 3:  49%|████▉     | 102504/210244 [22:10<22:16, 80.59batch/s]2025-04-02 21:28:50,096 - INFO - Epoch 3 (Step 523000): Train loss 3.953, Val loss 4.355\n",
      "Epoch 3:  49%|████▉     | 103200/210244 [22:20<22:14, 80.21batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:28:59,936] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  49%|████▉     | 103371/210244 [22:22<21:41, 82.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:29:02,117] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  49%|████▉     | 103512/210244 [22:24<22:29, 79.10batch/s]2025-04-02 21:29:03,850 - INFO - Epoch 3 (Step 524000): Train loss 4.125, Val loss 4.356\n",
      "Epoch 3:  50%|████▉     | 104509/210244 [22:36<21:38, 81.43batch/s]2025-04-02 21:29:16,293 - INFO - Epoch 3 (Step 525000): Train loss 4.064, Val loss 4.357\n",
      "Epoch 3:  50%|█████     | 105508/210244 [22:49<22:21, 78.08batch/s]2025-04-02 21:29:28,747 - INFO - Epoch 3 (Step 526000): Train loss 4.220, Val loss 4.352\n",
      "Epoch 3:  51%|█████     | 106506/210244 [23:03<21:19, 81.09batch/s]  2025-04-02 21:29:42,487 - INFO - Epoch 3 (Step 527000): Train loss 4.120, Val loss 4.361\n",
      "Epoch 3:  51%|█████     | 107509/210244 [23:15<21:01, 81.46batch/s]2025-04-02 21:29:55,027 - INFO - Epoch 3 (Step 528000): Train loss 3.943, Val loss 4.361\n",
      "Epoch 3:  51%|█████▏    | 107893/210244 [23:20<20:49, 81.89batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:29:59,853] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  52%|█████▏    | 108512/210244 [23:29<20:53, 81.13batch/s]  2025-04-02 21:30:08,876 - INFO - Epoch 3 (Step 529000): Train loss 4.206, Val loss 4.341\n",
      "Epoch 3:  52%|█████▏    | 109317/210244 [23:39<20:39, 81.46batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:30:18,972] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  52%|█████▏    | 109506/210244 [23:42<20:52, 80.41batch/s]2025-04-02 21:30:21,380 - INFO - Epoch 3 (Step 530000): Train loss 3.996, Val loss 4.350\n",
      "Epoch 3:  53%|█████▎    | 110510/210244 [23:54<20:38, 80.54batch/s]2025-04-02 21:30:33,751 - INFO - Epoch 3 (Step 531000): Train loss 4.011, Val loss 4.318\n",
      "Epoch 3:  53%|█████▎    | 111508/210244 [24:08<20:17, 81.07batch/s]  2025-04-02 21:30:47,591 - INFO - Epoch 3 (Step 532000): Train loss 4.062, Val loss 4.316\n",
      "Epoch 3:  54%|█████▎    | 112505/210244 [24:20<20:00, 81.38batch/s]2025-04-02 21:31:00,058 - INFO - Epoch 3 (Step 533000): Train loss 4.144, Val loss 4.333\n",
      "Epoch 3:  54%|█████▎    | 112549/210244 [24:21<21:08, 77.01batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:31:00,614] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  54%|█████▍    | 113509/210244 [24:34<19:34, 82.35batch/s]  2025-04-02 21:31:13,870 - INFO - Epoch 3 (Step 534000): Train loss 4.071, Val loss 4.319\n",
      "Epoch 3:  54%|█████▍    | 114509/210244 [24:47<19:52, 80.30batch/s]2025-04-02 21:31:26,468 - INFO - Epoch 3 (Step 535000): Train loss 4.016, Val loss 4.337\n",
      "Epoch 3:  55%|█████▍    | 115097/210244 [24:54<19:24, 81.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:31:33,821] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  55%|█████▍    | 115506/210244 [24:59<19:31, 80.89batch/s]2025-04-02 21:31:38,962 - INFO - Epoch 3 (Step 536000): Train loss 3.892, Val loss 4.343\n",
      "Epoch 3:  55%|█████▌    | 116506/210244 [25:13<19:29, 80.16batch/s]  2025-04-02 21:31:53,049 - INFO - Epoch 3 (Step 537000): Train loss 4.239, Val loss 4.356\n",
      "Epoch 3:  56%|█████▌    | 117511/210244 [25:26<19:02, 81.14batch/s]2025-04-02 21:32:05,476 - INFO - Epoch 3 (Step 538000): Train loss 3.946, Val loss 4.346\n",
      "Epoch 3:  56%|█████▋    | 118511/210244 [25:40<26:36, 57.46batch/s]  2025-04-02 21:32:19,401 - INFO - Epoch 3 (Step 539000): Train loss 4.153, Val loss 4.347\n",
      "Epoch 3:  57%|█████▋    | 119108/210244 [25:47<18:57, 80.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:32:26,866] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  57%|█████▋    | 119396/210244 [25:51<18:38, 81.21batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:32:30,411] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  57%|█████▋    | 119512/210244 [25:52<18:39, 81.02batch/s]2025-04-02 21:32:31,879 - INFO - Epoch 3 (Step 540000): Train loss 4.069, Val loss 4.353\n",
      "Epoch 3:  57%|█████▋    | 120510/210244 [26:04<18:14, 82.00batch/s]2025-04-02 21:32:44,291 - INFO - Epoch 3 (Step 541000): Train loss 4.137, Val loss 4.347\n",
      "Epoch 3:  58%|█████▊    | 121506/210244 [26:18<18:17, 80.89batch/s]  2025-04-02 21:32:57,973 - INFO - Epoch 3 (Step 542000): Train loss 4.240, Val loss 4.358\n",
      "Epoch 3:  58%|█████▊    | 122504/210244 [26:31<18:12, 80.33batch/s]2025-04-02 21:33:10,478 - INFO - Epoch 3 (Step 543000): Train loss 4.149, Val loss 4.361\n",
      "Epoch 3:  59%|█████▊    | 123508/210244 [26:43<18:06, 79.84batch/s]2025-04-02 21:33:22,862 - INFO - Epoch 3 (Step 544000): Train loss 3.984, Val loss 4.332\n",
      "Epoch 3:  59%|█████▉    | 123931/210244 [26:50<17:48, 80.79batch/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:33:29,513] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  59%|█████▉    | 124506/210244 [26:57<17:59, 79.42batch/s]2025-04-02 21:33:36,653 - INFO - Epoch 3 (Step 545000): Train loss 4.062, Val loss 4.368\n",
      "Epoch 3:  59%|█████▉    | 124611/210244 [26:58<18:14, 78.25batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:33:37,918] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  60%|█████▉    | 125194/210244 [27:05<17:25, 81.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:33:45,162] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  60%|█████▉    | 125508/210244 [27:09<17:28, 80.81batch/s]2025-04-02 21:33:49,083 - INFO - Epoch 3 (Step 546000): Train loss 4.053, Val loss 4.362\n",
      "Epoch 3:  60%|██████    | 126504/210244 [27:23<17:13, 81.03batch/s]  2025-04-02 21:34:02,885 - INFO - Epoch 3 (Step 547000): Train loss 4.021, Val loss 4.381\n",
      "Epoch 3:  61%|██████    | 127511/210244 [27:36<16:53, 81.65batch/s]2025-04-02 21:34:15,370 - INFO - Epoch 3 (Step 548000): Train loss 4.038, Val loss 4.366\n",
      "Epoch 3:  61%|██████    | 128507/210244 [27:48<16:45, 81.29batch/s]2025-04-02 21:34:27,901 - INFO - Epoch 3 (Step 549000): Train loss 4.129, Val loss 4.344\n",
      "Epoch 3:  62%|██████▏   | 129509/210244 [28:02<16:16, 82.65batch/s]  2025-04-02 21:34:41,750 - INFO - Epoch 3 (Step 550000): Train loss 4.071, Val loss 4.345\n",
      "Epoch 3:  62%|██████▏   | 129552/210244 [28:03<17:53, 75.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:34:42,321] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  62%|██████▏   | 130505/210244 [28:14<16:09, 82.28batch/s]2025-04-02 21:34:54,162 - INFO - Epoch 3 (Step 551000): Train loss 4.102, Val loss 4.350\n",
      "Epoch 3:  63%|██████▎   | 131512/210244 [28:28<17:07, 76.63batch/s]  2025-04-02 21:35:08,075 - INFO - Epoch 3 (Step 552000): Train loss 4.108, Val loss 4.327\n",
      "Epoch 3:  63%|██████▎   | 132507/210244 [28:41<15:53, 81.52batch/s]2025-04-02 21:35:20,521 - INFO - Epoch 3 (Step 553000): Train loss 4.082, Val loss 4.351\n",
      "Epoch 3:  63%|██████▎   | 132748/210244 [28:44<15:58, 80.86batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:35:23,558] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  64%|██████▎   | 133505/210244 [28:53<15:26, 82.83batch/s]2025-04-02 21:35:32,975 - INFO - Epoch 3 (Step 554000): Train loss 3.994, Val loss 4.328\n",
      "Epoch 3:  64%|██████▍   | 134511/210244 [29:07<15:30, 81.41batch/s]  2025-04-02 21:35:46,851 - INFO - Epoch 3 (Step 555000): Train loss 4.223, Val loss 4.331\n",
      "Epoch 3:  64%|██████▍   | 135511/210244 [29:20<15:07, 82.35batch/s]2025-04-02 21:35:59,352 - INFO - Epoch 3 (Step 556000): Train loss 4.074, Val loss 4.356\n",
      "Epoch 3:  65%|██████▍   | 135837/210244 [29:24<15:25, 80.36batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:36:03,480] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  65%|██████▍   | 136508/210244 [29:32<15:25, 79.67batch/s]2025-04-02 21:36:11,927 - INFO - Epoch 3 (Step 557000): Train loss 4.175, Val loss 4.340\n",
      "Epoch 3:  65%|██████▌   | 137510/210244 [29:46<15:21, 78.95batch/s]  2025-04-02 21:36:25,804 - INFO - Epoch 3 (Step 558000): Train loss 4.014, Val loss 4.351\n",
      "Epoch 3:  66%|██████▌   | 138511/210244 [29:59<14:32, 82.25batch/s]2025-04-02 21:36:38,314 - INFO - Epoch 3 (Step 559000): Train loss 4.156, Val loss 4.352\n",
      "Epoch 3:  66%|██████▋   | 139508/210244 [30:12<14:55, 79.03batch/s]  2025-04-02 21:36:52,191 - INFO - Epoch 3 (Step 560000): Train loss 4.064, Val loss 4.344\n",
      "Epoch 3:  67%|██████▋   | 140512/210244 [30:25<14:26, 80.51batch/s]2025-04-02 21:37:04,602 - INFO - Epoch 3 (Step 561000): Train loss 4.132, Val loss 4.365\n",
      "Epoch 3:  67%|██████▋   | 140735/210244 [30:28<15:04, 76.84batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:37:07,414] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  67%|██████▋   | 141510/210244 [30:37<14:31, 78.91batch/s]2025-04-02 21:37:17,108 - INFO - Epoch 3 (Step 562000): Train loss 3.924, Val loss 4.321\n",
      "Epoch 3:  67%|██████▋   | 141714/210244 [30:40<14:30, 78.76batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:37:21,083] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  68%|██████▊   | 142511/210244 [30:51<13:55, 81.10batch/s]  2025-04-02 21:37:31,042 - INFO - Epoch 3 (Step 563000): Train loss 4.081, Val loss 4.305\n",
      "Epoch 3:  68%|██████▊   | 143511/210244 [31:04<13:40, 81.33batch/s]2025-04-02 21:37:43,637 - INFO - Epoch 3 (Step 564000): Train loss 4.164, Val loss 4.330\n",
      "Epoch 3:  69%|██████▊   | 144509/210244 [31:17<13:15, 82.67batch/s]  2025-04-02 21:37:57,301 - INFO - Epoch 3 (Step 565000): Train loss 3.864, Val loss 4.336\n",
      "Epoch 3:  69%|██████▉   | 145511/210244 [31:30<13:24, 80.41batch/s]2025-04-02 21:38:09,707 - INFO - Epoch 3 (Step 566000): Train loss 3.932, Val loss 4.307\n",
      "Epoch 3:  70%|██████▉   | 146488/210244 [31:42<13:13, 80.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:38:21,927] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  70%|██████▉   | 146506/210244 [31:42<13:14, 80.25batch/s]2025-04-02 21:38:22,200 - INFO - Epoch 3 (Step 567000): Train loss 3.919, Val loss 4.316\n",
      "Epoch 3:  70%|███████   | 147506/210244 [31:56<12:38, 82.66batch/s]  2025-04-02 21:38:35,986 - INFO - Epoch 3 (Step 568000): Train loss 4.009, Val loss 4.326\n",
      "Epoch 3:  71%|███████   | 148266/210244 [32:06<12:53, 80.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:38:45,390] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  71%|███████   | 148506/210244 [32:09<12:44, 80.77batch/s]2025-04-02 21:38:48,476 - INFO - Epoch 3 (Step 569000): Train loss 3.996, Val loss 4.309\n",
      "Epoch 3:  71%|███████   | 149506/210244 [32:22<34:32, 29.30batch/s]2025-04-02 21:39:02,284 - INFO - Epoch 3 (Step 570000): Train loss 3.894, Val loss 4.318\n",
      "Epoch 3:  72%|███████▏  | 150512/210244 [32:35<12:23, 80.32batch/s]2025-04-02 21:39:14,734 - INFO - Epoch 3 (Step 571000): Train loss 4.114, Val loss 4.339\n",
      "Epoch 3:  72%|███████▏  | 151509/210244 [32:47<11:50, 82.64batch/s]2025-04-02 21:39:27,161 - INFO - Epoch 3 (Step 572000): Train loss 4.088, Val loss 4.320\n",
      "Epoch 3:  72%|███████▏  | 152097/210244 [32:56<34:35, 28.02batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:39:35,747] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  73%|███████▎  | 152507/210244 [33:01<11:41, 82.29batch/s]2025-04-02 21:39:41,053 - INFO - Epoch 3 (Step 573000): Train loss 4.003, Val loss 4.331\n",
      "Epoch 3:  73%|███████▎  | 153508/210244 [33:14<11:47, 80.14batch/s]2025-04-02 21:39:53,541 - INFO - Epoch 3 (Step 574000): Train loss 4.170, Val loss 4.309\n",
      "Epoch 3:  73%|███████▎  | 153811/210244 [33:18<11:44, 80.15batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:39:57,326] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  73%|███████▎  | 154507/210244 [33:26<11:27, 81.02batch/s]2025-04-02 21:40:06,023 - INFO - Epoch 3 (Step 575000): Train loss 4.000, Val loss 4.324\n",
      "Epoch 3:  74%|███████▍  | 155507/210244 [33:40<11:07, 82.00batch/s]2025-04-02 21:40:19,783 - INFO - Epoch 3 (Step 576000): Train loss 4.117, Val loss 4.320\n",
      "Epoch 3:  74%|███████▍  | 156506/210244 [33:52<11:09, 80.27batch/s]2025-04-02 21:40:32,268 - INFO - Epoch 3 (Step 577000): Train loss 4.122, Val loss 4.321\n",
      "Epoch 3:  75%|███████▍  | 157505/210244 [34:06<10:55, 80.51batch/s]2025-04-02 21:40:46,183 - INFO - Epoch 3 (Step 578000): Train loss 4.112, Val loss 4.315\n",
      "Epoch 3:  75%|███████▌  | 158509/210244 [34:19<10:28, 82.36batch/s]2025-04-02 21:40:58,600 - INFO - Epoch 3 (Step 579000): Train loss 3.881, Val loss 4.335\n",
      "Epoch 3:  76%|███████▌  | 159505/210244 [34:31<10:16, 82.28batch/s]2025-04-02 21:41:10,936 - INFO - Epoch 3 (Step 580000): Train loss 4.046, Val loss 4.329\n",
      "Epoch 3:  76%|███████▋  | 160477/210244 [34:45<10:07, 81.97batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:41:24,295] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  76%|███████▋  | 160504/210244 [34:45<10:30, 78.93batch/s]2025-04-02 21:41:24,750 - INFO - Epoch 3 (Step 581000): Train loss 3.951, Val loss 4.348\n",
      "Epoch 3:  76%|███████▋  | 160522/210244 [34:45<11:35, 71.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:41:24,910] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  77%|███████▋  | 161512/210244 [34:58<10:29, 77.38batch/s]2025-04-02 21:41:37,362 - INFO - Epoch 3 (Step 582000): Train loss 3.914, Val loss 4.342\n",
      "Epoch 3:  77%|███████▋  | 162508/210244 [35:11<21:57, 36.24batch/s]2025-04-02 21:41:51,100 - INFO - Epoch 3 (Step 583000): Train loss 4.118, Val loss 4.341\n",
      "Epoch 3:  78%|███████▊  | 163512/210244 [35:24<09:40, 80.50batch/s]2025-04-02 21:42:03,459 - INFO - Epoch 3 (Step 584000): Train loss 4.149, Val loss 4.322\n",
      "Epoch 3:  78%|███████▊  | 164505/210244 [35:36<09:20, 81.65batch/s]2025-04-02 21:42:15,975 - INFO - Epoch 3 (Step 585000): Train loss 4.201, Val loss 4.367\n",
      "Epoch 3:  79%|███████▊  | 165107/210244 [35:45<17:40, 42.56batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:42:24,768] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  79%|███████▊  | 165197/210244 [35:46<09:25, 79.70batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:42:25,821] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  79%|███████▊  | 165512/210244 [35:50<09:10, 81.29batch/s]2025-04-02 21:42:29,833 - INFO - Epoch 3 (Step 586000): Train loss 4.080, Val loss 4.346\n",
      "Epoch 3:  79%|███████▉  | 166505/210244 [36:02<08:56, 81.46batch/s]2025-04-02 21:42:42,264 - INFO - Epoch 3 (Step 587000): Train loss 4.111, Val loss 4.348\n",
      "Epoch 3:  80%|███████▉  | 167507/210244 [36:15<08:43, 81.60batch/s]2025-04-02 21:42:54,797 - INFO - Epoch 3 (Step 588000): Train loss 4.096, Val loss 4.337\n",
      "Epoch 3:  80%|████████  | 168253/210244 [36:26<08:28, 82.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:43:05,377] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|████████  | 168508/210244 [36:29<08:51, 78.56batch/s]2025-04-02 21:43:08,671 - INFO - Epoch 3 (Step 589000): Train loss 4.007, Val loss 4.331\n",
      "Epoch 3:  81%|████████  | 169504/210244 [36:41<08:18, 81.68batch/s]2025-04-02 21:43:21,220 - INFO - Epoch 3 (Step 590000): Train loss 4.090, Val loss 4.349\n",
      "Epoch 3:  81%|████████  | 170506/210244 [36:55<08:09, 81.12batch/s]2025-04-02 21:43:34,998 - INFO - Epoch 3 (Step 591000): Train loss 3.898, Val loss 4.346\n",
      "Epoch 3:  82%|████████▏ | 171510/210244 [37:08<08:05, 79.72batch/s]2025-04-02 21:43:47,574 - INFO - Epoch 3 (Step 592000): Train loss 4.211, Val loss 4.357\n",
      "Epoch 3:  82%|████████▏ | 172251/210244 [37:17<07:47, 81.24batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:43:56,854] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  82%|████████▏ | 172508/210244 [37:20<08:06, 77.60batch/s]2025-04-02 21:44:00,168 - INFO - Epoch 3 (Step 593000): Train loss 4.129, Val loss 4.333\n",
      "Epoch 3:  83%|████████▎ | 173507/210244 [37:34<07:37, 80.28batch/s]2025-04-02 21:44:13,976 - INFO - Epoch 3 (Step 594000): Train loss 3.999, Val loss 4.345\n",
      "Epoch 3:  83%|████████▎ | 173703/210244 [37:37<07:56, 76.76batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:44:16,365] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  83%|████████▎ | 174504/210244 [37:46<07:19, 81.39batch/s]2025-04-02 21:44:26,355 - INFO - Epoch 3 (Step 595000): Train loss 4.018, Val loss 4.333\n",
      "Epoch 3:  83%|████████▎ | 175506/210244 [38:00<07:31, 76.89batch/s]2025-04-02 21:44:40,265 - INFO - Epoch 3 (Step 596000): Train loss 3.970, Val loss 4.340\n",
      "Epoch 3:  84%|████████▍ | 176505/210244 [38:13<06:52, 81.74batch/s]2025-04-02 21:44:52,694 - INFO - Epoch 3 (Step 597000): Train loss 4.081, Val loss 4.351\n",
      "Epoch 3:  84%|████████▍ | 177507/210244 [38:25<06:40, 81.64batch/s]2025-04-02 21:45:05,254 - INFO - Epoch 3 (Step 598000): Train loss 4.032, Val loss 4.357\n",
      "Epoch 3:  85%|████████▍ | 178006/210244 [38:33<18:30, 29.02batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:45:12,773] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  85%|████████▍ | 178508/210244 [38:39<06:35, 80.18batch/s]2025-04-02 21:45:19,134 - INFO - Epoch 3 (Step 599000): Train loss 3.982, Val loss 4.338\n",
      "Epoch 3:  85%|████████▌ | 179509/210244 [38:52<06:23, 80.15batch/s]2025-04-02 21:45:31,594 - INFO - Epoch 3 (Step 600000): Train loss 3.909, Val loss 4.353\n",
      "Epoch 3:  86%|████████▌ | 180044/210244 [38:58<06:21, 79.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:45:38,203] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  86%|████████▌ | 180509/210244 [39:04<06:06, 81.23batch/s]2025-04-02 21:45:44,058 - INFO - Epoch 3 (Step 601000): Train loss 4.037, Val loss 4.372\n",
      "Epoch 3:  86%|████████▋ | 181504/210244 [39:18<05:57, 80.37batch/s]2025-04-02 21:45:57,826 - INFO - Epoch 3 (Step 602000): Train loss 3.950, Val loss 4.363\n",
      "Epoch 3:  87%|████████▋ | 182117/210244 [39:26<05:44, 81.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:46:05,442] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  87%|████████▋ | 182510/210244 [39:30<05:39, 81.74batch/s]2025-04-02 21:46:10,274 - INFO - Epoch 3 (Step 603000): Train loss 4.015, Val loss 4.342\n",
      "Epoch 3:  87%|████████▋ | 183383/210244 [39:43<05:33, 80.66batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:46:22,512] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  87%|████████▋ | 183512/210244 [39:44<05:33, 80.20batch/s]2025-04-02 21:46:24,189 - INFO - Epoch 3 (Step 604000): Train loss 3.874, Val loss 4.339\n",
      "Epoch 3:  88%|████████▊ | 184507/210244 [39:57<05:15, 81.48batch/s]2025-04-02 21:46:36,608 - INFO - Epoch 3 (Step 605000): Train loss 3.808, Val loss 4.347\n",
      "Epoch 3:  88%|████████▊ | 185505/210244 [40:09<05:01, 82.08batch/s]2025-04-02 21:46:49,033 - INFO - Epoch 3 (Step 606000): Train loss 3.988, Val loss 4.372\n",
      "Epoch 3:  89%|████████▊ | 186507/210244 [40:23<04:50, 81.62batch/s]2025-04-02 21:47:02,836 - INFO - Epoch 3 (Step 607000): Train loss 4.072, Val loss 4.330\n",
      "Epoch 3:  89%|████████▉ | 187511/210244 [40:35<04:33, 83.15batch/s]2025-04-02 21:47:15,249 - INFO - Epoch 3 (Step 608000): Train loss 4.012, Val loss 4.349\n",
      "Epoch 3:  89%|████████▉ | 188057/210244 [40:42<04:46, 77.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:47:22,065] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  90%|████████▉ | 188511/210244 [40:49<04:31, 79.90batch/s]2025-04-02 21:47:29,038 - INFO - Epoch 3 (Step 609000): Train loss 3.970, Val loss 4.348\n",
      "Epoch 3:  90%|█████████ | 189509/210244 [41:02<04:13, 81.83batch/s]2025-04-02 21:47:41,562 - INFO - Epoch 3 (Step 610000): Train loss 4.014, Val loss 4.371\n",
      "Epoch 3:  90%|█████████ | 189806/210244 [41:05<04:09, 82.08batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:47:45,215] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  91%|█████████ | 190508/210244 [41:14<04:04, 80.83batch/s]2025-04-02 21:47:53,909 - INFO - Epoch 3 (Step 611000): Train loss 3.904, Val loss 4.362\n",
      "Epoch 3:  91%|█████████ | 191510/210244 [41:28<03:54, 79.94batch/s]2025-04-02 21:48:07,792 - INFO - Epoch 3 (Step 612000): Train loss 4.102, Val loss 4.345\n",
      "Epoch 3:  92%|█████████▏| 192507/210244 [41:40<03:38, 81.07batch/s]2025-04-02 21:48:20,324 - INFO - Epoch 3 (Step 613000): Train loss 3.879, Val loss 4.330\n",
      "Epoch 3:  92%|█████████▏| 193107/210244 [41:48<03:27, 82.76batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:48:27,717] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  92%|█████████▏| 193510/210244 [41:53<03:24, 81.81batch/s]2025-04-02 21:48:32,753 - INFO - Epoch 3 (Step 614000): Train loss 4.079, Val loss 4.325\n",
      "Epoch 3:  93%|█████████▎| 194512/210244 [42:07<03:13, 81.25batch/s]2025-04-02 21:48:46,595 - INFO - Epoch 3 (Step 615000): Train loss 4.181, Val loss 4.319\n",
      "Epoch 3:  93%|█████████▎| 195506/210244 [42:19<03:02, 80.85batch/s]2025-04-02 21:48:59,073 - INFO - Epoch 3 (Step 616000): Train loss 3.993, Val loss 4.331\n",
      "Epoch 3:  93%|█████████▎| 196504/210244 [42:33<02:50, 80.76batch/s]2025-04-02 21:49:12,946 - INFO - Epoch 3 (Step 617000): Train loss 4.025, Val loss 4.339\n",
      "Epoch 3:  94%|█████████▍| 197255/210244 [42:42<02:41, 80.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:49:22,188] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  94%|█████████▍| 197505/210244 [42:45<02:35, 82.17batch/s]2025-04-02 21:49:25,342 - INFO - Epoch 3 (Step 618000): Train loss 3.708, Val loss 4.326\n",
      "Epoch 3:  94%|█████████▍| 198506/210244 [42:58<02:30, 78.01batch/s]2025-04-02 21:49:37,781 - INFO - Epoch 3 (Step 619000): Train loss 3.912, Val loss 4.321\n",
      "Epoch 3:  95%|█████████▍| 199277/210244 [43:09<02:14, 81.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:49:48,657] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  95%|█████████▍| 199511/210244 [43:12<02:11, 81.39batch/s]2025-04-02 21:49:51,608 - INFO - Epoch 3 (Step 620000): Train loss 4.232, Val loss 4.335\n",
      "Epoch 3:  95%|█████████▌| 200407/210244 [43:23<01:58, 83.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:50:02,734] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  95%|█████████▌| 200506/210244 [43:24<02:03, 79.07batch/s]2025-04-02 21:50:04,005 - INFO - Epoch 3 (Step 621000): Train loss 4.160, Val loss 4.329\n",
      "Epoch 3:  96%|█████████▌| 201506/210244 [43:38<01:45, 82.92batch/s]2025-04-02 21:50:17,702 - INFO - Epoch 3 (Step 622000): Train loss 4.148, Val loss 4.322\n",
      "Epoch 3:  96%|█████████▋| 202509/210244 [43:50<01:34, 81.71batch/s]2025-04-02 21:50:30,072 - INFO - Epoch 3 (Step 623000): Train loss 4.006, Val loss 4.356\n",
      "Epoch 3:  96%|█████████▋| 202724/210244 [43:53<01:32, 81.45batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:50:32,714] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  97%|█████████▋| 203512/210244 [44:03<01:23, 80.27batch/s]2025-04-02 21:50:42,427 - INFO - Epoch 3 (Step 624000): Train loss 4.058, Val loss 4.348\n",
      "Epoch 3:  97%|█████████▋| 204507/210244 [44:16<01:09, 82.23batch/s]2025-04-02 21:50:56,092 - INFO - Epoch 3 (Step 625000): Train loss 4.120, Val loss 4.335\n",
      "Epoch 3:  98%|█████████▊| 205509/210244 [44:29<00:57, 82.18batch/s]2025-04-02 21:51:08,407 - INFO - Epoch 3 (Step 626000): Train loss 3.995, Val loss 4.331\n",
      "Epoch 3:  98%|█████████▊| 206508/210244 [44:41<00:45, 81.74batch/s]2025-04-02 21:51:20,717 - INFO - Epoch 3 (Step 627000): Train loss 4.317, Val loss 4.317\n",
      "Epoch 3:  98%|█████████▊| 207072/210244 [44:49<00:40, 79.23batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:51:28,995] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  99%|█████████▊| 207507/210244 [44:55<00:33, 80.52batch/s]2025-04-02 21:51:34,569 - INFO - Epoch 3 (Step 628000): Train loss 3.948, Val loss 4.361\n",
      "Epoch 3:  99%|█████████▉| 208505/210244 [45:07<00:21, 81.84batch/s]2025-04-02 21:51:47,018 - INFO - Epoch 3 (Step 629000): Train loss 3.900, Val loss 4.370\n",
      "Epoch 3:  99%|█████████▉| 208697/210244 [45:10<00:19, 80.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:51:49,367] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████▉| 209511/210244 [45:21<00:09, 81.17batch/s]2025-04-02 21:52:00,805 - INFO - Epoch 3 (Step 630000): Train loss 4.079, Val loss 4.354\n",
      "Epoch 3: 100%|██████████| 210244/210244 [45:30<00:00, 76.99batch/s]\n",
      "2025-04-02 21:52:09,905 - INFO - Epoch 3 completed. Generating a sample...\n",
      "2025-04-02 21:52:09,979 - INFO - Generated Text: Every effort moves you out of the room . \" \n",
      "   = = = = = = \n",
      "   The song was released on the iTunes Store on September 2 , 2008 , and was released on September 2 , 2008 . It was released on September 2 ,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you out of the room . \"     = = = = = =     The song was released on the iTunes Store on September 2 , 2008 , and was released on September 2 , 2008 . It was released on September 2 ,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0004\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m train_losses, track_tokens_seen \u001b[38;5;241m=\u001b[39m train_model_simple(\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      8\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m      9\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     11\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     12\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     13\u001b[0m     eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     14\u001b[0m     eval_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     15\u001b[0m     start_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvery effort moves you\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     17\u001b[0m     deepspeed_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ds_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "train_losses, track_tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=3,\n",
    "    eval_freq=1000,\n",
    "    eval_iter=10,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer,\n",
    "    deepspeed_config=\"./ds_config.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiyoon/miniconda3/envs/LLMs/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3213514/2491510290.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# 6. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sllm_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=10,\n",
    "    deepspeed=\"./ds_config.json\",  # <- 여전히 사용 가능\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    # ddp_backend=None\n",
    ")\n",
    "\n",
    "# 7. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-01 23:59:57,902] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import os\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# os.environ[\"USE_MPI\"] = \"0\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Now, trainer.train() will handle the DataLoader and training.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMs/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMs/lib/python3.10/site-packages/transformers/trainer.py:2500\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2498\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2499\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2500\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2502\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMs/lib/python3.10/site-packages/transformers/trainer.py:5180\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5179\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5180\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5181\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5182\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMs/lib/python3.10/site-packages/accelerate/data_loader.py:566\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMs/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMs/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMs/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMs/lib/python3.10/site-packages/transformers/data/data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMs/lib/python3.10/site-packages/transformers/data/data_collator.py:131\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m--> 131\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mvars\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m    132\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMs/lib/python3.10/site-packages/transformers/data/data_collator.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m--> 131\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m    132\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"USE_MPI\"] = \"0\"\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()  # Now, trainer.train() will handle the DataLoader and training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
