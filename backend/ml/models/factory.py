import torch
import torch.nn as nn
from typing import Dict, Any, Union, List
from .utils import str_to_torch_dtype

from .components.token_embedding import TokenEmbedding
from .components.positional_embedding import (
    LearnedPositionalEmbedding,
    SinusoidalPositionalEmbedding,
    RelativePositionalEmbedding,
    RotaryPositionalEmbedding,
)
from .components.attention import (
    MultiHeadAttentionUsingSDP,
    GroupedQueryAttention,
)
from .components.normalization import LayerNorm, RMSNorm
from .components.ffn import CustomFFN
from .components.residual import ResidualConnection
from .components.transformer_block import TrasnformerBlock


# ===== Factory Classes =====
class LayerFactory:
    """ë©”ì¸ íŒ©í† ë¦¬ í´ë˜ìŠ¤: ëª¨ë“  ë ˆì´ì–´ ìƒì„±ì„ ë‹´ë‹¹"""
    @staticmethod
    def create_layer(node: Dict[str, Any], dtype=torch.float32) -> nn.Module:
        layer_type = node["type"]
        data = node["data"].copy()
        data["dtype"] = dtype  # dtypeì„ dataì— ì¶”ê°€

        factory_map = {
            "tokenEmbedding": TokenEmbeddingFactory,
            "positionalEmbedding": PositionalEmbeddingFactory,
            "normalization": NormalizationFactory,
            "attention": AttentionFactory,
            "mhAttention": AttentionFactory,
            "flashAttention": AttentionFactory,
            "gqAttention": AttentionFactory,
            "feedForward": FeedForwardFactory,
            "residual": ResidualFactory,
            "transformerBlock": TransformerBlockFactory,
            "linear": LinearFactory,
            "dropout": DropoutFactory,
        }

        factory = factory_map.get(layer_type)
        if factory is None:
            raise ValueError(f"Unknown layer type: {layer_type}")

        # AttentionFactoryì˜ ê²½ìš° layer_typeì„ ì¶”ê°€ë¡œ ì „ë‹¬
        if factory == AttentionFactory:
            return factory.create(
                data if layer_type != "transformerBlock" else node,
                dtype=dtype,
                layer_type=layer_type,
            )
        else:
            return factory.create(
                data if layer_type != "transformerBlock" else node, dtype=dtype
            )


class TokenEmbeddingFactory:
    """í† í° ì„ë² ë”© ë ˆì´ì–´ ìƒì„±"""
    @staticmethod
    def create(data: Dict[str, Any], dtype=torch.float32) -> TokenEmbedding:
        return TokenEmbedding(
            vocab_size=data["vocabSize"],
            d_model=data["embDim"],
            dtype=dtype,  # dtype ì¶”ê°€
        )


class PositionalEmbeddingFactory:
    """í¬ì§€ì…”ë„ ì„ë² ë”© ë ˆì´ì–´ ìƒì„±"""
    @staticmethod
    def create(data: Dict[str, Any], dtype=torch.float32):
        mode = data.get("mode", "Learned Positional Embedding")
        common_args = {
            "ctx_length": data["ctxLength"],
            "emb_dim": data["embDim"],
            "dtype": dtype,  # dtype ì¶”ê°€
        }

        if mode == "Learned Positional Embedding":
            return LearnedPositionalEmbedding(**common_args)
        elif mode == "Sinusoidal Positional Embedding":
            return SinusoidalPositionalEmbedding(**common_args)
        elif mode == "Relative Positional Embedding":
            return RelativePositionalEmbedding(**common_args)
        elif mode == "Rotary Positional Embedding":
            return RotaryPositionalEmbedding(
                data["embDim"], data["ctxLength"], dtype=dtype
            )
        else:
            raise ValueError(f"Unknown positional embedding mode: {mode}")


class NormalizationFactory:
    """ì •ê·œí™” ë ˆì´ì–´ ìƒì„±"""
    @staticmethod
    def create(data: Dict[str, Any], dtype=torch.float32) -> LayerNorm:
        d_model = data.get("embDim") or data.get("inDim") or data.get("outDim")
        
        if data.get("normType") == "Layer Normalization":
            return LayerNorm(d_model, dtype=dtype)  # dtype ì¶”ê°€
        elif data.get("normType") == "RMS Normalization":
            return RMSNorm(d_model, dtype=dtype)  # dtype ì¶”ê°€
        else:
            raise ValueError(f"Unknown normalization type: {data.get('normType')}")


class AttentionFactory:
    """ì–´í…ì…˜ ë ˆì´ì–´ ìƒì„± (ìƒˆë¡œìš´ íƒ€ì…ë³„ ì²˜ë¦¬)"""
    @staticmethod
    def create(data: Dict[str, Any], dtype=torch.float32, layer_type: str = None):
        # ê³µí†µ ì¸ì ì„¤ì •
        common_args = {
            "d_in": data.get("inDim") or data.get("embDim"),
            "d_out": data.get("outDim") or data.get("embDim"),
            "context_length": data["ctxLength"],
            "dtype": dtype,
        }

        # íƒ€ì…ë³„ ì²˜ë¦¬
        if layer_type == "mhAttention" or (
            layer_type is None and "numHeads" in data
        ):
            # Scaled Dot-Product Attention
            common_args.update(
                {
                    "num_heads": data["numHeads"],
                    "dropout": data.get("dropoutRate"),
                    "qkv_bias": data.get("qkvBias"),
                    "is_rope": data.get("isRoPE"),
                    "rope_base": data.get("ropeBase", 10000.0),
                }
            )
            return MultiHeadAttentionUsingSDP(**common_args)

        elif layer_type == "gqAttention":
            # Grouped Query Attention (+ RoPE)
            common_args.update(
                {
                    "num_heads": data["numHeads"],
                    "dropout": data.get("dropoutRate"),
                    "qkv_bias": data.get("qkvBias"),
                    "is_rope": data.get("isRoPE"),
                    "rope_base": data.get("ropeBase"),
                    "rope_config": data.get("ropeConfig"),
                    "num_kv_groups": data["numKvGroups"],
                }
            )
            
            return GroupedQueryAttention(**common_args)

        else:  
            # ê¸°ì¡´ attention íƒ€ì… (í•˜ìœ„ í˜¸í™˜ì„±)
            attn_type = data.get("attn_type", "default")
            common_args.update(
                {
                    "num_heads": data["numHeads"],
                    "dropout": data.get("dropoutRate"),
                    "qkv_bias": data.get("qkvBias"),
                    "is_rope": data.get("isRoPE"),
                    "rope_base": data.get("ropeBase"),
                    "rope_config": data.get("ropeConfig"),
                    "num_kv_groups": data["numKvGroups"],
                }
            )

            if attn_type == "default" or attn_type == "mha":
                return MultiHeadAttentionUsingSDP(**common_args)
            elif attn_type == "gqa":
                # ë ˆê±°ì‹œ í‚¤ ì´ë¦„ì„ ì“°ëŠ” ê²½ìš°ë„ ë™ì¼ ì²˜ë¦¬
                head_dim = (common_args["d_out"] // common_args["numHeads"])
                if head_dim % 2 != 0:
                    raise ValueError(
                        f"head_dim({head_dim}) must be even for RoPE (GQA)."
                    )
                return GroupedQueryAttention(
                    **{k: v for k, v in common_args.items() if k != "qkvBias"},
                    num_kv_groups=data["numKvGroups"],
                    rope_base=data.get("ropeBase"),
                    rope_config=data.get("ropeConfig"),
                )
            else:
                raise ValueError(f"Unknown attention type: {attn_type}")


class FeedForwardFactory:
    """í”¼ë“œí¬ì›Œë“œ ë ˆì´ì–´ ìƒì„±"""
    @staticmethod
    def create(data: Dict[str, Any], dtype=torch.float32) -> CustomFFN:
        if data.get("actFunc") is None:
            raise ValueError(f"FeedForward layer '{data.get('id', 'unknown')}' must have an 'actFunc' field")
        if data.get("feedForwardType") is None:
            raise ValueError(f"FeedForward layer '{data.get('id', 'unknown')}' must have a 'feedForwardType' field")
        if data.get("bias") is None:
            raise ValueError(f"FeedForward layer '{data.get('id', 'unknown')}' must have a 'bias' field")
        if data.get("hiddenDim") is None:
            raise ValueError(f"FeedForward layer '{data.get('id', 'unknown')}' must have a 'hiddenDim' field")
        
        return CustomFFN(
            emb_dim=data.get("outDim") or data.get("inDim"),
            hidden_dim=data.get("hiddenDim"),
            activation=data.get("actFunc"),
            is_gated=data.get("feedForwardType") == "Gated",
            bias=data.get("bias"),
            dtype=dtype,  # dtype ì´ë¯¸ ìˆìŒ
        )


class ResidualFactory:
    """ì”ì°¨ ì—°ê²° ë ˆì´ì–´ ìƒì„±"""
    @staticmethod
    def create(data: Dict[str, Any], dtype=torch.float32) -> ResidualConnection:
        if "source" not in data:
            raise ValueError(
                f"Residual layer '{data.get('id', 'unknown')}' must have a 'source' field"
            )
        return ResidualConnection(data["source"])


class TransformerBlockFactory:
    """íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ ë ˆì´ì–´ ìƒì„±"""
    @staticmethod
    def create(node: Dict[str, Any], dtype=torch.float32) -> TrasnformerBlock:
        children = [
            LayerFactory.create_layer(child, dtype=dtype)
            for child in node.get("children", [])
        ]
        num_layers = node["data"].get("numOfBlocks", 1)
        block_id = node["data"].get("id")
        return TrasnformerBlock(*children, num_layers=num_layers, block_id=block_id)


class LinearFactory:
    """ì„ í˜• ë ˆì´ì–´ ìƒì„±"""
    @staticmethod
    def create(data: Dict[str, Any], dtype=torch.float32) -> nn.Linear:
        if data.get("bias") is None:
            raise ValueError(f"Linear layer '{data.get('id', 'unknown')}' must have a 'bias' field")
        if data.get("weightTying") is None:
            raise ValueError(f"Linear layer '{data.get('id', 'unknown')}' must have a 'weightTying' field")
        
        layer = nn.Linear(
            in_features=data["inDim"],
            out_features=data["outDim"],
            bias=data.get("bias"),
            dtype=dtype,  # dtype ì¶”ê°€
        )
        
        # ğŸ”‘ ì´í›„ tying ë‹¨ê³„ì—ì„œ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í”Œë˜ê·¸ì™€ ë©”íƒ€ì •ë³´ ë¶€ì°©
        layer._weight_tying = bool(data.get("weightTying"))   # <-- ì¶”ê°€
        layer._declared_inDim = data["inDim"]                 # <-- ì¶”ê°€
        layer._declared_outDim = data["outDim"]               # <-- ì¶”ê°€
        return layer


class DropoutFactory:
    """ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ ìƒì„±"""
    @staticmethod
    def create(data: Dict[str, Any], dtype=torch.float32) -> nn.Dropout:
        return nn.Dropout(data.get("dropoutRate", 0.1))


# ===== Model Classes =====
class CustomSequential(nn.Module):
    """ì»¤ìŠ¤í…€ ì‹œí€€ì…œ ëª¨ë¸: ë ˆì´ì–´ ê°„ ì—°ê²°ê³¼ ìºì‹±ì„ ì²˜ë¦¬"""
    def __init__(self, layer_list: List[nn.Module], id_to_module_map: Dict[str, nn.Module]):
        super().__init__()
        self.layers = nn.ModuleList(layer_list)
        self.id_to_module = id_to_module_map

    def forward(self, x):
        cache = {}
        prev_out = None
        for i, layer in enumerate(self.layers):
            # 1) TokenEmbedding â†’ PositionalEmbedding(learned/sinusoidal/relative) ìë™ í•©ì‚°
            #    RotaryPositionalEmbeddingì€ ì œì™¸ (RoPEëŠ” ì–´í…ì…˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨)
            if (
                i > 0
                and isinstance(self.layers[i - 1], TokenEmbedding)
                and isinstance(
                    layer,
                    (
                        LearnedPositionalEmbedding,
                        SinusoidalPositionalEmbedding,
                        RelativePositionalEmbedding,
                    ),
                )
            ):
                pos_out = layer(x)
                x = prev_out + pos_out

            # 2) ResidualConnection ì²˜ë¦¬
            elif isinstance(layer, ResidualConnection):
                source_id = layer.source_id
                if source_id in cache:
                    x = x + cache[source_id]
                else:
                    # TrasnformerBlock ë‚´ë¶€ì˜ ë ˆì´ì–´ IDë„ í™•ì¸
                    block_source_id = (
                        f"{layer.source_id}_layer_{i}"
                        if hasattr(layer, "block_id")
                        else layer.source_id
                    )
                    if block_source_id in cache:
                        x = x + cache[block_source_id]
                    else:
                        # ì´ì „ ì¶œë ¥ì„ ì‚¬ìš©
                        x = x + prev_out
            else:
                x = layer(x)

            prev_out = x
            if hasattr(layer, "layer_id"):
                cache[layer.layer_id] = x
        return x

    def forward_cached(self, x, caches=None, start_pos=0, use_cache=True):
        """ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ë ˆì´ì–´ë¥¼ í¬ì›Œë“œ (Llama2 í˜•ì‹)"""
        if caches is None: caches = {}
        new_caches = {}
        for i, layer in enumerate(self.layers):
            if isinstance(layer, (MultiHeadAttentionUsingSDP, GroupedQueryAttention)):
                out, new_cache = layer(x, start_pos=start_pos, kv_cache=caches.get(i), use_cache=use_cache, return_cache=True)
                x = out
                new_caches[i] = new_cache
            else:
                # ê¸°ì¡´ í† í°/ìƒëŒ€/í•™ìŠµí˜• í¬ì§€ì…”ë„ ì„ë² ë”© í•©ì‚° ë¡œì§ì€ ìœ ì§€
                x = layer(x)
        return x, new_caches


# ===== Public API =====
def build_model_from_json(
    json_list: List[Dict[str, Any]], dtype: str = "fp32"
) -> CustomSequential:
    """JSONìœ¼ë¡œë¶€í„° ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ (dtype ì§€ì›)"""
    id_map = {}
    id_to_module = {}

    torch_dtype = str_to_torch_dtype(dtype)

    # ì²« ë²ˆì§¸ ê°ì²´(config)ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë ˆì´ì–´ë“¤ë§Œ ì²˜ë¦¬
    layer_nodes = [node for node in json_list if "type" in node]

    for node in layer_nodes:
        id_map[node["data"]["id"]] = node

    layers = []
    for node in layer_nodes:
        print(f"Creating layer: {node['data']['id']}")
        layer = LayerFactory.create_layer(node, dtype=torch_dtype)
        if "id" in node["data"]:
            layer.layer_id = node["data"]["id"]
            id_to_module[layer.layer_id] = layer
        layers.append(layer)

    # === Weight tying ë‹¨ê³„ ì¶”ê°€ ===
    # 1) ê¸°ì¤€ì´ ë  TokenEmbeddingì„ ì°¾ìŒ (ê°€ì¥ ë¨¼ì €/ë§ˆì§€ë§‰ìœ¼ë¡œ ë“±ì¥í•œ ê²ƒì„ ì„ íƒ)
    token_emb = None
    for m in layers:
        if isinstance(m, TokenEmbedding):
            token_emb = m   # ì—¬ëŸ¬ ê°œë©´ ë§ˆì§€ë§‰ ê²ƒì„ ì‚¬ìš©

    if token_emb is not None:
        # TokenEmbedding ë‚´ë¶€ weight ì ‘ê·¼ (TokenEmbedding.weight ë˜ëŠ” TokenEmbedding.embedding.weight ì¼€ì´ìŠ¤ ëª¨ë‘ ì²˜ë¦¬)
        emb_w = getattr(token_emb, "weight", None)
        if emb_w is None and hasattr(token_emb, "embedding"):
            emb_w = getattr(token_emb.embedding, "weight", None)

        if emb_w is None:
            raise RuntimeError("TokenEmbedding weight not found for tying.")

        vocab_size, emb_dim = emb_w.shape

        for m in layers:
            if isinstance(m, torch.nn.Linear) and getattr(m, "_weight_tying", False):
                # í¬ê¸° ê²€ì¦
                if m._declared_inDim != emb_dim or m._declared_outDim != vocab_size:
                    raise ValueError(
                        f"Weight tying size mismatch: Linear(in={m._declared_inDim}, out={m._declared_outDim}) "
                        f"vs Embedding(vocab={vocab_size}, emb={emb_dim})"
                    )
                # ì‹¤ì œ tying: ê°™ì€ Parameterë¥¼ ì°¸ì¡°í•˜ê²Œ í•¨
                m.weight = emb_w
                print(f"--- Weight tying: {m.layer_id} ---")

                # (ì„ íƒ) lm_head biasë¥¼ ì“°ì§€ ì•Šë„ë¡ ê¶Œì¥
                # if m.bias is not None:
                #     with torch.no_grad():
                #         m.bias.zero_()
    else:
        # ëª¨ë¸ ì•ˆì— TokenEmbeddingì´ ì—†ëŠ”ë° weightTying Trueì¸ Linearê°€ ìˆìœ¼ë©´ ì—ëŸ¬ë¡œ ì•ˆë‚´
        for m in layers:
            if isinstance(m, torch.nn.Linear) and getattr(m, "_weight_tying", False):
                raise ValueError("Linear(weightTying=True) found but no TokenEmbedding layer exists.")

    return CustomSequential(layers, id_to_module)
