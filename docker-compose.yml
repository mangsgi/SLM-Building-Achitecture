services:
  # redis 컨테이너
  redis:
    image: redis:7-alpine
    container_name: slm-redis
    ports: ["6379:6379"]
    volumes: ["redis_data:/data"]
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
  # mlflow 컨테이너
  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.3.2
    container_name: slm-mlflow
    command: >
      mlflow server --host 0.0.0.0 --port 5000
      --backend-store-uri sqlite:////mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
    volumes:
      - mlflow_data:/mlflow
    tmpfs:
      - /mlflow/artifacts:size=8g,mode=1777
    ports: ["5000:5000"]
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
  # backend 컨테이너
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: slm-backend
    working_dir: /app/backend
    environment:
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
      - ./backend:/app/backend # 코드 저장 시 즉시 반영 (uvicorn --reload)
      - ./backend/model_structures:/app/backend/model_structures # 모델 구조를 로컬에서도 확인
      - mlflow_data:/mlflow:rw # (필요 시) mlflow 경로 공유
    tmpfs:
      - /app/backend/completed:size=2g,mode=1777
    depends_on: [redis, mlflow]
    ports: ["8000:8000"]
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
  # worker 컨테이너
  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: slm-celery-worker
    working_dir: /app/backend
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - REDIS_URL=redis://redis:6379/0
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
      - ./backend:/app/backend # 코드 변경 시 자동 재시작 감지
      - ./backend/model_structures:/app/backend/model_structures # 모델 구조를 로컬에서도 확인
      - mlflow_data:/mlflow:rw
    tmpfs:
      - /app/backend/completed:size=2g,mode=1777
    depends_on: [redis, mlflow, backend]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # 모든 GPU 사용
              capabilities: [gpu]
    command: >
      watchmedo auto-restart
      --directory=/app/backend
      --pattern="*.py"
      --ignore-patterns="*.pyc;*~;__pycache__/*;*.log"
      --recursive
      --interval=1 --
      celery -A celery_worker.celery_app worker --loglevel=info
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
  # frontend 컨테이너
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: slm-frontend
    working_dir: /app/frontend
    environment:
      - VITE_API_BASE=http://localhost:8000
      - CHOKIDAR_USEPOLLING=true # WSL/도커 파일감시 보정
      - CHOKIDAR_INTERVAL=100
    volumes:
      - ./frontend:/app/frontend # 덮어씀 방지
      - frontend_node_modules:/app/frontend/node_modules
    depends_on: [backend]
    ports: ["5173:5173"]
    command: npm run dev -- --host 0.0.0.0
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  redis_data:
  mlflow_data:
  frontend_node_modules:
